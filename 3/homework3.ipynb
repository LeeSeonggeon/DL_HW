{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1528d477f56b21ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /home/work/.local/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: torchinfo in /home/work/.local/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/work/.local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e45fd25a69ab664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtmdrjs0040\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767b0f0398a3f2ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T07:09:52.168824Z",
     "start_time": "2024-11-19T07:09:50.414324Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a8e4470c45438b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T07:42:24.618480Z",
     "start_time": "2024-11-19T07:42:24.609767Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "def get_fashion_mnist_data():\n",
    "    #데이터 다운\n",
    "    #만약 root 경로에 데이터가 없다면 다운로드 \n",
    "    #train=True/False는 데이터의 용도 구분 따로 validation이 없기 때문에 validation data는 train_data에서 알아서 분할해야 할듯\n",
    "    f_mnist_train = FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    \n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "    \n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.2860495448112488, std=0.32041478157043457), \n",
    "                                #mean값과 std값을 이용해 데이터의 평균이 0 표준편차가 1이 되도록 한다.\n",
    "                                #gray image이므로 모든 픽셀의 값을 평균내고 표준편차값을 구한다.\n",
    "                                #train 데이터의 값만으로 정한다.(검증과 테스트는 성능 확인용이므로 통계에 포함하면 일반화 성능 왜곡)\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146ecff9f8e7b526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T07:42:34.539624Z",
     "start_time": "2024-11-19T07:42:25.749608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.2861834168434143, Std: 0.3204808533191681\n"
     ]
    }
   ],
   "source": [
    "#정규화를 위한 mean과 std 구하기\n",
    "\n",
    "f_mnist_train = FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "mean = 0\n",
    "std = 0\n",
    "total_images = len(f_mnist_train)\n",
    "\n",
    "for img,_ in f_mnist_train: #이미지 별로 평균과 표준편차를 구해서 합산\n",
    "    img = img.view(1, -1)\n",
    "    mean += img.mean()\n",
    "    std += img.std()\n",
    "\n",
    "#합산된 평균과 표준편차를 평균내어 구하기\n",
    "mean /= total_images\n",
    "std /= total_images\n",
    "\n",
    "print(f\"Mean: {mean}, Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "890ce4c60975a8ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:27:25.947324Z",
     "start_time": "2024-11-19T08:27:25.941293Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cnn_model(dropout=0.5):\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, in_channels, n_output, dropout):\n",
    "            super().__init__()\n",
    "\n",
    "            self.model = nn.Sequential(\n",
    "                # B x 1 x 28 x 28 --> B x 6 x (28 - 5 + 1) x (28 - 5 + 1) = B x 6 x 24 x 24\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=(5, 5), stride=(1, 1)),\n",
    "                # B x 6 x 24 x 24 --> B x 6 x 12 x 12\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(\n",
    "                    num_features = 6, eps= 1e-05, momentum=0.1\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                # B x 6 x 12 x 12 --> B x 16 x (12 - 5 + 1) x (12 - 5 + 1) = B x 16 x 8 x 8\n",
    "                nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1, 1)),\n",
    "                # B x 16 x 8 x 8 --> B x 16 x 4 x 4\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(\n",
    "                    num_features = 16, eps= 1e-05, momentum=0.1\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                # B x 16 x 4 x 4 --> B x 256\n",
    "                nn.Flatten(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(\n",
    "                    num_features = 128, eps=1e-05, momentum=0.1\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(128, n_output),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    # 1 * 28 * 28\n",
    "    my_model = MyModel(in_channels=1, n_output=10, dropout = dropout)\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9578e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_model(dropout=0.5):\n",
    "    def vgg_block(num_conv_layer, out_channels):\n",
    "        layers = []\n",
    "        for _ in range(num_conv_layer):\n",
    "            layers.append(nn.LazyConv2d(\n",
    "                out_channels=out_channels, kernel_size=3, padding=1\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        block = nn.Sequential(*layers)\n",
    "        return block\n",
    "    \n",
    "    class VGG(nn.Module):\n",
    "        def __init__(self, block_info, n_output=10):\n",
    "            super().__init__()\n",
    "            conv_blocks=[]\n",
    "            for(num_conv_layers, out_channels) in block_info:\n",
    "                conv_blocks.append(vgg_block(num_conv_layers, out_channels))\n",
    "            \n",
    "            self.model = nn.Sequential(\n",
    "                *conv_blocks,\n",
    "                nn.Flatten(), \n",
    "                nn.LazyLinear(out_features = 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.LazyLinear(out_features = 128),\n",
    "                nn.ReLU(),\n",
    "                nn.LazyLinear(out_features = 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.LazyLinear(n_output)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "    \n",
    "    my_model = VGG(\n",
    "        block_info = (\n",
    "            (3, 128), (3, 256)\n",
    "        ),\n",
    "        n_output = 10\n",
    "    )\n",
    "    return my_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc89563ecf6bc4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:33:09.740332Z",
     "start_time": "2024-11-19T08:33:09.732984Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "            self, patience = 10, delta =0.0001, project_name = None, checkpoint_file_path = None, run_time_str = None\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.delta = delta\n",
    "        self.val_loss_min = None\n",
    "        self.file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "        )\n",
    "        self.latest_file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}_latest.pt\"# 여러 테스트를 확인할때 똑같이 latest로만 하니까 테스트 별로 구분이 안되서 변경\n",
    "        )\n",
    "        \n",
    "    def check_and_save(self, new_validation_loss, model): #val_loss를 확인하고, 작아졌는지 확인하고 count 여부를 정하며, 만약 counter가 patience보다 커지면 이를 알린다.\n",
    "        early_stop = False\n",
    "        message = None\n",
    "        \n",
    "        if self.val_loss_min is None:\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            message =f\"Early stopping is stated!\"\n",
    "        elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "            message =f'V_loss decreased ({self.val_loss_min:6.3f} --> {new_validation_loss:6.3f}). Saving model..'\n",
    "            self.save_checkpoint(new_validation_loss, model)\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "            if self.counter >= self.patience:\n",
    "                early_stop = True\n",
    "                message += \"*** TRAIN EARLY STOPPED! ***\"\n",
    "            \n",
    "        return message, early_stop, self.counter\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model): #모델을 저장하는 함수\n",
    "        torch.save(model.state_dict(), self.file_path)\n",
    "        torch.save(model.state_dict(), self.latest_file_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "558c44489a1a7a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:33:10.296448Z",
     "start_time": "2024-11-19T08:33:10.283669Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from utils import strfdelta\n",
    "\n",
    "\n",
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "    \n",
    "    self.exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "    \n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Will be explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=-1)\n",
    "\n",
    "      # >>> predicted_train: tensor([5, 8, 9, 0, 9, 8, 9, 8, ..., 0, 1, 3, 7, 1, 4, 3])\n",
    "      # >>> target_train:    tensor([5, 8, 9, 2, 9, 8, 7, 8, ..., 4, 1, 9, 6, 1, 4, 3])\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()   \n",
    "    \n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop, _ = early_stopping.check_and_save(validation_loss, self.model)\n",
    "        self.exp_lr_scheduler.step()\n",
    "        \n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da77f16f982986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs = 1000, batch_size = 2048, validation_intervals = 10, learning_rate = 1e-3, early_stop_patience = 10, early_stop_delta = 0.0001, weight_decay = 0.001, dropout = 0.5):\n",
    "    #checkpoint의 경로 설정 + 디렉토리 생성\n",
    "    CHECKPOINT_FILE_PATH = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "    if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "        os.makedirs(CHECKPOINT_FILE_PATH)\n",
    "        \n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    config = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_intervals': validation_intervals,\n",
    "    'learning_rate': learning_rate,\n",
    "    'early_stop_patience': early_stop_patience,\n",
    "    'early_stop_delta': early_stop_delta,\n",
    "    'weight_decay': weight_decay,\n",
    "    'dropout': dropout\n",
    "  }\n",
    "    \n",
    "    project_name = \"FashionMNIST\"\n",
    "    wandb.init(\n",
    "        mode=\"online\",\n",
    "        project=project_name,\n",
    "        notes = \"Fashion MNIST\",\n",
    "        tags = [\"cnn\", \"FashionMNIST\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device {device}.\")\n",
    "  \n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    #model = get_cnn_model(dropout)\n",
    "    model = get_vgg_model()\n",
    "    model.to(device)\n",
    "    \n",
    "    print(summary(model=model, input_size=(1, 1, 28, 28)))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate, betas=(0.9, 0.999), weight_decay=wandb.config.weight_decay)\n",
    "    \n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms, run_time_str, wandb, device, CHECKPOINT_FILE_PATH\n",
    "    )\n",
    "    classification_trainer.train_loop()\n",
    "    \n",
    "    wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5617dfd080caaa89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/3/wandb/run-20241123_105721-894wpar6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/FashionMNIST/runs/894wpar6' target=\"_blank\">2024-11-23_10-57-21</a></strong> to <a href='https://wandb.ai/tmdrjs0040/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/FashionMNIST' target=\"_blank\">https://wandb.ai/tmdrjs0040/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/FashionMNIST/runs/894wpar6' target=\"_blank\">https://wandb.ai/tmdrjs0040/FashionMNIST/runs/894wpar6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda:0.\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      [1, 10]                   --\n",
      "├─Sequential: 1-1                        [1, 10]                   --\n",
      "│    └─Sequential: 2-1                   [1, 128, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-1                  [1, 128, 28, 28]          1,280\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-3                    [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-4                  [1, 128, 28, 28]          147,584\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-6                    [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-7                  [1, 128, 28, 28]          147,584\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-9                    [1, 128, 28, 28]          --\n",
      "│    │    └─MaxPool2d: 3-10              [1, 128, 14, 14]          --\n",
      "│    └─Sequential: 2-2                   [1, 256, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-11                 [1, 256, 14, 14]          295,168\n",
      "│    │    └─BatchNorm2d: 3-12            [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-13                   [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-14                 [1, 256, 14, 14]          590,080\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-16                   [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-17                 [1, 256, 14, 14]          590,080\n",
      "│    │    └─BatchNorm2d: 3-18            [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-19                   [1, 256, 14, 14]          --\n",
      "│    │    └─MaxPool2d: 3-20              [1, 256, 7, 7]            --\n",
      "│    └─Flatten: 2-3                      [1, 12544]                --\n",
      "│    └─Linear: 2-4                       [1, 256]                  3,211,520\n",
      "│    └─ReLU: 2-5                         [1, 256]                  --\n",
      "│    └─Dropout: 2-6                      [1, 256]                  --\n",
      "│    └─Linear: 2-7                       [1, 128]                  32,896\n",
      "│    └─ReLU: 2-8                         [1, 128]                  --\n",
      "│    └─Linear: 2-9                       [1, 64]                   8,256\n",
      "│    └─ReLU: 2-10                        [1, 64]                   --\n",
      "│    └─Dropout: 2-11                     [1, 64]                   --\n",
      "│    └─Linear: 2-12                      [1, 10]                   650\n",
      "==========================================================================================\n",
      "Total params: 5,027,402\n",
      "Trainable params: 5,027,402\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 524.84\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 7.23\n",
      "Params size (MB): 20.11\n",
      "Estimated Total Size (MB): 27.34\n",
      "==========================================================================================\n",
      "[Epoch   1] T_loss: 1.50432, T_accuracy: 43.8709 | V_loss: 1.10891, V_accuracy: 59.8800 | Early stopping is stated! | T_time: 00:00:12, T_speed: 0.083\n",
      "[Epoch   2] T_loss: 0.71985, T_accuracy: 74.0018 | V_loss: 0.50705, V_accuracy: 81.9400 | V_loss decreased ( 1.109 -->  0.507). Saving model.. | T_time: 00:00:24, T_speed: 0.083\n",
      "[Epoch   3] T_loss: 0.52609, T_accuracy: 81.6709 | V_loss: 0.40362, V_accuracy: 85.2800 | V_loss decreased ( 0.507 -->  0.404). Saving model.. | T_time: 00:00:36, T_speed: 0.083\n",
      "[Epoch   4] T_loss: 0.43467, T_accuracy: 85.2618 | V_loss: 0.34955, V_accuracy: 88.3200 | V_loss decreased ( 0.404 -->  0.350). Saving model.. | T_time: 00:00:49, T_speed: 0.082\n",
      "[Epoch   5] T_loss: 0.38026, T_accuracy: 87.2527 | V_loss: 0.33287, V_accuracy: 88.3200 | V_loss decreased ( 0.350 -->  0.333). Saving model.. | T_time: 00:01:01, T_speed: 0.082\n",
      "[Epoch   6] T_loss: 0.34170, T_accuracy: 88.6618 | V_loss: 0.29526, V_accuracy: 89.9600 | V_loss decreased ( 0.333 -->  0.295). Saving model.. | T_time: 00:01:14, T_speed: 0.081\n",
      "[Epoch   7] T_loss: 0.30933, T_accuracy: 90.0127 | V_loss: 0.28038, V_accuracy: 90.4400 | V_loss decreased ( 0.295 -->  0.280). Saving model.. | T_time: 00:01:26, T_speed: 0.081\n",
      "[Epoch   8] T_loss: 0.28516, T_accuracy: 90.7745 | V_loss: 0.26384, V_accuracy: 90.8400 | V_loss decreased ( 0.280 -->  0.264). Saving model.. | T_time: 00:01:38, T_speed: 0.082\n",
      "[Epoch   9] T_loss: 0.26726, T_accuracy: 91.3200 | V_loss: 0.25657, V_accuracy: 91.4600 | V_loss decreased ( 0.264 -->  0.257). Saving model.. | T_time: 00:01:51, T_speed: 0.081\n",
      "[Epoch  10] T_loss: 0.24588, T_accuracy: 92.1273 | V_loss: 0.23093, V_accuracy: 92.0200 | V_loss decreased ( 0.257 -->  0.231). Saving model.. | T_time: 00:02:03, T_speed: 0.081\n",
      "[Epoch  11] T_loss: 0.23703, T_accuracy: 92.4964 | V_loss: 0.22863, V_accuracy: 92.3600 | V_loss decreased ( 0.231 -->  0.229). Saving model.. | T_time: 00:02:16, T_speed: 0.081\n",
      "[Epoch  12] T_loss: 0.22212, T_accuracy: 92.9164 | V_loss: 0.23441, V_accuracy: 92.1000 | Early stopping counter: 1 out of 15 | T_time: 00:02:27, T_speed: 0.082\n",
      "[Epoch  13] T_loss: 0.21261, T_accuracy: 93.1218 | V_loss: 0.21510, V_accuracy: 92.9000 | V_loss decreased ( 0.229 -->  0.215). Saving model.. | T_time: 00:02:38, T_speed: 0.082\n",
      "[Epoch  14] T_loss: 0.20324, T_accuracy: 93.5673 | V_loss: 0.21657, V_accuracy: 92.3600 | Early stopping counter: 1 out of 15 | T_time: 00:02:49, T_speed: 0.083\n",
      "[Epoch  15] T_loss: 0.19610, T_accuracy: 93.8655 | V_loss: 0.21183, V_accuracy: 92.7600 | V_loss decreased ( 0.215 -->  0.212). Saving model.. | T_time: 00:03:00, T_speed: 0.083\n",
      "[Epoch  16] T_loss: 0.18411, T_accuracy: 94.2145 | V_loss: 0.21310, V_accuracy: 93.4800 | Early stopping counter: 1 out of 15 | T_time: 00:03:11, T_speed: 0.084\n",
      "[Epoch  17] T_loss: 0.16746, T_accuracy: 94.7527 | V_loss: 0.22006, V_accuracy: 93.0800 | Early stopping counter: 2 out of 15 | T_time: 00:03:22, T_speed: 0.084\n",
      "[Epoch  18] T_loss: 0.16803, T_accuracy: 94.7527 | V_loss: 0.20080, V_accuracy: 93.6000 | V_loss decreased ( 0.212 -->  0.201). Saving model.. | T_time: 00:03:33, T_speed: 0.085\n",
      "[Epoch  19] T_loss: 0.15803, T_accuracy: 95.1182 | V_loss: 0.20285, V_accuracy: 93.7800 | Early stopping counter: 1 out of 15 | T_time: 00:03:44, T_speed: 0.085\n",
      "[Epoch  20] T_loss: 0.14848, T_accuracy: 95.4545 | V_loss: 0.19984, V_accuracy: 93.9000 | V_loss decreased ( 0.201 -->  0.200). Saving model.. | T_time: 00:03:55, T_speed: 0.085\n",
      "[Epoch  21] T_loss: 0.13986, T_accuracy: 95.6418 | V_loss: 0.21426, V_accuracy: 93.6200 | Early stopping counter: 1 out of 15 | T_time: 00:04:06, T_speed: 0.085\n",
      "[Epoch  22] T_loss: 0.13529, T_accuracy: 95.9345 | V_loss: 0.19958, V_accuracy: 93.6600 | V_loss decreased ( 0.200 -->  0.200). Saving model.. | T_time: 00:04:16, T_speed: 0.086\n",
      "[Epoch  23] T_loss: 0.12989, T_accuracy: 96.0491 | V_loss: 0.20169, V_accuracy: 94.1600 | Early stopping counter: 1 out of 15 | T_time: 00:04:27, T_speed: 0.086\n",
      "[Epoch  24] T_loss: 0.12511, T_accuracy: 96.2018 | V_loss: 0.20965, V_accuracy: 94.0800 | Early stopping counter: 2 out of 15 | T_time: 00:04:38, T_speed: 0.086\n",
      "[Epoch  25] T_loss: 0.11574, T_accuracy: 96.5200 | V_loss: 0.20466, V_accuracy: 94.2400 | Early stopping counter: 3 out of 15 | T_time: 00:04:49, T_speed: 0.087\n",
      "[Epoch  26] T_loss: 0.11290, T_accuracy: 96.5709 | V_loss: 0.19737, V_accuracy: 94.2600 | V_loss decreased ( 0.200 -->  0.197). Saving model.. | T_time: 00:05:00, T_speed: 0.087\n",
      "[Epoch  27] T_loss: 0.10532, T_accuracy: 96.8527 | V_loss: 0.20961, V_accuracy: 94.2400 | Early stopping counter: 1 out of 15 | T_time: 00:05:11, T_speed: 0.087\n",
      "[Epoch  28] T_loss: 0.10469, T_accuracy: 96.9127 | V_loss: 0.20570, V_accuracy: 94.1400 | Early stopping counter: 2 out of 15 | T_time: 00:05:22, T_speed: 0.087\n",
      "[Epoch  29] T_loss: 0.09993, T_accuracy: 97.0873 | V_loss: 0.20552, V_accuracy: 94.0800 | Early stopping counter: 3 out of 15 | T_time: 00:05:33, T_speed: 0.087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  30] T_loss: 0.09792, T_accuracy: 97.1927 | V_loss: 0.20912, V_accuracy: 93.9800 | Early stopping counter: 4 out of 15 | T_time: 00:05:44, T_speed: 0.087\n",
      "[Epoch  31] T_loss: 0.09512, T_accuracy: 97.3400 | V_loss: 0.20508, V_accuracy: 94.2600 | Early stopping counter: 5 out of 15 | T_time: 00:05:54, T_speed: 0.088\n",
      "[Epoch  32] T_loss: 0.09247, T_accuracy: 97.3818 | V_loss: 0.20972, V_accuracy: 93.8400 | Early stopping counter: 6 out of 15 | T_time: 00:06:05, T_speed: 0.088\n",
      "[Epoch  33] T_loss: 0.08774, T_accuracy: 97.5145 | V_loss: 0.20642, V_accuracy: 94.0000 | Early stopping counter: 7 out of 15 | T_time: 00:06:16, T_speed: 0.088\n",
      "[Epoch  34] T_loss: 0.08597, T_accuracy: 97.5327 | V_loss: 0.20592, V_accuracy: 94.1600 | Early stopping counter: 8 out of 15 | T_time: 00:06:27, T_speed: 0.088\n",
      "[Epoch  35] T_loss: 0.08400, T_accuracy: 97.6564 | V_loss: 0.20528, V_accuracy: 94.2000 | Early stopping counter: 9 out of 15 | T_time: 00:06:38, T_speed: 0.088\n",
      "[Epoch  36] T_loss: 0.08207, T_accuracy: 97.7273 | V_loss: 0.20905, V_accuracy: 94.1800 | Early stopping counter: 10 out of 15 | T_time: 00:06:49, T_speed: 0.088\n",
      "[Epoch  37] T_loss: 0.08067, T_accuracy: 97.6836 | V_loss: 0.21164, V_accuracy: 94.1800 | Early stopping counter: 11 out of 15 | T_time: 00:07:00, T_speed: 0.088\n",
      "[Epoch  38] T_loss: 0.07869, T_accuracy: 97.8036 | V_loss: 0.20807, V_accuracy: 94.1400 | Early stopping counter: 12 out of 15 | T_time: 00:07:11, T_speed: 0.088\n",
      "[Epoch  39] T_loss: 0.07804, T_accuracy: 97.8273 | V_loss: 0.21095, V_accuracy: 94.1200 | Early stopping counter: 13 out of 15 | T_time: 00:07:23, T_speed: 0.088\n",
      "[Epoch  40] T_loss: 0.07858, T_accuracy: 97.8109 | V_loss: 0.21808, V_accuracy: 93.9600 | Early stopping counter: 14 out of 15 | T_time: 00:07:34, T_speed: 0.088\n",
      "[Epoch  41] T_loss: 0.07679, T_accuracy: 97.8655 | V_loss: 0.21000, V_accuracy: 94.2200 | Early stopping counter: 15 out of 15*** TRAIN EARLY STOPPED! *** | T_time: 00:07:45, T_speed: 0.088\n",
      "Final training time: 00:07:45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Training accuracy (%)</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>Training loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▃▃▃▂▂▁▁▂▁▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>Validation accuracy (%)</td><td>▁▅▆▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>Validation loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>41</td></tr><tr><td>Training accuracy (%)</td><td>97.86545</td></tr><tr><td>Training loss</td><td>0.07679</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.08817</td></tr><tr><td>Validation accuracy (%)</td><td>94.22</td></tr><tr><td>Validation loss</td><td>0.21</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-11-23_10-57-21</strong> at: <a href='https://wandb.ai/tmdrjs0040/FashionMNIST/runs/894wpar6' target=\"_blank\">https://wandb.ai/tmdrjs0040/FashionMNIST/runs/894wpar6</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/FashionMNIST' target=\"_blank\">https://wandb.ai/tmdrjs0040/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241123_105721-894wpar6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    epochs = 1000\n",
    "    batch_size = 4096\n",
    "    validation_intervals = 1\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 15\n",
    "    early_stop_delta = 0.0001\n",
    "    weight_decay = 0.001\n",
    "    dropout = 0.5\n",
    "    main(epochs, batch_size, validation_intervals, learning_rate, early_stop_patience, early_stop_delta, weight_decay, dropout)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480ca5c",
   "metadata": {},
   "source": [
    "# 정리\n",
    "* 22와 23번 시도에서 원하는 성능(validation >=94, test>=93)을 얻음\n",
    "    - 22: validation_accuracy: 94.0000, test_accuracy: 93.220%\n",
    "    - 23: validation_accuracy: 94.2600, test_accuracy: 93.660%\n",
    "* 여러번의 시도를 하나의 main cell에서 시행해서 23의 훈련 과정의 출력이 나와았지만 22의 훈련 과정은 출력이 없음 -> 밑의 cell에 wandb에 기록된 결과를 가져옴\n",
    "* test과정은 개인 컴퓨터에서 진행\n",
    "    - 이유: Backend.AI에서 test 진행할 경우 세션이 종료\n",
    "    - 훈련과정만 이 노트북에 있고, 이후의 과정은 다른 노트북(homework3_test.ipynb) 에서 진행\n",
    "    \n",
    "-------------------------------------------------------------------------- \n",
    "# 하이퍼파라미터\n",
    "## 22\n",
    "    epochs = 1000\n",
    "    batch_size = 4096\n",
    "    validation_intervals = 5\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 5\n",
    "    early_stop_delta = 0.0001\n",
    "    weight_decay = 0.001\n",
    "    dropout = 0.5\n",
    "\n",
    "## 23\n",
    "    epochs = 1000\n",
    "    batch_size = 4096\n",
    "    validation_intervals = 1\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 15\n",
    "    early_stop_delta = 0.0001\n",
    "    weight_decay = 0.001\n",
    "    dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29ebf2",
   "metadata": {},
   "source": [
    "## 22번 훈련 과정(wandb 기록)\n",
    "     1 Training on device cuda:0.\n",
    "     2 Num Train Samples:  55000\n",
    "     3 Num Validation Samples:  5000\n",
    "     4 Sample Shape:  torch.Size([1, 28, 28])\n",
    "     5 Number of Data Loading Workers: 2\n",
    "     6 ==========================================================================================\n",
    "     7 Layer (type:depth-idx)                   Output Shape              Param #\n",
    "     8 ==========================================================================================\n",
    "     9 VGG                                      [1, 10]                   --\n",
    "    10 ├─Sequential: 1-1                        [1, 10]                   --\n",
    "    11 │    └─Sequential: 2-1                   [1, 128, 14, 14]          --\n",
    "    12 │    │    └─Conv2d: 3-1                  [1, 128, 28, 28]          1,280\n",
    "    13 │    │    └─BatchNorm2d: 3-2             [1, 128, 28, 28]          256\n",
    "    14 │    │    └─ReLU: 3-3                    [1, 128, 28, 28]          --\n",
    "    15 │    │    └─Conv2d: 3-4                  [1, 128, 28, 28]          147,584\n",
    "    16 │    │    └─BatchNorm2d: 3-5             [1, 128, 28, 28]          256\n",
    "    17 │    │    └─ReLU: 3-6                    [1, 128, 28, 28]          --\n",
    "    18 │    │    └─Conv2d: 3-7                  [1, 128, 28, 28]          147,584\n",
    "    19 │    │    └─BatchNorm2d: 3-8             [1, 128, 28, 28]          256\n",
    "    20 │    │    └─ReLU: 3-9                    [1, 128, 28, 28]          --\n",
    "    21 │    │    └─MaxPool2d: 3-10              [1, 128, 14, 14]          --\n",
    "    22 │    └─Sequential: 2-2                   [1, 256, 7, 7]            --\n",
    "    23 │    │    └─Conv2d: 3-11                 [1, 256, 14, 14]          295,168\n",
    "    24 │    │    └─BatchNorm2d: 3-12            [1, 256, 14, 14]          512\n",
    "    25 │    │    └─ReLU: 3-13                   [1, 256, 14, 14]          --\n",
    "    26 │    │    └─Conv2d: 3-14                 [1, 256, 14, 14]          590,080\n",
    "    27 │    │    └─BatchNorm2d: 3-15            [1, 256, 14, 14]          512\n",
    "    28 │    │    └─ReLU: 3-16                   [1, 256, 14, 14]          --\n",
    "    29 │    │    └─Conv2d: 3-17                 [1, 256, 14, 14]          590,080\n",
    "    30 │    │    └─BatchNorm2d: 3-18            [1, 256, 14, 14]          512\n",
    "    31 │    │    └─ReLU: 3-19                   [1, 256, 14, 14]          --\n",
    "    32 │    │    └─MaxPool2d: 3-20              [1, 256, 7, 7]            --\n",
    "    33 │    └─Flatten: 2-3                      [1, 12544]                --\n",
    "    34 │    └─Linear: 2-4                       [1, 256]                  3,211,520\n",
    "    35 │    └─ReLU: 2-5                         [1, 256]                  --\n",
    "    36 │    └─Dropout: 2-6                      [1, 256]                  --\n",
    "    37 │    └─Linear: 2-7                       [1, 128]                  32,896\n",
    "    38 │    └─ReLU: 2-8                         [1, 128]                  --\n",
    "    39 │    └─Linear: 2-9                       [1, 64]                   8,256\n",
    "    40 │    └─ReLU: 2-10                        [1, 64]                   --\n",
    "    41 │    └─Dropout: 2-11                     [1, 64]                   --\n",
    "    42 │    └─Linear: 2-12                      [1, 10]                   650\n",
    "    43 ==========================================================================================\n",
    "    44 Total params: 5,027,402\n",
    "    45 Trainable params: 5,027,402\n",
    "    46 Non-trainable params: 0\n",
    "    47 Total mult-adds (M): 524.84\n",
    "    48 ==========================================================================================\n",
    "    49 Input size (MB): 0.00\n",
    "    50 Forward/backward pass size (MB): 7.23\n",
    "    51 Params size (MB): 20.11\n",
    "    52 Estimated Total Size (MB): 27.34\n",
    "    53 ==========================================================================================\n",
    "    54 [Epoch   1] T_loss: 1.68430, T_accuracy: 36.5564 | V_loss: 0.94141, V_accuracy: 69.7800 | Early stopping is stated! | T_time: 00:00:15, T_speed: 0.067\n",
    "    55 [Epoch   5] T_loss: 0.42778, T_accuracy: 85.6600 | V_loss: 0.33866, V_accuracy: 89.5600 | V_loss decreased ( 0.941 -->  0.339). Saving model.. | T_time: 00:01:14, T_speed: 0.068"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20736b1",
   "metadata": {},
   "source": [
    "## 과정 기록\n",
    "----------------------------------------------------------------------\n",
    "# 1\n",
    "### 10_a,b 코드 거의 복붙\n",
    "#### optimizer = optim.SDG\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 10\n",
    "   * early_stop_delta = 0.0001\n",
    "  \n",
    "### 결과    \n",
    "   * T_loss: 0.44936, T_accuracy: 83.7200 | V_loss: 0.46497, V_accuracy: 83.1400  \n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_latest.pt\n",
    "   * TEST RESULTS: 82.980%\n",
    "\n",
    "\n",
    "# 2 \n",
    "### 1에서 optimizer를 Adam으로 교체\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 10\n",
    "   * early_stop_delta = 0.0001\n",
    "\n",
    "### 결과    \n",
    "   * T_loss: 0.12889, T_accuracy: 95.3745 | V_loss: 0.32309, V_accuracy: 90.0400 | Early stopping counter: 10 out of 10*** TRAIN EARLY STOPPED! *** | T_time: 00:09:07, T_speed: 0.384\n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_latest.pt\n",
    "   * TEST RESULTS: 89.700%\n",
    "\n",
    "\n",
    "# 3\n",
    "### 2에서 weight_decay 추가\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 10\n",
    "   * early_stop_delta = 0.0001\n",
    "   * weight_decay = 0.001\n",
    "   \n",
    "### 결과\n",
    "   * T_loss: 0.14863, T_accuracy: 94.7127 | V_loss: 0.28628, V_accuracy: 89.7200 | Early stopping counter: 10 out of 10*** TRAIN EARLY STOPPED! *** | T_time: 00:07:52, T_speed: 0.424\n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_latest.pt\n",
    "   * TEST RESULTS: 89.590%\n",
    "\n",
    "# 4\n",
    "### 2에서 dropout 추가\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 10\n",
    "   * early_stop_delta = 0.0001\n",
    "   * dropout(p = 0.5)\n",
    "\n",
    "### 결과\n",
    "   * T_loss: 0.29177, T_accuracy: 89.2764 | V_loss: 0.25406, V_accuracy: 90.9400 | Early stopping counter: 10 out of 10*** TRAIN EARLY STOPPED! *** | T_time: 00:29:59, T_speed: 0.434\n",
    "  \n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_2024-11-20_12-44-29_latest.pt\n",
    "   * TEST RESULTS: 90.520%\n",
    "  \n",
    "----------------------------------------------------------\n",
    "### 결과쪽에서 stop된 곳을 기준으로 하는 것이 아니라 Early stoppint counter가 올라가기 직전의 결과를 가져와야 맞는 것임을 생각 못하고 마지막 출력을 가져옴\n",
    "--------------------------------------------------------------\n",
    "\n",
    "# 5\n",
    "### 2에서 decay와 dropout 추가\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 10\n",
    "   * early_stop_delta = 0.0001\n",
    "   * weight_decay = 0.001\n",
    "   * dropout(p = 0.5)\n",
    "\n",
    "### 결과\n",
    "   * T_loss: 0.33893, T_accuracy: 87.8073 | V_loss: 0.26139, V_accuracy: 90.7000 | V_loss decreased ( 0.265 -->  0.261). Saving model.. | T_time: 00:18:25, T_speed: 0.434\n",
    "  \n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_2024-11-20_13-32-16_latest.pt\n",
    "   * TEST RESULTS: 89.440%\n",
    "  \n",
    "# 6\n",
    "### 5에서 Batch Normalization 추가\n",
    "### 하이퍼파라미터 동일\n",
    "   * T_loss: 0.33092, T_accuracy: 88.2436 | V_loss: 0.24330, V_accuracy: 90.6000 | V_loss decreased ( 0.244 -->  0.243). Saving model.. | T_time: 00:15:23, T_speed: 0.433\n",
    "\n",
    "### test  \n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_2024-11-20_14-15-44_latest.pt\n",
    "   * TEST RESULTS: 89.670%\n",
    " \n",
    "# 7\n",
    "### 6에서 하이퍼파라미터 조절해보기\n",
    "## 1\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 30\n",
    "   * early_stop_delta = 0.01\n",
    "   * weight_decay = 0.001\n",
    "   * dropout = 0.3\n",
    "\n",
    "### 결과\n",
    "   * T_loss: 0.25547, T_accuracy: 90.8582 | V_loss: 0.24847, V_accuracy: 90.9600 | V_loss decreased ( 0.260 -->  0.248). Saving model.. | T_time: 00:12:45, T_speed: 0.431\n",
    " \n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_2024-11-20_16-24-01_latest.pt\n",
    "   * TEST RESULTS: 90.090%\n",
    "# 8\n",
    "### vgg이용\n",
    "   * block_info = ((2, 64), (2, 128)), n_output = 10\n",
    "### 하이퍼파라미터\n",
    "### 결과\n",
    "   * T_loss: 0.24728, T_accuracy: 91.6636 | V_loss: 0.21479, V_accuracy: 92.8000 | V_loss decreased ( 0.258 -->  0.215). Saving model.. | T_time: 00:01:01, T_speed: 0.328\n",
    "### test\n",
    "   * Num Test Samples:  10000\n",
    "   * Sample Shape:  torch.Size([1, 28, 28])\n",
    "   * MODEL FILE: /home/work/DL/3/checkpoints/FashionMNIST_checkpoint_2024-11-21_07-33-02_latest.pt\n",
    "   * /usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
    "   * warnings.warn('Lazy modules are a new feature under heavy development '\n",
    "   \n",
    "# 9\n",
    "### 8에서 learning rate decay(lr_scheduler.StepLR)\n",
    "### 하이퍼파라미터\n",
    "   * epochs = 1000\n",
    "   * batch_size = 2048\n",
    "   * validation_intervals = 10\n",
    "   * learning_rate = 1e-3\n",
    "   * early_stop_patience = 30\n",
    "   * early_stop_delta = 0.0001\n",
    "   * weight_decay = 0.001\n",
    "   * dropout = 0.5\n",
    "### 결과\n",
    "   * T_loss: 0.26904, T_accuracy: 91.0345 | V_loss: 0.25182, V_accuracy: 91.3400 | V_loss decreased ( 0.252 -->  0.252). Saving model.. | T_time: 00:02:29, T_speed: 0.336\n",
    "### 테스트\n",
    "   * The kernel appears to have died. It will restart automatically.\n",
    "\n",
    "# 10\n",
    "### 9에서 LazyConv에서 일반 Conv로 변경해서 실행\n",
    "   * The kernel appears to have died. It will restart automatically.\n",
    "   * vgg자체가 문제?\n",
    "   \n",
    "# 11\n",
    "### 10에서 Learning rate decay 제거\n",
    "\n",
    "   * T_loss: 0.20541, T_accuracy: 93.0745 | V_loss: 0.21959, V_accuracy: 92.7800 | V_loss decreased ( 0.235 -->  0.220). Saving model.. | T_time: 00:01:30, T_speed: 0.333\n",
    "   * 똑같이 test가 안나옴\n",
    "   * learning rate decay 문제도 아니고, LazyConv도 아니고... \n",
    "   * 뤼튼에 'The kernel appears to have died. It will restart automatically.' 검색 결과 메모리의 문제일 수있다 해서 colab에서 진행해봄\n",
    "# 12\n",
    "### colab으로 이동해서 실행\n",
    "   * T_loss: 0.16558, T_accuracy: 94.3345 | V_loss: 0.21178, V_accuracy: 93.2400 | V_loss decreased ( 0.213 -->  0.212). Saving model.. | T_time: 00:04:56, T_speed: 0.135\n",
    "   * TEST RESULTS: 92.680%\n",
    "   * colab에서는 무리 없이 되는 것으로 보아 메모리 문제일 확률이 높아 보임\n",
    "# 13\n",
    "### 12에서 learning rate decay추가 단 eraly stop count가 있을 경우에만 count해서 \n",
    "   * T_loss: 0.13224, T_accuracy: 95.3345 | V_loss: 0.20729, V_accuracy: 93.5800 | V_loss decreased ( 0.210 -->  0.207). Saving model.. | T_time: 00:07:29, T_speed: 0.134\n",
    "   * TEST RESULTS: 92.770% \n",
    "   * 성능은 좋아졌지만 원하는 성능에는 부족\n",
    "   * learning rate decay를 validation을 확인하는 위치를 기준으로 step하기, 하이퍼파라미터만 변경해보기\n",
    "   * 여태 확인한 것으로 early_stop_patience이 커 봤자 train_loss만 더 작아지는 overfitting이 확인되고, validation_loss는 작아지지 않음 -> 처음 있던 10까지만 확인 해도 좋을 것 으로 보인다.\n",
    "# 14\n",
    "### learning rate decay 위치 do_validation 다음으로 변경\n",
    "   * T_loss: 0.17335, T_accuracy: 94.0309 | V_loss: 0.20633, V_accuracy: 93.1600 | V_loss decreased ( 0.221 -->  0.206). Saving model.. | T_time: 00:04:59, T_speed: 0.134\n",
    "   * TEST RESULTS: 92.640%\n",
    "   * 마찬가지로 early_stop_patience을 그대로 둔 상태로 상황을 보니 overfitting 상황발생-> 시간만 잡아먹는다. -> 다음부터는 줄여서 시행\n",
    "   * 1000이전에 early_stop이 발생하므로 epochs을 높일 필요는 없어 보임\n",
    "     \n",
    "# 15\n",
    "### 하이퍼파라미터 조절\n",
    "#### dropout =0.5 -> 0.3 (이전까지는 그냥 모델에 0.5로 박은상태로 진행해서 wandb에 잘못 적힘)\n",
    "#### weight_decay = 0.001 -> 0.01\n",
    "#### early_stop_patience = 30 -> 5\n",
    "   * T_loss: 0.28774, T_accuracy: 90.7218 | V_loss: 0.26197, V_accuracy: 90.3800 | V_loss decreased ( 0.262 -->  0.262). Saving model.. | T_time: 00:53:35, T_speed: 0.134\n",
    "   * TEST RESULTS: 90.060%\n",
    "\n",
    "   * 학습 효과가 이전보다 좋지 못함 하지만, 한번 early stop count가 한 이후 거의 끝나는 이전과 달리 count했다가 validation이 줄어들고가 많이 반복됨 -> dropout을 다시 0.5로 올려서 dropout과 weight_decay중 어느 것이 영향을 주었는지 확인\n",
    "   \n",
    "# 16\n",
    "### dropout 0.5로 상승\n",
    "### colab 사용시간으로 다시 학교 backAI로 돌아옴\n",
    "   * T_loss: 0.27838, T_accuracy: 90.9309 | V_loss: 0.25139, V_accuracy: 90.9600 | V_loss decreased ( 0.252 -->  0.251). Saving model.. | T_time: 00:22:53, T_speed: 0.335\n",
    "   * TEST RESULTS: 90.270%\n",
    "   * 별 차이 없음 -> weight_decay가 높아져서 성능이 낮아졌다. 하지만, Train과 validation의 차이는 줄어듬 -> 일반화 성능이 올랐다라고 봐도 되나?\n",
    "\n",
    "# 17\n",
    "### 모델 변경 -> vgg블록 추가해서\n",
    "   *  T_loss: 2.30260, T_accuracy: 10.0055 | V_loss: 2.30268, V_accuracy: 9.9400 | V_loss decreased ( 2.304 -->  2.303). Saving model.. | T_time: 00:00:32, T_speed: 0.312\n",
    "   * 시작부터 2점대의 Loss로 시작 -> 모델 구조는 기존것이 더 좋아 보임\n",
    "   \n",
    "# 18\n",
    "### 모델에 vgg에 batch norm추가\n",
    "   * T_loss: 0.12093, T_accuracy: 96.8564 | V_loss: 0.20334, V_accuracy: 93.1200 | V_loss decreased ( 0.208 -->  0.203). Saving model.. | T_time: 00:07:04, T_speed: 0.165\n",
    "   * TEST RESULTS: 92.300%\n",
    "\n",
    "# 19\n",
    "### 모델 구조 변경. flatten 이후를 변경\n",
    "   * T_loss: 0.16988, T_accuracy: 95.0764 | V_loss: 0.21688, V_accuracy: 92.5600 | V_loss decreased ( 0.252 -->  0.217). Saving model.. | T_time: 00:05:50, T_speed: 0.143\n",
    "   \n",
    "# 20\n",
    "### batch 를 4096으로 늘림, weight_decay를 0.001로 돌림\n",
    "   * T_loss: 0.17377, T_accuracy: 94.3182 | V_loss: 0.20258, V_accuracy: 92.8400 | V_loss decreased ( 0.241 -->  0.203). Saving model.. | T_time: 00:03:00, T_speed: 0.111\n",
    "   * TEST RESULTS: 92.620%\n",
    "   \n",
    "# 21\n",
    "### VGG BLOCK의 CONV개수를 3개로\n",
    "   * T_loss: 0.16957, T_accuracy: 94.4545 | V_loss: 0.20401, V_accuracy: 92.2400 | V_loss decreased ( 0.220 -->  0.204). Saving model.. | T_time: 00:04:39, T_speed: 0.072\n",
    "   * early stop이 count된 다음에 intervals 10이 의미가 있는가?\n",
    "   * intervals를 낮춰보자\n",
    "\n",
    "# 22\n",
    "### intervals를 5로 낮춤\n",
    "   * T_loss: 0.12988, T_accuracy: 95.9182 | V_loss: 0.18195, V_accuracy: 94.0000 | V_loss decreased ( 0.200 -->  0.182). Saving model.. | T_time: 00:06:09, T_speed: 0.068\n",
    "   * TEST RESULTS: 93.220%\n",
    "   \n",
    "# 23\n",
    "### intervals를 1로 낮추는 대신에 early stop을 15로 늘림\n",
    "   * T_loss: 0.11290, T_accuracy: 96.5709 | V_loss: 0.19737, V_accuracy: 94.2600 | V_loss decreased ( 0.200 -->  0.197). Saving model.. | T_time: 00:05:00, T_speed: 0.087\n",
    "   * TEST RESULTS: 93.660%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a95be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3 (NGC 24.03/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
