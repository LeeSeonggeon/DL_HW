{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "+# hw2 - titanic ",
   "id": "269566f7987f7e1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.100180Z",
     "start_time": "2024-10-25T02:12:51.823165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "id": "589e4985c6360f46",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.108253Z",
     "start_time": "2024-10-25T02:12:57.103176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_data를 위한 데이터셋 클래스 정의 -> feature와 target 모두 정의하고 반환한다.\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.LongTensor(y.values)\n",
    "    \n",
    "    #데이터 셋의 크기(가지고 있는 데이터(샘플)의 개수)\n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    \n",
    "    #인덱스 입력시 출력되는 값들을 정의\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return {'input':feature, 'target':target} #튜플\n",
    "    \n",
    "    #dataset의 정보를 출력해준다.\n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(len(self.X), self.X.shape, self.y.shape)\n",
    "        return str\n",
    "        "
   ],
   "id": "c086e574a1e3cb2e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.115440Z",
     "start_time": "2024-10-25T02:12:57.109254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_data를 위한 데이터셋 클래스 정의 -> feature만을 정의하고 반환한다.\n",
    "class TitanicTestDataset(Dataset): \n",
    "    def __init__(self, X): \n",
    "        self.X = torch.FloatTensor(X.values)  \n",
    "    def __len__(self): \n",
    "        return len(self.X)  \n",
    "    def __getitem__(self, idx):  \n",
    "        feature = self.X[idx] \n",
    "        return {'input': feature}  \n",
    "    def __str__(self): \n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(  len(self.X), self.X.shape )  \n",
    "        return str"
   ],
   "id": "6b374cade8ff50be",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test data를 다루기 위한 데이터셋으로 feature만 나오게 되어있다.",
   "id": "72b3b6b49a44a875"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.125495Z",
     "start_time": "2024-10-25T02:12:57.116440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_preprocessed_dataset():\n",
    "    #CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__)) #dirname: path의 디렉토리 부분만 가져오기,abspath: 현재 파일의 절대 경로가져오기\n",
    "    CURRENT_FILE_PATH = os.getcwd() # ipynb에서 __file__이 정의되어있지 않음으로 현재 작업 디렉토리를 가져오는 getcwd를 이용\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, 'train.csv')\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, 'test.csv')\n",
    "    \n",
    "    train_df = pd.read_csv(train_data_path)# train data load\n",
    "    test_df = pd.read_csv(test_data_path) # test data load\n",
    "    \n",
    "    #train_df와 test_df 결합\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    #전처리 함수 (전처리 하고 len(train_df)와 len(test_df)를 이용해 다시 분할해야 train, val, test 분할이 가능하다.) train_data와 test_data의 개수로 봤을때 val을 구분하는게 맞는가?\n",
    "    #전처리 1: 데이터에서 승객번호, 이름, 티켓번호, 방 호수 삭제\n",
    "    #전처리 2: 적절한 타입으로 변경(feature는 float으로 target은 int64로 변경해야 한다.)\n",
    "    #전처리 3: NaN값을 가지고 있는 Age에 대한 처리\n",
    "    #전처리 4: 데이터를 train과 test 분리\n",
    "    #전처리 5: dataset 클래스에 데이터 저장\n",
    "    #전처리 6: train_dataset에 random_split을 적용해서 train과 val로 분할\n",
    "\n",
    "    #전처리 1 - 원하는 열만 남기기\n",
    "    all_df = all_df[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\", \"Embarked\" ]]\n",
    "    \n",
    "    #전처리 2 - Dataset클래스에 맞게 데이터 타입 변경\n",
    "    #2-1 object타입인 Sex, Embarked를 One-Hot Encoding\n",
    "    all_df = pd.get_dummies(all_df, columns = [\"Sex\", \"Embarked\"], drop_first=True) # drop_first: 첫번째 카테고리를 제고한다. -> 하나를 삭제하더라도 나머지가 False이면 당연히 삭제된 카테고리가 True이므로 삭제해도 문제 없다. 검색에서는 다중 공선성을 방지한다 하지만 아직 이해가 안된다. \n",
    "    #2-2 int64타입인 Pclass, SibSp, Parch을 float으로 변경\n",
    "    all_df[[\"Pclass\", \"SibSp\", \"Parch\"]] = all_df[[\"Pclass\", \"SibSp\", \"Parch\"]].astype(float)\n",
    "    #2-3 One-Hot Encoding으로 bool 타입이된 Sex와 Embarked를 float으로 변경한다.\n",
    "    all_df[[\"Sex_male\", \"Embarked_Q\", \"Embarked_S\"]] = all_df[[\"Sex_male\", \"Embarked_Q\", \"Embarked_S\"]].astype(float)\n",
    "    \n",
    "    #전처리 3 - NaN값 이 있는 경우 해당 값을 평균값으로 대체\n",
    "    all_df.fillna(all_df.mean(), inplace=True)#fillna: NaN값을 대체한다. 이 경우 각 열의 평균값으로 채우는 것이다. inplace는 원본을 수정할지 여부를 정하는 것으로 False이면 새로운 DataFrame을 반환한다.\n",
    "    \n",
    "    #전처리 4 - all_df를 train과 test로 분리\n",
    "    train_df = all_df.iloc[:len(train_df),] \n",
    "    test_df = all_df.iloc[len(train_df):,] \n",
    "    \n",
    "    #전처리 5 - 적절한 dataset 클래스에 데이터 저장\n",
    "    #5-1: 데이터를 target과 feature로 분리\n",
    "    train_df_target = train_df[[\"Survived\"]]\n",
    "    train_df_feature = train_df.drop([\"Survived\"], axis=1)\n",
    "    test_df_feature = test_df.drop([\"Survived\"], axis=1, errors='ignore')\n",
    "    #5-2: 데이터를 TitanicDataset에 저장\n",
    "    train_dataset = TitanicDataset(train_df_feature, train_df_target)\n",
    "    test_dataset = TitanicTestDataset(test_df_feature)\n",
    "    \n",
    "    #전처리 6 - random_split을 이용한 train과 validation 분리\n",
    "    train_dataset, validation_dataset = random_split(train_dataset,[0.8,0.2])\n",
    "    \n",
    "    return train_dataset, validation_dataset, test_dataset"
   ],
   "id": "fa4d2d6731f1c170",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "feature, target 정리\n",
    "- target: Sruvived\n",
    "- feature: Pclass, Sex_male, Age, SibSp, Parch, Fare, Embarked_Q, Embarked_S\n"
   ],
   "id": "3731a0a8818c3d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.132594Z",
     "start_time": "2024-10-25T02:12:57.126808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential( #안의 것을 순차적으로 실행\n",
    "            nn.Linear(n_input, 30), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, n_output),\n",
    "        ) # hidden은 뉴런 30짜리 2개, 활성화 함수는 ReLU 사용\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "id": "bf226e364595297",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:57.572205Z",
     "start_time": "2024-10-25T02:12:57.135582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    #타이타닉 데이터 로드\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "    print(\"train_dataset: {0}, validation_dataset: {1}, test_dataset: {2}\".format(len(train_dataset), len(validation_dataset), len(test_dataset)))\n",
    "    print(\"#\"*50, 1)\n",
    "    \n",
    "    #train 데이터를 \"(인덱스) - (feature): (target)\" 형태로 전체 출력하기 \n",
    "    for idx, sample in enumerate(train_dataset):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "    print(\"#\"*50, 2)\n",
    "    \n",
    "    #데이터셋을 DataLoader에 batch_size와 shuffle 유무를 설정해 저장하기\n",
    "    train_data_loader = DataLoader(dataset = train_dataset, batch_size = 16, shuffle = True)\n",
    "    validation_data_loader = DataLoader(dataset = validation_dataset, batch_size = 16, shuffle = True)\n",
    "    test_data_loader = DataLoader(dataset = test_dataset, batch_size = len(test_dataset))\n",
    "    \n",
    "    print(\"[TRAIN]\")\n",
    "    #배치 단위로 idx와 feature의 구조, target 구조를 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "    print(\"[VALIDATION]\")\n",
    "    for idx, batch in enumerate(validation_data_loader):\n",
    "        print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "        \n",
    "    print(\"#\" * 50, 3)\n",
    "    \n",
    "    print(\"[TEST]\")\n",
    "    #test_data_Loader의 데이터를 가져와 my_model에 입력해 예측 결과 출력하기\n",
    "    #현재 model의 학습이 안된 상태이므로 학습하는 코드의 추가가 필요\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    my_model = MyModel(n_input=8, n_output=2)\n",
    "    output_batch = my_model(batch['input'])\n",
    "    prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "    for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "        print(idx, prediction.item())"
   ],
   "id": "943e60f16c5f701d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 713, validation_dataset: 178, test_dataset: 418\n",
      "################################################## 1\n",
      "0 - tensor([ 3.0000,  5.0000,  0.0000,  0.0000, 12.4750,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "1 - tensor([ 1.0000, 65.0000,  0.0000,  1.0000, 61.9792,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "2 - tensor([ 3.0000, 33.0000,  3.0000,  0.0000, 15.8500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "3 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8292,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "4 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "5 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  9.8375,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "6 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "7 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "8 - tensor([ 2., 52.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "9 - tensor([ 1.0000, 30.0000,  0.0000,  0.0000, 86.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "10 - tensor([ 3.0000, 38.0000,  1.0000,  5.0000, 31.3875,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "11 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "12 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "13 - tensor([ 3.0000, 25.0000,  1.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "14 - tensor([ 1.0000, 46.0000,  1.0000,  0.0000, 61.1750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "15 - tensor([  1.0000,  29.0000,   0.0000,   0.0000, 211.3375,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "16 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "17 - tensor([ 2.0000,  0.8300,  0.0000,  2.0000, 29.0000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "18 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "19 - tensor([ 1., 64.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "20 - tensor([ 2.,  4.,  2.,  1., 39.,  0.,  0.,  1.]): tensor([1])\n",
      "21 - tensor([ 3.0000,  2.0000,  4.0000,  1.0000, 29.1250,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "22 - tensor([ 3., 19.,  0.,  0.,  0.,  1.,  0.,  1.]): tensor([0])\n",
      "23 - tensor([ 2., 30.,  1.,  0., 24.,  1.,  0.,  0.]): tensor([0])\n",
      "24 - tensor([ 3.0000, 17.0000,  1.0000,  1.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "25 - tensor([ 1.0000, 27.0000,  0.0000,  0.0000, 30.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "26 - tensor([ 3.0000, 17.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "27 - tensor([  1.0000,  50.0000,   2.0000,   0.0000, 133.6500,   1.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "28 - tensor([ 2., 23.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "29 - tensor([ 3.0000, 16.0000,  1.0000,  3.0000, 34.3750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "30 - tensor([ 2.0000, 28.0000,  0.0000,  0.0000, 12.6500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "31 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "32 - tensor([ 2.0000, 29.0000,  1.0000,  0.0000, 27.7208,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "33 - tensor([ 3.0000,  2.0000,  3.0000,  2.0000, 27.9000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "34 - tensor([ 3.0000,  4.0000,  4.0000,  1.0000, 29.1250,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "35 - tensor([ 2.0000, 31.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "36 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 79.2000,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "37 - tensor([ 3.0000, 39.0000,  1.0000,  5.0000, 31.2750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "38 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.9250,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "39 - tensor([ 2., 25.,  0.,  1., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "40 - tensor([ 3.0000, 15.0000,  0.0000,  0.0000,  8.0292,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "41 - tensor([  1.0000,  36.0000,   0.0000,   1.0000, 512.3292,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "42 - tensor([ 2., 24.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "43 - tensor([ 2., 28.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "44 - tensor([  1.0000,  58.0000,   0.0000,   1.0000, 153.4625,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "45 - tensor([ 1.0000, 54.0000,  0.0000,  0.0000, 51.8625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "46 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "47 - tensor([ 3.0000, 29.8811,  8.0000,  2.0000, 69.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "48 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7333,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "49 - tensor([ 3.0000, 23.5000,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "50 - tensor([ 1.0000, 25.0000,  1.0000,  0.0000, 55.4417,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "51 - tensor([ 2., 39.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "52 - tensor([ 1.0000, 30.0000,  0.0000,  0.0000, 56.9292,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "53 - tensor([ 3.0000, 31.0000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "54 - tensor([ 3.0000, 45.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "55 - tensor([ 2.,  4.,  1.,  1., 23.,  0.,  0.,  1.]): tensor([1])\n",
      "56 - tensor([ 1.0000, 21.0000,  0.0000,  1.0000, 77.2875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "57 - tensor([ 3.0000, 51.0000,  0.0000,  0.0000,  7.7500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "58 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "59 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  7.8750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "60 - tensor([ 3.0000, 24.0000,  0.0000,  2.0000, 16.7000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "61 - tensor([ 3.0000, 61.0000,  0.0000,  0.0000,  6.2375,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "62 - tensor([ 3.0000, 14.0000,  5.0000,  2.0000, 46.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "63 - tensor([ 2., 30.,  3.,  0., 21.,  0.,  0.,  1.]): tensor([1])\n",
      "64 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.7750,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "65 - tensor([ 1.0000, 36.0000,  0.0000,  0.0000, 26.3875,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "66 - tensor([ 1.0000, 56.0000,  0.0000,  1.0000, 83.1583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "67 - tensor([ 2., 34.,  1.,  0., 21.,  1.,  0.,  1.]): tensor([0])\n",
      "68 - tensor([ 3.0000, 50.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "69 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 15.5000,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "70 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "71 - tensor([ 1.0000, 29.0000,  1.0000,  0.0000, 66.6000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "72 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 25.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "73 - tensor([ 1.0000, 35.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "74 - tensor([ 2.0000, 33.0000,  1.0000,  2.0000, 27.7500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "75 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "76 - tensor([ 3.0000, 18.0000,  1.0000,  1.0000, 20.2125,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "77 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "78 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.7125,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "79 - tensor([ 2., 35.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "80 - tensor([ 2.0000, 29.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "81 - tensor([ 2., 36.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "82 - tensor([ 3.0000, 36.0000,  1.0000,  0.0000, 15.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "83 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "84 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "85 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "86 - tensor([ 1.0000, 34.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "87 - tensor([ 3.0000,  1.0000,  4.0000,  1.0000, 39.6875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "88 - tensor([ 2., 54.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "89 - tensor([ 3.0000, 40.0000,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "90 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  4.0125,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "91 - tensor([ 1.0000, 49.0000,  1.0000,  0.0000, 56.9292,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "92 - tensor([ 3., 31.,  1.,  0., 18.,  0.,  0.,  1.]): tensor([0])\n",
      "93 - tensor([ 2., 32.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([1])\n",
      "94 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "95 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "96 - tensor([ 3.0000, 24.0000,  1.0000,  0.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "97 - tensor([ 1., 35.,  1.,  0., 52.,  0.,  0.,  1.]): tensor([1])\n",
      "98 - tensor([ 1.0000, 45.0000,  0.0000,  0.0000, 35.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "99 - tensor([ 3.0000, 25.0000,  1.0000,  0.0000, 17.8000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "100 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  8.6625,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "101 - tensor([ 2.0000, 23.0000,  0.0000,  0.0000, 13.7917,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "102 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "103 - tensor([ 3.0000, 26.0000,  1.0000,  0.0000, 14.4542,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "104 - tensor([  1.0000,  29.8811,   1.0000,   0.0000, 133.6500,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "105 - tensor([ 1., 33.,  0.,  0.,  5.,  1.,  0.,  1.]): tensor([0])\n",
      "106 - tensor([ 3.0000, 26.0000,  1.0000,  2.0000, 20.5750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "107 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "108 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 19.9667,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "109 - tensor([ 2., 36.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "110 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "111 - tensor([ 3.0000, 27.0000,  0.0000,  2.0000, 11.1333,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "112 - tensor([ 1.0000, 45.0000,  1.0000,  0.0000, 83.4750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "113 - tensor([ 2., 23.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "114 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  8.6625,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "115 - tensor([ 1., 47.,  0.,  0., 52.,  1.,  0.,  1.]): tensor([0])\n",
      "116 - tensor([ 2., 16.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "117 - tensor([ 3.0000,  7.0000,  4.0000,  1.0000, 39.6875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "118 - tensor([  1.0000,  43.0000,   0.0000,   1.0000, 211.3375,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "119 - tensor([ 2.0000, 30.0000,  0.0000,  0.0000, 12.3500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "120 - tensor([ 1.0000, 52.0000,  0.0000,  0.0000, 30.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "121 - tensor([  1.0000,  21.0000,   2.0000,   2.0000, 262.3750,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "122 - tensor([ 2.0000, 36.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "123 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "124 - tensor([ 3.0000, 29.8811,  0.0000,  2.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "125 - tensor([ 3.0000, 33.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "126 - tensor([ 3.0000, 30.0000,  1.0000,  1.0000, 24.1500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "127 - tensor([ 3.0000, 17.0000,  0.0000,  0.0000,  7.1250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "128 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "129 - tensor([ 3.0000, 17.0000,  1.0000,  0.0000,  7.0542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "130 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 15.5000,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "131 - tensor([ 2.0000, 51.0000,  0.0000,  0.0000, 12.5250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "132 - tensor([ 3.0000, 18.0000,  0.0000,  1.0000,  9.3500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "133 - tensor([ 3.0000, 63.0000,  0.0000,  0.0000,  9.5875,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "134 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "135 - tensor([ 2., 40.,  1.,  1., 39.,  0.,  0.,  1.]): tensor([1])\n",
      "136 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.8542,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "137 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "138 - tensor([ 1.0000, 45.5000,  0.0000,  0.0000, 28.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "139 - tensor([ 2.0000, 28.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "140 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 14.4542,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "141 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "142 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "143 - tensor([ 3.0000,  9.0000,  2.0000,  2.0000, 34.3750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "144 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  8.4333,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "145 - tensor([ 2., 28.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "146 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "147 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "148 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  7.7958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "149 - tensor([ 3.0000, 24.5000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "150 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "151 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  7.4958,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "152 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  7.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "153 - tensor([ 3.0000, 14.0000,  0.0000,  0.0000,  7.8542,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "154 - tensor([ 1.0000, 60.0000,  1.0000,  0.0000, 75.2500,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "155 - tensor([ 2., 34.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "156 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  7.7333,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "157 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "158 - tensor([ 3.0000, 32.0000,  1.0000,  1.0000, 15.5000,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "159 - tensor([ 3.0000,  8.0000,  3.0000,  1.0000, 21.0750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "160 - tensor([ 1.0000, 19.0000,  1.0000,  0.0000, 91.0792,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "161 - tensor([ 3.0000, 43.0000,  0.0000,  0.0000,  6.4500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "162 - tensor([ 3.0000, 25.0000,  1.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "163 - tensor([ 3.0000, 26.0000,  2.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "164 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7375,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "165 - tensor([ 1.0000, 52.0000,  1.0000,  1.0000, 79.6500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "166 - tensor([ 3.0000, 36.0000,  1.0000,  0.0000, 17.4000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "167 - tensor([ 1.0000, 48.0000,  0.0000,  0.0000, 25.9292,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "168 - tensor([ 1.0000, 35.0000,  1.0000,  0.0000, 83.4750,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "169 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "170 - tensor([ 3.0000, 43.0000,  1.0000,  6.0000, 46.9000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "171 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.6500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "172 - tensor([ 2., 42.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([1])\n",
      "173 - tensor([ 2., 31.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([1])\n",
      "174 - tensor([ 2., 30.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "175 - tensor([ 1.0000, 61.0000,  0.0000,  0.0000, 32.3208,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "176 - tensor([ 3.0000, 36.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "177 - tensor([ 1.0000, 24.0000,  0.0000,  0.0000, 69.3000,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "178 - tensor([ 2., 42.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "179 - tensor([ 3.0000, 24.0000,  2.0000,  0.0000, 24.1500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "180 - tensor([ 2.0000, 32.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "181 - tensor([ 3.0000,  0.7500,  2.0000,  1.0000, 19.2583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "182 - tensor([ 1.0000, 56.0000,  0.0000,  0.0000, 30.6958,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "183 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "184 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "185 - tensor([ 1.0000, 44.0000,  0.0000,  0.0000, 27.7208,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "186 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.4583,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "187 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 30.6958,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "188 - tensor([ 3.0000, 42.0000,  0.0000,  0.0000,  7.6500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "189 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000, 14.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "190 - tensor([ 3.0000, 14.0000,  4.0000,  1.0000, 39.6875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "191 - tensor([ 2.0000, 24.0000,  0.0000,  2.0000, 14.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "192 - tensor([ 3.0000, 30.5000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "193 - tensor([ 1.0000, 21.0000,  0.0000,  0.0000, 77.9583,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "194 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.7958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "195 - tensor([  1.0000,  42.0000,   0.0000,   0.0000, 227.5250,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "196 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([1])\n",
      "197 - tensor([ 3.0000, 36.0000,  0.0000,  0.0000,  7.4958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "198 - tensor([ 2., 25.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "199 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "200 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "201 - tensor([ 3.0000, 40.0000,  1.0000,  0.0000,  9.4750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "202 - tensor([ 1.0000, 32.0000,  0.0000,  0.0000, 30.5000,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "203 - tensor([ 2.0000, 19.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "204 - tensor([ 2., 39.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "205 - tensor([ 1.0000, 28.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "206 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "207 - tensor([ 2.0000, 50.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "208 - tensor([ 1., 38.,  0.,  0., 80.,  0.,  0.,  0.]): tensor([1])\n",
      "209 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "210 - tensor([ 3.0000,  8.0000,  4.0000,  1.0000, 29.1250,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "211 - tensor([ 1.0000, 51.0000,  1.0000,  0.0000, 77.9583,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "212 - tensor([ 1.0000, 54.0000,  1.0000,  0.0000, 59.4000,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "213 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "214 - tensor([ 1.0000, 30.0000,  0.0000,  0.0000, 27.7500,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "215 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "216 - tensor([ 3.0000, 29.8811,  3.0000,  1.0000, 25.4667,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "217 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "218 - tensor([ 1., 26.,  0.,  0., 30.,  1.,  0.,  0.]): tensor([1])\n",
      "219 - tensor([ 3.0000, 39.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "220 - tensor([  1.0000,  29.8811,   0.0000,   0.0000, 221.7792,   1.0000,   0.0000,\n",
      "          1.0000]): tensor([0])\n",
      "221 - tensor([ 2., 33.,  0.,  2., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "222 - tensor([ 2., 48.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "223 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "224 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  9.8458,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "225 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "226 - tensor([ 3.0000, 29.0000,  1.0000,  0.0000,  7.0458,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "227 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "228 - tensor([ 3.0000, 14.0000,  1.0000,  0.0000, 11.2417,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "229 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.1125,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "230 - tensor([3.0000, 0.4200, 0.0000, 1.0000, 8.5167, 1.0000, 0.0000, 0.0000]): tensor([1])\n",
      "231 - tensor([ 2., 30.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "232 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "233 - tensor([  1.0000,   0.9200,   1.0000,   2.0000, 151.5500,   1.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "234 - tensor([ 2.0000, 28.0000,  0.0000,  0.0000, 13.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "235 - tensor([  1.,  14.,   1.,   2., 120.,   0.,   0.,   1.]): tensor([1])\n",
      "236 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "237 - tensor([ 1.0000, 42.0000,  0.0000,  0.0000, 26.2875,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "238 - tensor([ 2., 42.,  1.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "239 - tensor([ 3.0000, 29.8811,  3.0000,  1.0000, 25.4667,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "240 - tensor([ 3.0000, 10.0000,  3.0000,  2.0000, 27.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "241 - tensor([ 1.0000, 33.0000,  1.0000,  0.0000, 53.1000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "242 - tensor([ 1., 22.,  0.,  1., 55.,  0.,  0.,  1.]): tensor([1])\n",
      "243 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "244 - tensor([ 2.0000, 32.5000,  0.0000,  0.0000, 13.0000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "245 - tensor([ 3.0000, 35.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "246 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "247 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8792,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "248 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "249 - tensor([ 3.0000,  1.0000,  5.0000,  2.0000, 46.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "250 - tensor([ 3.0000, 18.0000,  0.0000,  1.0000, 14.4542,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "251 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 24.1500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "252 - tensor([ 2., 24.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([0])\n",
      "253 - tensor([ 3.0000, 29.8811,  1.0000,  1.0000, 15.2458,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "254 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 31.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "255 - tensor([ 2., 54.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "256 - tensor([ 2.0000, 33.0000,  0.0000,  0.0000, 12.2750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "257 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8792,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "258 - tensor([  1.,  23.,   3.,   2., 263.,   0.,   0.,   1.]): tensor([1])\n",
      "259 - tensor([ 3.0000, 21.0000,  1.0000,  0.0000,  9.8250,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "260 - tensor([ 3.0000,  5.0000,  4.0000,  2.0000, 31.3875,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "261 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "262 - tensor([ 1.0000, 45.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "263 - tensor([ 2.0000, 66.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "264 - tensor([ 2., 24.,  1.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "265 - tensor([ 1.0000, 32.0000,  0.0000,  0.0000, 76.2917,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "266 - tensor([ 2.0000, 18.0000,  0.0000,  0.0000, 73.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "267 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "268 - tensor([ 1.0000, 51.0000,  0.0000,  1.0000, 61.3792,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "269 - tensor([ 3.0000,  4.0000,  3.0000,  2.0000, 27.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "270 - tensor([ 2.0000, 32.5000,  1.0000,  0.0000, 30.0708,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "271 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "272 - tensor([ 3.0000, 20.0000,  1.0000,  1.0000, 15.7417,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "273 - tensor([ 3.0000, 27.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "274 - tensor([  1.0000,  50.0000,   0.0000,   1.0000, 247.5208,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "275 - tensor([ 3.0000, 29.0000,  1.0000,  1.0000, 10.4625,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "276 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "277 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "278 - tensor([ 2., 60.,  1.,  1., 39.,  1.,  0.,  1.]): tensor([0])\n",
      "279 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  6.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "280 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 24.1500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "281 - tensor([ 1., 30.,  0.,  0., 31.,  0.,  0.,  0.]): tensor([1])\n",
      "282 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000, 18.7875,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "283 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "284 - tensor([ 2.0000,  5.0000,  1.0000,  2.0000, 27.7500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "285 - tensor([  1.0000,  35.0000,   0.0000,   0.0000, 512.3292,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "286 - tensor([ 2., 26.,  1.,  1., 26.,  0.,  0.,  1.]): tensor([0])\n",
      "287 - tensor([ 3.0000, 20.5000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "288 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "289 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 15.5000,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "290 - tensor([ 2., 40.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "291 - tensor([ 3.0000, 59.0000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "292 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  6.9500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "293 - tensor([ 3.0000, 38.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "294 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  8.8500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "295 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  9.2250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "296 - tensor([ 2.0000, 31.0000,  1.0000,  1.0000, 26.2500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "297 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "298 - tensor([ 1.0000, 55.0000,  0.0000,  0.0000, 30.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "299 - tensor([ 3.0000, 19.0000,  1.0000,  0.0000,  7.8542,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "300 - tensor([ 3.0000, 35.0000,  1.0000,  1.0000, 20.2500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "301 - tensor([ 2.0000, 40.0000,  0.0000,  0.0000, 15.7500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "302 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "303 - tensor([ 1.0000, 58.0000,  0.0000,  0.0000, 26.5500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "304 - tensor([ 3.0000, 10.0000,  0.0000,  2.0000, 24.1500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "305 - tensor([ 3.0000, 27.0000,  0.0000,  0.0000,  7.7958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "306 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  9.2167,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "307 - tensor([ 1.0000, 61.0000,  0.0000,  0.0000, 33.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "308 - tensor([ 2., 30.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "309 - tensor([ 2.0000, 36.5000,  0.0000,  2.0000, 26.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "310 - tensor([ 3., 18.,  2.,  0., 18.,  0.,  0.,  1.]): tensor([0])\n",
      "311 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 16.1000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "312 - tensor([ 1., 17.,  1.,  0., 57.,  0.,  0.,  1.]): tensor([1])\n",
      "313 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "314 - tensor([ 3.0000,  9.0000,  5.0000,  2.0000, 46.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "315 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 14.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "316 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "317 - tensor([ 2., 28.,  0.,  1., 33.,  1.,  0.,  1.]): tensor([0])\n",
      "318 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  9.4833,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "319 - tensor([ 3., 47.,  0.,  0.,  9.,  1.,  0.,  1.]): tensor([0])\n",
      "320 - tensor([ 2.0000, 52.0000,  0.0000,  0.0000, 13.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "321 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "322 - tensor([ 3.0000, 70.5000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "323 - tensor([ 1., 39.,  0.,  0.,  0.,  1.,  0.,  1.]): tensor([0])\n",
      "324 - tensor([ 1., 42.,  1.,  0., 52.,  1.,  0.,  1.]): tensor([0])\n",
      "325 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "326 - tensor([ 3.0000, 33.0000,  0.0000,  0.0000,  8.6542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "327 - tensor([ 1., 62.,  0.,  0., 80.,  0.,  0.,  0.]): tensor([1])\n",
      "328 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  7.1417,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "329 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 29.7000,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "330 - tensor([ 2.0000, 24.0000,  2.0000,  0.0000, 73.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "331 - tensor([ 3.0000, 25.0000,  0.0000,  0.0000,  7.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "332 - tensor([ 3.0000,  2.0000,  4.0000,  1.0000, 39.6875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "333 - tensor([ 3.0000,  1.0000,  0.0000,  2.0000, 15.7417,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "334 - tensor([  1.0000,  31.0000,   1.0000,   0.0000, 113.2750,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "335 - tensor([ 2.0000, 23.0000,  2.0000,  1.0000, 11.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "336 - tensor([ 3.0000, 29.8811,  2.0000,  0.0000, 23.2500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "337 - tensor([ 3.0000, 28.0000,  2.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "338 - tensor([ 2., 54.,  1.,  3., 23.,  0.,  0.,  1.]): tensor([1])\n",
      "339 - tensor([ 1., 40.,  0.,  0.,  0.,  1.,  0.,  1.]): tensor([0])\n",
      "340 - tensor([ 2.0000, 32.0000,  2.0000,  0.0000, 73.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "341 - tensor([ 2.0000, 18.0000,  0.0000,  0.0000, 11.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "342 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "343 - tensor([ 3.0000, 42.0000,  0.0000,  0.0000,  7.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "344 - tensor([ 3.0000, 25.0000,  0.0000,  0.0000,  7.7417,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "345 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "346 - tensor([  1.0000,  41.0000,   0.0000,   0.0000, 134.5000,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "347 - tensor([ 2.0000,  3.0000,  1.0000,  2.0000, 41.5792,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "348 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  8.3625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "349 - tensor([  1.0000,  49.0000,   1.0000,   1.0000, 110.8833,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([0])\n",
      "350 - tensor([ 2., 36.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "351 - tensor([ 3.0000,  9.0000,  1.0000,  1.0000, 15.9000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "352 - tensor([  1.0000,  17.0000,   1.0000,   0.0000, 108.9000,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "353 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "354 - tensor([  1.0000,  22.0000,   0.0000,   0.0000, 135.6333,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([0])\n",
      "355 - tensor([ 2.0000, 41.0000,  0.0000,  1.0000, 19.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "356 - tensor([ 2.,  2.,  1.,  1., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "357 - tensor([  1.,  24.,   3.,   2., 263.,   0.,   0.,   1.]): tensor([1])\n",
      "358 - tensor([ 1.0000, 19.0000,  1.0000,  0.0000, 53.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "359 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "360 - tensor([ 2.0000,  3.0000,  1.0000,  1.0000, 18.7500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "361 - tensor([ 3.0000, 29.8811,  3.0000,  1.0000, 25.4667,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "362 - tensor([ 2., 29.,  1.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "363 - tensor([ 3.0000, 24.0000,  1.0000,  0.0000, 15.8500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "364 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "365 - tensor([ 3.0000, 29.8811,  2.0000,  0.0000, 23.2500,  1.0000,  1.0000,  0.0000]): tensor([1])\n",
      "366 - tensor([ 3.0000, 26.0000,  1.0000,  0.0000, 16.1000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "367 - tensor([ 3.0000,  4.0000,  4.0000,  2.0000, 31.2750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "368 - tensor([ 3.0000, 18.0000,  1.0000,  0.0000,  6.4958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "369 - tensor([ 1.0000, 24.0000,  0.0000,  0.0000, 49.5042,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "370 - tensor([ 2.0000, 29.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "371 - tensor([ 3.0000, 20.0000,  1.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "372 - tensor([ 3.0000,  1.0000,  1.0000,  1.0000, 11.1333,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "373 - tensor([ 1.0000,  4.0000,  0.0000,  2.0000, 81.8583,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "374 - tensor([ 3.0000, 34.5000,  0.0000,  0.0000,  6.4375,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "375 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "376 - tensor([ 3.0000,  4.0000,  1.0000,  1.0000, 11.1333,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "377 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "378 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 35.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "379 - tensor([ 3.0000,  6.0000,  0.0000,  1.0000, 12.4750,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "380 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000, 12.4750,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "381 - tensor([ 2.0000, 19.0000,  1.0000,  1.0000, 36.7500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "382 - tensor([ 1.0000, 38.0000,  1.0000,  0.0000, 71.2833,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "383 - tensor([  1.,  36.,   1.,   2., 120.,   1.,   0.,   1.]): tensor([1])\n",
      "384 - tensor([  1.0000,  39.0000,   1.0000,   1.0000, 110.8833,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "385 - tensor([ 1.0000, 40.0000,  0.0000,  0.0000, 27.7208,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "386 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "387 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000, 33.0000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "388 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "389 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 30.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "390 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "391 - tensor([ 2., 39.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "392 - tensor([ 1.0000, 33.0000,  0.0000,  0.0000, 86.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "393 - tensor([ 1.0000, 29.8811,  1.0000,  0.0000, 82.1708,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "394 - tensor([  1.0000,   2.0000,   1.0000,   2.0000, 151.5500,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([0])\n",
      "395 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.8000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "396 - tensor([ 3.0000, 24.0000,  0.0000,  3.0000, 19.2583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "397 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000, 13.0000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "398 - tensor([ 3.0000,  2.0000,  0.0000,  1.0000, 12.2875,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "399 - tensor([  1.0000,  17.0000,   0.0000,   2.0000, 110.8833,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "400 - tensor([  1.,  64.,   1.,   4., 263.,   1.,   0.,   1.]): tensor([0])\n",
      "401 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 14.4583,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "402 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 52.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "403 - tensor([ 3.0000, 65.0000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "404 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "405 - tensor([ 3.0000, 17.0000,  4.0000,  2.0000,  7.9250,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "406 - tensor([ 3.0000,  3.0000,  3.0000,  1.0000, 21.0750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "407 - tensor([ 3.0000, 15.0000,  0.0000,  0.0000,  7.2250,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "408 - tensor([ 1.0000, 48.0000,  1.0000,  0.0000, 39.6000,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "409 - tensor([  1.0000,  58.0000,   0.0000,   0.0000, 146.5208,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "410 - tensor([ 3.0000, 14.5000,  1.0000,  0.0000, 14.4542,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "411 - tensor([ 1., 33.,  1.,  0., 90.,  0.,  1.,  0.]): tensor([1])\n",
      "412 - tensor([ 3.0000, 23.0000,  0.0000,  0.0000,  7.5500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "413 - tensor([ 3.0000, 38.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "414 - tensor([ 1., 80.,  0.,  0., 30.,  1.,  0.,  1.]): tensor([1])\n",
      "415 - tensor([  1.,  36.,   1.,   2., 120.,   0.,   0.,   1.]): tensor([1])\n",
      "416 - tensor([  1.0000,  45.0000,   1.0000,   1.0000, 164.8667,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "417 - tensor([ 2., 38.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([0])\n",
      "418 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "419 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "420 - tensor([  1.0000,  40.0000,   0.0000,   0.0000, 153.4625,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "421 - tensor([ 2., 44.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "422 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2250,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "423 - tensor([ 3.0000, 74.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "424 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7375,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "425 - tensor([ 1.0000, 60.0000,  1.0000,  1.0000, 79.2000,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "426 - tensor([ 1.0000, 37.0000,  1.0000,  1.0000, 52.5542,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "427 - tensor([  1.0000,  31.0000,   0.0000,   2.0000, 164.8667,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "428 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "429 - tensor([ 3.0000, 29.8811,  1.0000,  2.0000, 23.4500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "430 - tensor([ 3.0000, 21.0000,  2.0000,  2.0000, 34.3750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "431 - tensor([ 1.0000, 54.0000,  1.0000,  0.0000, 78.2667,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "432 - tensor([ 3.0000, 51.0000,  0.0000,  0.0000,  7.0542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "433 - tensor([ 2.,  6.,  0.,  1., 33.,  0.,  0.,  1.]): tensor([1])\n",
      "434 - tensor([ 2., 24.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "435 - tensor([ 2., 22.,  1.,  1., 29.,  0.,  0.,  1.]): tensor([1])\n",
      "436 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "437 - tensor([ 3.0000, 48.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "438 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "439 - tensor([ 1.0000, 37.0000,  1.0000,  0.0000, 53.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "440 - tensor([ 3.0000, 17.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "441 - tensor([ 1., 70.,  1.,  1., 71.,  1.,  0.,  1.]): tensor([0])\n",
      "442 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 30.0000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "443 - tensor([ 1.0000, 71.0000,  0.0000,  0.0000, 49.5042,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "444 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.7500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "445 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "446 - tensor([ 3.0000, 45.0000,  1.0000,  4.0000, 27.9000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "447 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "448 - tensor([ 2.0000, 21.0000,  1.0000,  0.0000, 11.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "449 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "450 - tensor([ 3.0000, 18.0000,  1.0000,  1.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "451 - tensor([ 3.0000,  2.0000,  3.0000,  1.0000, 21.0750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "452 - tensor([ 1.0000, 48.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "453 - tensor([ 2., 18.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "454 - tensor([ 2.0000,  8.0000,  0.0000,  2.0000, 26.2500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "455 - tensor([ 3.0000, 26.0000,  1.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "456 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "457 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "458 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "459 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "460 - tensor([ 3.0000, 25.0000,  0.0000,  0.0000,  7.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "461 - tensor([ 2., 23.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "462 - tensor([ 2., 55.,  0.,  0., 16.,  0.,  0.,  1.]): tensor([1])\n",
      "463 - tensor([ 3.0000, 23.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "464 - tensor([ 3.0000, 28.0000,  1.0000,  0.0000, 15.8500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "465 - tensor([ 3.0000, 30.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "466 - tensor([ 3.0000,  2.0000,  0.0000,  1.0000, 10.4625,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "467 - tensor([ 2.0000,  0.6700,  1.0000,  1.0000, 14.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "468 - tensor([ 2., 54.,  0.,  0., 14.,  1.,  0.,  1.]): tensor([0])\n",
      "469 - tensor([ 2., 24.,  2.,  1., 27.,  0.,  0.,  1.]): tensor([1])\n",
      "470 - tensor([ 2.0000, 21.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "471 - tensor([ 2.0000, 18.0000,  0.0000,  0.0000, 11.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "472 - tensor([ 2., 34.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "473 - tensor([ 1.0000, 36.0000,  0.0000,  0.0000, 26.2875,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "474 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "475 - tensor([ 3.0000, 29.8811,  8.0000,  2.0000, 69.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "476 - tensor([ 3.0000, 39.0000,  1.0000,  5.0000, 31.2750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "477 - tensor([ 2.0000, 24.0000,  2.0000,  3.0000, 18.7500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "478 - tensor([ 3.0000, 22.0000,  1.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "479 - tensor([ 3., 36.,  0.,  0.,  0.,  1.,  0.,  1.]): tensor([0])\n",
      "480 - tensor([ 2., 32.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "481 - tensor([ 1.0000, 24.0000,  0.0000,  0.0000, 83.1583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "482 - tensor([ 2., 35.,  0.,  0., 21.,  0.,  0.,  1.]): tensor([1])\n",
      "483 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "484 - tensor([ 3.0000, 27.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "485 - tensor([ 1.0000, 16.0000,  0.0000,  1.0000, 39.4000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "486 - tensor([ 3.0000, 55.5000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "487 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "488 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 27.7208,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "489 - tensor([ 3.0000, 45.0000,  0.0000,  0.0000,  7.7500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "490 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "491 - tensor([ 2.0000, 27.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "492 - tensor([  1.0000,  58.0000,   0.0000,   2.0000, 113.2750,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([0])\n",
      "493 - tensor([ 1.0000, 52.0000,  1.0000,  0.0000, 78.2667,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "494 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 14.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "495 - tensor([ 3.0000, 40.5000,  0.0000,  2.0000, 14.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "496 - tensor([ 3.0000, 27.0000,  0.0000,  0.0000,  6.9750,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "497 - tensor([ 3.0000, 44.0000,  0.0000,  1.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "498 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.2500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "499 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "500 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "501 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "502 - tensor([ 3.0000, 29.8811,  8.0000,  2.0000, 69.5500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "503 - tensor([ 1.0000, 44.0000,  0.0000,  1.0000, 57.9792,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "504 - tensor([ 1., 40.,  0.,  0., 31.,  1.,  0.,  0.]): tensor([1])\n",
      "505 - tensor([ 1.0000, 16.0000,  0.0000,  0.0000, 86.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "506 - tensor([ 2.0000, 57.0000,  0.0000,  0.0000, 12.3500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "507 - tensor([ 1.0000, 36.0000,  0.0000,  0.0000, 40.1250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "508 - tensor([ 3.0000, 41.0000,  0.0000,  5.0000, 39.6875,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "509 - tensor([ 3.0000, 11.0000,  0.0000,  0.0000, 18.7875,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "510 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.6292,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "511 - tensor([ 2., 27.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "512 - tensor([  1.0000,  18.0000,   2.0000,   2.0000, 262.3750,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "513 - tensor([ 3.0000, 29.8811,  2.0000,  0.0000, 21.6792,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "514 - tensor([ 1.0000, 30.0000,  0.0000,  0.0000, 93.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "515 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "516 - tensor([ 2.0000,  8.0000,  1.0000,  1.0000, 36.7500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "517 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "518 - tensor([ 3.0000, 44.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "519 - tensor([ 1.0000, 16.0000,  0.0000,  1.0000, 57.9792,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "520 - tensor([ 1.0000, 54.0000,  0.0000,  1.0000, 77.2875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "521 - tensor([ 1.0000, 62.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "522 - tensor([ 1.0000, 62.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "523 - tensor([ 3.0000, 36.0000,  1.0000,  1.0000, 24.1500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "524 - tensor([ 2.0000, 22.0000,  1.0000,  2.0000, 41.5792,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "525 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.5500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "526 - tensor([  1.0000,  18.0000,   1.0000,   0.0000, 227.5250,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "527 - tensor([ 1.0000, 58.0000,  0.0000,  0.0000, 29.7000,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "528 - tensor([ 3.0000,  1.0000,  1.0000,  2.0000, 20.5750,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "529 - tensor([ 3.0000, 35.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "530 - tensor([ 1.0000, 27.0000,  0.0000,  0.0000, 76.7292,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "531 - tensor([ 3.0000, 31.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "532 - tensor([ 1.0000, 19.0000,  0.0000,  2.0000, 26.2833,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "533 - tensor([ 1.0000, 28.0000,  1.0000,  0.0000, 82.1708,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "534 - tensor([ 2., 46.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "535 - tensor([ 3.0000, 44.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "536 - tensor([ 3.0000, 40.0000,  1.0000,  4.0000, 27.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "537 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "538 - tensor([ 3.0000, 29.0000,  0.0000,  2.0000, 15.2458,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "539 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  6.7500,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "540 - tensor([ 2., 37.,  1.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "541 - tensor([ 3.0000, 31.0000,  0.0000,  0.0000,  7.8542,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "542 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "543 - tensor([ 3.0000, 39.0000,  0.0000,  0.0000, 24.1500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "544 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  7.7958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "545 - tensor([ 3.0000,  4.0000,  0.0000,  2.0000, 22.0250,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "546 - tensor([ 1.0000, 29.8811,  1.0000,  0.0000, 89.1042,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "547 - tensor([ 2.0000, 36.0000,  0.0000,  0.0000, 12.8750,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "548 - tensor([ 1.0000, 63.0000,  1.0000,  0.0000, 77.9583,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "549 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 24.1500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "550 - tensor([ 3.0000, 33.0000,  1.0000,  1.0000, 20.5250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "551 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  8.1583,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "552 - tensor([ 1.0000, 53.0000,  2.0000,  0.0000, 51.4792,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "553 - tensor([ 3.0000, 25.0000,  0.0000,  0.0000,  7.7750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "554 - tensor([ 3.0000, 16.0000,  5.0000,  2.0000, 46.9000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "555 - tensor([ 3., 49.,  0.,  0.,  0.,  1.,  0.,  1.]): tensor([0])\n",
      "556 - tensor([ 2., 25.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "557 - tensor([  1.0000,  36.0000,   0.0000,   0.0000, 135.6333,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "558 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "559 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "560 - tensor([ 2.0000, 31.0000,  1.0000,  1.0000, 37.0042,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "561 - tensor([ 3.0000, 37.0000,  0.0000,  0.0000,  9.5875,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "562 - tensor([ 2., 34.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([1])\n",
      "563 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "564 - tensor([ 2., 36.,  1.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "565 - tensor([ 1.0000, 47.0000,  0.0000,  0.0000, 34.0208,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "566 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.5208,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "567 - tensor([ 3.0000,  7.0000,  4.0000,  1.0000, 29.1250,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "568 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "569 - tensor([ 2.,  3.,  1.,  1., 26.,  1.,  0.,  1.]): tensor([1])\n",
      "570 - tensor([ 3., 16.,  2.,  0., 18.,  1.,  0.,  1.]): tensor([0])\n",
      "571 - tensor([  1.0000,  23.0000,   1.0000,   0.0000, 113.2750,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "572 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  7.6500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "573 - tensor([ 3.0000, 23.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "574 - tensor([ 1.0000, 29.8811,  0.0000,  0.0000, 35.0000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "575 - tensor([ 3.0000, 30.0000,  1.0000,  0.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "576 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "577 - tensor([ 3.0000,  3.0000,  4.0000,  2.0000, 31.3875,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "578 - tensor([ 3.0000, 11.0000,  4.0000,  2.0000, 31.2750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "579 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.3125,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "580 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 15.5000,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "581 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "582 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "583 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 16.1000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "584 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([1])\n",
      "585 - tensor([ 3.0000, 26.0000,  0.0000,  0.0000,  7.8875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "586 - tensor([ 1.0000, 39.0000,  1.0000,  1.0000, 79.6500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "587 - tensor([  1.0000,  22.0000,   0.0000,   0.0000, 151.5500,   0.0000,   0.0000,\n",
      "          1.0000]): tensor([1])\n",
      "588 - tensor([ 2.0000, 62.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "589 - tensor([ 3.0000, 47.0000,  1.0000,  0.0000, 14.5000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "590 - tensor([ 3.0000, 16.0000,  1.0000,  1.0000, 20.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "591 - tensor([ 1.0000, 47.0000,  0.0000,  0.0000, 38.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "592 - tensor([ 1., 19.,  0.,  0., 30.,  0.,  0.,  1.]): tensor([1])\n",
      "593 - tensor([ 2.0000,  7.0000,  0.0000,  2.0000, 26.2500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "594 - tensor([ 2.0000, 34.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "595 - tensor([  1.,  11.,   1.,   2., 120.,   1.,   0.,   1.]): tensor([1])\n",
      "596 - tensor([ 2.0000, 21.0000,  2.0000,  0.0000, 73.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "597 - tensor([ 3.0000, 29.8811,  1.0000,  0.0000, 15.5000,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "598 - tensor([ 3.0000, 33.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "599 - tensor([ 3.0000, 42.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "600 - tensor([ 2.0000, 26.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "601 - tensor([ 1.0000, 49.0000,  0.0000,  0.0000, 25.9292,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "602 - tensor([ 1.0000, 18.0000,  0.0000,  2.0000, 79.6500,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "603 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "604 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "605 - tensor([ 3.0000, 37.0000,  2.0000,  0.0000,  7.9250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "606 - tensor([ 1.0000, 50.0000,  1.0000,  0.0000, 55.9000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "607 - tensor([ 3.0000,  9.0000,  3.0000,  2.0000, 27.9000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "608 - tensor([ 2.0000, 25.0000,  1.0000,  2.0000, 41.5792,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "609 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "610 - tensor([ 3.0000, 51.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "611 - tensor([ 2.0000, 34.0000,  1.0000,  1.0000, 32.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "612 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  7.7500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "613 - tensor([ 2., 27.,  0.,  0., 26.,  1.,  0.,  1.]): tensor([0])\n",
      "614 - tensor([  1.0000,  24.0000,   0.0000,   1.0000, 247.5208,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([0])\n",
      "615 - tensor([ 2.0000, 14.0000,  1.0000,  0.0000, 30.0708,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "616 - tensor([ 3.0000, 17.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "617 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "618 - tensor([  1.0000,  40.0000,   1.0000,   1.0000, 134.5000,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "619 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000, 15.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "620 - tensor([ 3.0000, 29.8811,  1.0000,  2.0000, 23.4500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "621 - tensor([ 2., 28.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "622 - tensor([  1.0000,  29.8811,   1.0000,   0.0000, 146.5208,   0.0000,   0.0000,\n",
      "          0.0000]): tensor([1])\n",
      "623 - tensor([ 2., 19.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "624 - tensor([ 2., 29.,  1.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "625 - tensor([ 3.0000, 29.8811,  8.0000,  2.0000, 69.5500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "626 - tensor([ 3.0000,  5.0000,  2.0000,  1.0000, 19.2583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "627 - tensor([ 3.0000, 47.0000,  0.0000,  0.0000,  7.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "628 - tensor([ 2., 42.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "629 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  7.1250,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "630 - tensor([ 3.0000, 15.0000,  1.0000,  0.0000, 14.4542,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "631 - tensor([ 2.0000, 29.8811,  0.0000,  0.0000, 12.3500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "632 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.7333,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "633 - tensor([ 1., 36.,  0.,  2., 71.,  0.,  0.,  1.]): tensor([1])\n",
      "634 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "635 - tensor([ 2.,  1.,  2.,  1., 39.,  1.,  0.,  1.]): tensor([1])\n",
      "636 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "637 - tensor([ 2.0000, 50.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "638 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.1375,  0.0000,  1.0000,  0.0000]): tensor([0])\n",
      "639 - tensor([ 3.0000, 32.0000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "640 - tensor([ 3.0000, 20.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "641 - tensor([ 3.0000, 24.0000,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "642 - tensor([ 3.0000,  4.0000,  0.0000,  1.0000, 13.4167,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "643 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "644 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "645 - tensor([ 1.0000, 29.8811,  1.0000,  0.0000, 51.8625,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "646 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "647 - tensor([ 3.0000, 33.0000,  0.0000,  0.0000,  7.7750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "648 - tensor([ 2.0000, 16.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "649 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8792,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "650 - tensor([ 2.0000, 57.0000,  0.0000,  0.0000, 10.5000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "651 - tensor([ 1.0000, 24.0000,  0.0000,  0.0000, 69.3000,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "652 - tensor([ 2.0000, 59.0000,  0.0000,  0.0000, 13.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "653 - tensor([ 3.0000, 34.0000,  0.0000,  0.0000,  6.4958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "654 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "655 - tensor([ 3.0000, 29.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "656 - tensor([ 1.0000, 35.0000,  1.0000,  0.0000, 53.1000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "657 - tensor([ 3.0000, 29.8811,  1.0000,  1.0000, 15.2458,  1.0000,  0.0000,  0.0000]): tensor([1])\n",
      "658 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "659 - tensor([ 3.0000, 34.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "660 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "661 - tensor([ 1.0000, 36.0000,  1.0000,  0.0000, 78.8500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "662 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.7500,  0.0000,  1.0000,  0.0000]): tensor([1])\n",
      "663 - tensor([  1.0000,  29.8811,   0.0000,   0.0000, 227.5250,   1.0000,   0.0000,\n",
      "          0.0000]): tensor([0])\n",
      "664 - tensor([ 3.0000, 29.8811,  1.0000,  1.0000, 22.3583,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "665 - tensor([ 3.0000, 25.0000,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "666 - tensor([ 3.0000, 28.0000,  1.0000,  1.0000, 14.4000,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "667 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  7.7958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "668 - tensor([ 2.0000, 24.0000,  0.0000,  0.0000, 10.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "669 - tensor([ 1.0000, 47.0000,  1.0000,  1.0000, 52.5542,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "670 - tensor([ 3.0000, 16.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "671 - tensor([ 3.0000, 28.5000,  0.0000,  0.0000, 16.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "672 - tensor([ 1.0000, 65.0000,  0.0000,  0.0000, 26.5500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "673 - tensor([ 3.0000, 31.0000,  1.0000,  1.0000, 20.5250,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "674 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  0.0000,  0.0000,  0.0000]): tensor([1])\n",
      "675 - tensor([ 2., 30.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "676 - tensor([ 3.0000, 22.0000,  0.0000,  0.0000,  9.3500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "677 - tensor([ 3.0000, 34.0000,  0.0000,  0.0000,  8.0500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "678 - tensor([ 3.0000, 32.0000,  1.0000,  0.0000, 15.8500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "679 - tensor([ 1.0000, 47.0000,  0.0000,  0.0000, 25.5875,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "680 - tensor([ 1.0000, 28.0000,  0.0000,  0.0000, 47.1000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "681 - tensor([ 3.0000, 17.0000,  0.0000,  0.0000, 14.4583,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "682 - tensor([ 1., 31.,  1.,  0., 57.,  1.,  0.,  1.]): tensor([1])\n",
      "683 - tensor([ 2., 34.,  0.,  0., 13.,  0.,  0.,  1.]): tensor([1])\n",
      "684 - tensor([ 3.0000, 27.0000,  0.0000,  1.0000, 12.4750,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "685 - tensor([ 3.0000, 40.0000,  1.0000,  1.0000, 15.5000,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "686 - tensor([ 2., 48.,  1.,  2., 65.,  0.,  0.,  1.]): tensor([1])\n",
      "687 - tensor([ 3.0000,  9.0000,  1.0000,  1.0000, 15.2458,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "688 - tensor([ 3.0000, 33.0000,  0.0000,  0.0000,  8.6625,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "689 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000, 56.4958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "690 - tensor([ 3.0000, 21.0000,  0.0000,  0.0000,  7.8542,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "691 - tensor([ 1., 31.,  1.,  0., 52.,  1.,  0.,  1.]): tensor([0])\n",
      "692 - tensor([ 1.0000, 50.0000,  0.0000,  0.0000, 28.7125,  0.0000,  0.0000,  0.0000]): tensor([0])\n",
      "693 - tensor([ 1.0000, 22.0000,  1.0000,  0.0000, 66.6000,  0.0000,  0.0000,  1.0000]): tensor([1])\n",
      "694 - tensor([ 2., 27.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "695 - tensor([ 2., 34.,  0.,  0., 13.,  1.,  0.,  1.]): tensor([0])\n",
      "696 - tensor([ 1.0000, 42.0000,  1.0000,  0.0000, 52.5542,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "697 - tensor([ 3.0000, 19.0000,  0.0000,  0.0000,  7.8958,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "698 - tensor([ 2.0000, 21.0000,  0.0000,  0.0000, 73.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "699 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  7.2292,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "700 - tensor([ 2.0000, 31.0000,  1.0000,  1.0000, 26.2500,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "701 - tensor([ 3.0000, 18.0000,  0.0000,  0.0000,  7.7750,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "702 - tensor([ 3.0000, 40.5000,  0.0000,  0.0000,  7.7500,  1.0000,  1.0000,  0.0000]): tensor([0])\n",
      "703 - tensor([ 2., 19.,  0.,  0., 26.,  0.,  0.,  1.]): tensor([1])\n",
      "704 - tensor([ 3.0000, 45.5000,  0.0000,  0.0000,  7.2250,  1.0000,  0.0000,  0.0000]): tensor([0])\n",
      "705 - tensor([ 3.0000, 41.0000,  0.0000,  2.0000, 20.2125,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "706 - tensor([ 3.0000, 34.0000,  1.0000,  1.0000, 14.4000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "707 - tensor([ 2., 27.,  1.,  0., 21.,  0.,  0.,  1.]): tensor([0])\n",
      "708 - tensor([ 3.0000, 28.0000,  0.0000,  0.0000,  7.8958,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "709 - tensor([ 3.0000, 20.0000,  1.0000,  0.0000,  9.8250,  0.0000,  0.0000,  1.0000]): tensor([0])\n",
      "710 - tensor([ 3.0000, 45.0000,  0.0000,  0.0000,  6.9750,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "711 - tensor([ 3.0000, 29.8811,  0.0000,  0.0000,  9.5000,  1.0000,  0.0000,  1.0000]): tensor([0])\n",
      "712 - tensor([ 1.0000, 28.0000,  0.0000,  0.0000, 35.5000,  1.0000,  0.0000,  1.0000]): tensor([1])\n",
      "################################################## 2\n",
      "[TRAIN]\n",
      "0 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "1 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "2 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "3 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "4 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "5 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "6 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "7 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "8 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "9 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "10 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "11 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "12 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "13 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "14 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "15 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "16 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "17 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "18 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "19 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "20 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "21 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "22 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "23 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "24 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "25 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "26 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "27 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "28 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "29 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "30 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "31 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "32 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "33 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "34 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "35 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "36 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "37 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "38 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "39 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "40 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "41 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "42 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "43 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "44 - torch.Size([9, 8]): torch.Size([9, 1])\n",
      "[VALIDATION]\n",
      "0 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "1 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "2 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "3 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "4 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "5 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "6 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "7 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "8 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "9 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "10 - torch.Size([16, 8]): torch.Size([16, 1])\n",
      "11 - torch.Size([2, 8]): torch.Size([2, 1])\n",
      "################################################## 3\n",
      "[TEST]\n",
      "torch.Size([418, 8])\n",
      "892 0\n",
      "893 0\n",
      "894 0\n",
      "895 0\n",
      "896 1\n",
      "897 1\n",
      "898 0\n",
      "899 1\n",
      "900 0\n",
      "901 1\n",
      "902 0\n",
      "903 1\n",
      "904 1\n",
      "905 0\n",
      "906 1\n",
      "907 1\n",
      "908 0\n",
      "909 0\n",
      "910 0\n",
      "911 0\n",
      "912 1\n",
      "913 1\n",
      "914 1\n",
      "915 1\n",
      "916 1\n",
      "917 0\n",
      "918 1\n",
      "919 0\n",
      "920 1\n",
      "921 1\n",
      "922 1\n",
      "923 1\n",
      "924 1\n",
      "925 1\n",
      "926 1\n",
      "927 0\n",
      "928 0\n",
      "929 0\n",
      "930 0\n",
      "931 1\n",
      "932 0\n",
      "933 1\n",
      "934 0\n",
      "935 0\n",
      "936 1\n",
      "937 0\n",
      "938 1\n",
      "939 0\n",
      "940 1\n",
      "941 0\n",
      "942 1\n",
      "943 1\n",
      "944 1\n",
      "945 1\n",
      "946 1\n",
      "947 1\n",
      "948 0\n",
      "949 0\n",
      "950 1\n",
      "951 1\n",
      "952 1\n",
      "953 0\n",
      "954 0\n",
      "955 0\n",
      "956 1\n",
      "957 1\n",
      "958 0\n",
      "959 1\n",
      "960 1\n",
      "961 1\n",
      "962 0\n",
      "963 0\n",
      "964 0\n",
      "965 1\n",
      "966 1\n",
      "967 1\n",
      "968 0\n",
      "969 1\n",
      "970 0\n",
      "971 0\n",
      "972 1\n",
      "973 1\n",
      "974 1\n",
      "975 0\n",
      "976 0\n",
      "977 1\n",
      "978 0\n",
      "979 0\n",
      "980 0\n",
      "981 1\n",
      "982 1\n",
      "983 0\n",
      "984 1\n",
      "985 0\n",
      "986 1\n",
      "987 0\n",
      "988 1\n",
      "989 0\n",
      "990 0\n",
      "991 0\n",
      "992 1\n",
      "993 1\n",
      "994 0\n",
      "995 0\n",
      "996 1\n",
      "997 1\n",
      "998 0\n",
      "999 0\n",
      "1000 0\n",
      "1001 1\n",
      "1002 0\n",
      "1003 0\n",
      "1004 1\n",
      "1005 0\n",
      "1006 1\n",
      "1007 1\n",
      "1008 0\n",
      "1009 1\n",
      "1010 1\n",
      "1011 1\n",
      "1012 1\n",
      "1013 0\n",
      "1014 1\n",
      "1015 0\n",
      "1016 0\n",
      "1017 1\n",
      "1018 0\n",
      "1019 1\n",
      "1020 0\n",
      "1021 0\n",
      "1022 0\n",
      "1023 0\n",
      "1024 1\n",
      "1025 0\n",
      "1026 0\n",
      "1027 0\n",
      "1028 0\n",
      "1029 1\n",
      "1030 0\n",
      "1031 1\n",
      "1032 1\n",
      "1033 1\n",
      "1034 1\n",
      "1035 1\n",
      "1036 1\n",
      "1037 1\n",
      "1038 1\n",
      "1039 0\n",
      "1040 1\n",
      "1041 1\n",
      "1042 1\n",
      "1043 0\n",
      "1044 1\n",
      "1045 0\n",
      "1046 1\n",
      "1047 0\n",
      "1048 1\n",
      "1049 0\n",
      "1050 1\n",
      "1051 1\n",
      "1052 0\n",
      "1053 1\n",
      "1054 0\n",
      "1055 0\n",
      "1056 0\n",
      "1057 1\n",
      "1058 1\n",
      "1059 1\n",
      "1060 1\n",
      "1061 0\n",
      "1062 0\n",
      "1063 0\n",
      "1064 1\n",
      "1065 0\n",
      "1066 1\n",
      "1067 1\n",
      "1068 1\n",
      "1069 1\n",
      "1070 1\n",
      "1071 1\n",
      "1072 0\n",
      "1073 1\n",
      "1074 1\n",
      "1075 0\n",
      "1076 1\n",
      "1077 0\n",
      "1078 1\n",
      "1079 1\n",
      "1080 1\n",
      "1081 0\n",
      "1082 1\n",
      "1083 1\n",
      "1084 1\n",
      "1085 0\n",
      "1086 1\n",
      "1087 0\n",
      "1088 1\n",
      "1089 0\n",
      "1090 1\n",
      "1091 0\n",
      "1092 0\n",
      "1093 1\n",
      "1094 1\n",
      "1095 1\n",
      "1096 0\n",
      "1097 1\n",
      "1098 0\n",
      "1099 1\n",
      "1100 1\n",
      "1101 0\n",
      "1102 1\n",
      "1103 0\n",
      "1104 1\n",
      "1105 0\n",
      "1106 0\n",
      "1107 1\n",
      "1108 0\n",
      "1109 1\n",
      "1110 1\n",
      "1111 0\n",
      "1112 1\n",
      "1113 0\n",
      "1114 0\n",
      "1115 0\n",
      "1116 0\n",
      "1117 1\n",
      "1118 0\n",
      "1119 0\n",
      "1120 0\n",
      "1121 0\n",
      "1122 1\n",
      "1123 1\n",
      "1124 0\n",
      "1125 0\n",
      "1126 1\n",
      "1127 0\n",
      "1128 1\n",
      "1129 0\n",
      "1130 1\n",
      "1131 1\n",
      "1132 0\n",
      "1133 1\n",
      "1134 1\n",
      "1135 0\n",
      "1136 1\n",
      "1137 1\n",
      "1138 1\n",
      "1139 1\n",
      "1140 1\n",
      "1141 1\n",
      "1142 1\n",
      "1143 0\n",
      "1144 1\n",
      "1145 0\n",
      "1146 0\n",
      "1147 0\n",
      "1148 0\n",
      "1149 0\n",
      "1150 1\n",
      "1151 0\n",
      "1152 1\n",
      "1153 0\n",
      "1154 1\n",
      "1155 1\n",
      "1156 0\n",
      "1157 0\n",
      "1158 0\n",
      "1159 0\n",
      "1160 0\n",
      "1161 1\n",
      "1162 1\n",
      "1163 0\n",
      "1164 1\n",
      "1165 1\n",
      "1166 0\n",
      "1167 1\n",
      "1168 0\n",
      "1169 1\n",
      "1170 1\n",
      "1171 1\n",
      "1172 0\n",
      "1173 1\n",
      "1174 0\n",
      "1175 1\n",
      "1176 1\n",
      "1177 0\n",
      "1178 0\n",
      "1179 1\n",
      "1180 0\n",
      "1181 0\n",
      "1182 1\n",
      "1183 0\n",
      "1184 0\n",
      "1185 1\n",
      "1186 0\n",
      "1187 0\n",
      "1188 1\n",
      "1189 1\n",
      "1190 1\n",
      "1191 0\n",
      "1192 0\n",
      "1193 1\n",
      "1194 1\n",
      "1195 0\n",
      "1196 0\n",
      "1197 0\n",
      "1198 1\n",
      "1199 1\n",
      "1200 1\n",
      "1201 0\n",
      "1202 1\n",
      "1203 0\n",
      "1204 0\n",
      "1205 0\n",
      "1206 1\n",
      "1207 0\n",
      "1208 1\n",
      "1209 1\n",
      "1210 0\n",
      "1211 1\n",
      "1212 0\n",
      "1213 0\n",
      "1214 1\n",
      "1215 1\n",
      "1216 1\n",
      "1217 0\n",
      "1218 1\n",
      "1219 1\n",
      "1220 1\n",
      "1221 1\n",
      "1222 1\n",
      "1223 1\n",
      "1224 0\n",
      "1225 1\n",
      "1226 0\n",
      "1227 1\n",
      "1228 0\n",
      "1229 0\n",
      "1230 1\n",
      "1231 0\n",
      "1232 1\n",
      "1233 0\n",
      "1234 1\n",
      "1235 1\n",
      "1236 1\n",
      "1237 0\n",
      "1238 1\n",
      "1239 0\n",
      "1240 1\n",
      "1241 1\n",
      "1242 1\n",
      "1243 0\n",
      "1244 1\n",
      "1245 1\n",
      "1246 1\n",
      "1247 1\n",
      "1248 1\n",
      "1249 0\n",
      "1250 0\n",
      "1251 1\n",
      "1252 1\n",
      "1253 1\n",
      "1254 1\n",
      "1255 0\n",
      "1256 1\n",
      "1257 1\n",
      "1258 1\n",
      "1259 1\n",
      "1260 1\n",
      "1261 1\n",
      "1262 1\n",
      "1263 1\n",
      "1264 0\n",
      "1265 0\n",
      "1266 1\n",
      "1267 1\n",
      "1268 1\n",
      "1269 1\n",
      "1270 1\n",
      "1271 1\n",
      "1272 0\n",
      "1273 0\n",
      "1274 0\n",
      "1275 1\n",
      "1276 0\n",
      "1277 1\n",
      "1278 0\n",
      "1279 0\n",
      "1280 0\n",
      "1281 1\n",
      "1282 1\n",
      "1283 1\n",
      "1284 1\n",
      "1285 0\n",
      "1286 1\n",
      "1287 1\n",
      "1288 0\n",
      "1289 1\n",
      "1290 0\n",
      "1291 0\n",
      "1292 1\n",
      "1293 1\n",
      "1294 1\n",
      "1295 1\n",
      "1296 1\n",
      "1297 1\n",
      "1298 1\n",
      "1299 1\n",
      "1300 0\n",
      "1301 1\n",
      "1302 0\n",
      "1303 1\n",
      "1304 0\n",
      "1305 0\n",
      "1306 1\n",
      "1307 0\n",
      "1308 0\n",
      "1309 1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 요구사항 2 - _01_code/_07_learning_and_optimization/f_my_model_training_with_argparse_wandb.py 를 이용해 코드 수정하기",
   "id": "8c265ba4f870087"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:59.108161Z",
     "start_time": "2024-10-25T02:12:57.573205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "#chekpoint의 경로를 설정하고, 만약 경로의 디렉토리가 없다면 디렉토리를 만든다."
   ],
   "id": "7f988c845732dbb1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:59.115131Z",
     "start_time": "2024-10-25T02:12:59.109161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        Activation_Function = wandb.config.Activation_Function\n",
    "        self.model = nn.Sequential( #안의 것을 순차적으로 실행\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0],wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        ) # hidden은 뉴런 30짜리 2개, 활성화 함수는 ReLU 사용\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "id": "f79d09f62febf832",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:59.120816Z",
     "start_time": "2024-10-25T02:12:59.116154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_and_optimizer():\n",
    "    my_model = MyModel(n_input=8, n_output=2)\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "    \n",
    "    return my_model, optimizer"
   ],
   "id": "1a6cebf131a6fb2c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:59.130577Z",
     "start_time": "2024-10-25T02:12:59.121997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    next_print_epoch = 100  #100 epoch마다 출력\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train= 0.0\n",
    "        num_trains = 0\n",
    "        for train_batch in train_data_loader:\n",
    "            input = train_batch['input']            #input, target = train_batch  이런 형태로 하면 input이 str 자료형이라는 오류가 생김\n",
    "            target = train_batch['target'].view(-1)\n",
    "            output_train = model(input)             #예측 시작\n",
    "            loss = loss_fn(output_train, target)    #예측을 토대로 loss 계산\n",
    "            \n",
    "            loss_train += loss.item() # epoch동안 손실을 누적\n",
    "            num_trains += 1\n",
    "            \n",
    "            #파라미터 최적화 진행\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        #validation에 대해서 train과 동일하게 적용해 validation_loss 계산\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input = validation_batch['input']\n",
    "                target = validation_batch['target'].view(-1)\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "                num_validations += 1\n",
    "                \n",
    "        #학습 결과로 나온 Loss를 wandb에 기록\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Loss\": loss_train/num_trains,\n",
    "            \"Validation Loss\": loss_validation/num_validations,\n",
    "        })\n",
    "        \n",
    "        if epoch >= next_print_epoch:\n",
    "          print(\n",
    "            f\"Epoch {epoch}, \"\n",
    "            f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "            f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "          )\n",
    "          next_print_epoch += 100\n",
    "    "
   ],
   "id": "aba60ffea24da239",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T02:12:59.140944Z",
     "start_time": "2024-10-25T02:12:59.132568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(epochs=1000, batch_size=512, learning_rate=1e-3, activation_function=0):\n",
    "    current_time_str = datetime.now().astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    Activation_Function_list = [nn.ReLU(), nn.ELU(), nn.LeakyReLU(), nn.PReLU(), nn.SELU(), nn.Softplus(), nn.SiLU()]   #확인할 activation_function 리스트\n",
    "    config = {\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_hidden_unit_list': [30,30],\n",
    "        'Activation_Function': Activation_Function_list[activation_function]\n",
    "    }\n",
    "    wandb.init( #wandb설정 값들\n",
    "        mode = \"online\" if wandb else \"disabled\",\n",
    "        project = \"link_DL\",\n",
    "        notes = \"HW2_Titanic\",\n",
    "        tags = [\"my_model\", \"Titanic\"],\n",
    "        name = current_time_str,\n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #학습을 할 device 확인 및 지정\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    \n",
    "    #사용할 데이터를 전처리해 로드\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "    \n",
    "    linear_model, optimizer = get_model_and_optimizer() # 모델과 최적화 방법을 로드\n",
    "    linear_model.to(device)\n",
    "    \n",
    "    train_data_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    validation_data_loader = DataLoader(dataset = validation_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    \n",
    "    training_loop( # 학습을 진행\n",
    "        model= linear_model,\n",
    "        optimizer= optimizer,\n",
    "        train_data_loader= train_data_loader,\n",
    "        validation_data_loader= validation_data_loader,\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    return linear_model"
   ],
   "id": "8173b5bf68ee9c9c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:31:25.658313Z",
     "start_time": "2024-10-22T14:26:43.615157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 1000\n",
    "    batch_size = 16\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    #확인할 활성화 함수 목록: [nn.ReLU(), nn.ELU(), nn.LeakyReLU(), nn.PReLU(), nn.SELU(), nn.Softplus(), nn.SiLU()]\n",
    "    for activation_function in range(7):\n",
    "        models.append(main(epochs, batch_size, learning_rate, activation_function))"
   ],
   "id": "5be28d621bc6b114",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232643-ieo9oqxp</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/ieo9oqxp' target=\"_blank\">2024_10_22_23_26_43</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/ieo9oqxp' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/ieo9oqxp</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'ReLU()'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5776, Validation loss 0.5619\n",
      "Epoch 200, Training loss 0.5384, Validation loss 0.5531\n",
      "Epoch 300, Training loss 0.5250, Validation loss 0.5078\n",
      "Epoch 400, Training loss 0.5113, Validation loss 0.4384\n",
      "Epoch 500, Training loss 0.4861, Validation loss 0.4972\n",
      "Epoch 600, Training loss 0.4634, Validation loss 0.4491\n",
      "Epoch 700, Training loss 0.4680, Validation loss 0.5176\n",
      "Epoch 800, Training loss 0.4535, Validation loss 0.4724\n",
      "Epoch 900, Training loss 0.4514, Validation loss 0.3899\n",
      "Epoch 1000, Training loss 0.4656, Validation loss 0.4531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Training Loss</td><td>█▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▄▃▃▃▃▃▂▃▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>▇▄▄▄▄▃▄▃▃▃▃▃█▂▃▃▂▄▂▂▃▂▃▃▂▂▂▂▂▂▂▂▂▄▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.46558</td></tr><tr><td>Validation Loss</td><td>0.45313</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_26_43</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/ieo9oqxp' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/ieo9oqxp</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232643-ieo9oqxp\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232721-iaku4015</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/iaku4015' target=\"_blank\">2024_10_22_23_27_21</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/iaku4015' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/iaku4015</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'ELU(alpha=1.0)'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5811, Validation loss 0.5659\n",
      "Epoch 200, Training loss 0.5374, Validation loss 0.5549\n",
      "Epoch 300, Training loss 0.5026, Validation loss 0.5440\n",
      "Epoch 400, Training loss 0.4733, Validation loss 0.6037\n",
      "Epoch 500, Training loss 0.4773, Validation loss 0.5289\n",
      "Epoch 600, Training loss 0.4500, Validation loss 0.5404\n",
      "Epoch 700, Training loss 0.4418, Validation loss 0.4524\n",
      "Epoch 800, Training loss 0.4406, Validation loss 0.4709\n",
      "Epoch 900, Training loss 0.4349, Validation loss 0.4536\n",
      "Epoch 1000, Training loss 0.4284, Validation loss 0.4471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇███</td></tr><tr><td>Training Loss</td><td>███▇▇▇▇▆▇▆▅▅▅▅▄▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>Validation Loss</td><td>▅▅▅▄▄▅▄▄▅▅▆▄▃▄▄▄▄▃▃▃▃▃▃█▂▁▃▃▃▁▁▄▁▂▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.42842</td></tr><tr><td>Validation Loss</td><td>0.44708</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_27_21</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/iaku4015' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/iaku4015</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232721-iaku4015\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232759-xvv6t0ut</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/xvv6t0ut' target=\"_blank\">2024_10_22_23_27_59</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/xvv6t0ut' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/xvv6t0ut</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'LeakyReLU(negative_slope=0.01)'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5831, Validation loss 0.5937\n",
      "Epoch 200, Training loss 0.5582, Validation loss 0.5503\n",
      "Epoch 300, Training loss 0.5239, Validation loss 0.6275\n",
      "Epoch 400, Training loss 0.4938, Validation loss 0.5276\n",
      "Epoch 500, Training loss 0.4789, Validation loss 0.5788\n",
      "Epoch 600, Training loss 0.4744, Validation loss 0.5027\n",
      "Epoch 700, Training loss 0.4468, Validation loss 0.4733\n",
      "Epoch 800, Training loss 0.4454, Validation loss 0.5305\n",
      "Epoch 900, Training loss 0.4361, Validation loss 0.4818\n",
      "Epoch 1000, Training loss 0.4299, Validation loss 0.5562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>Training Loss</td><td>██▇▇▆▆▆▆▅▅▅▅▅▅▄▃▃▃▄▃▂▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>▃▄▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▂▂▄▁▂█▂▁▇▂▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.42992</td></tr><tr><td>Validation Loss</td><td>0.55616</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_27_59</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/xvv6t0ut' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/xvv6t0ut</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232759-xvv6t0ut\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232837-pf0xo2nl</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/pf0xo2nl' target=\"_blank\">2024_10_22_23_28_37</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/pf0xo2nl' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/pf0xo2nl</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'PReLU(num_parameters=1)'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5650, Validation loss 0.6132\n",
      "Epoch 200, Training loss 0.5214, Validation loss 0.5511\n",
      "Epoch 300, Training loss 0.5031, Validation loss 0.5059\n",
      "Epoch 400, Training loss 0.4715, Validation loss 0.6201\n",
      "Epoch 500, Training loss 0.4817, Validation loss 0.4809\n",
      "Epoch 600, Training loss 0.4486, Validation loss 0.5172\n",
      "Epoch 700, Training loss 0.4585, Validation loss 0.5354\n",
      "Epoch 800, Training loss 0.4324, Validation loss 0.4999\n",
      "Epoch 900, Training loss 0.4368, Validation loss 0.4646\n",
      "Epoch 1000, Training loss 0.4323, Validation loss 0.5219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>Training Loss</td><td>█▇▆▆▅▄▅▅▄▄▄▄▄▄▃▃▃▂▃▂▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▂▁</td></tr><tr><td>Validation Loss</td><td>▇▇▇██▆▇▇▆▅▅▅▂▃▅▂▄█▆▆▄▆▃▆▆▇▁▇▃▂▂▃▄▄▁▂▄▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.4323</td></tr><tr><td>Validation Loss</td><td>0.52187</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_28_37</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/pf0xo2nl' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/pf0xo2nl</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232837-pf0xo2nl\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232915-8cudbagr</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/8cudbagr' target=\"_blank\">2024_10_22_23_29_15</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/8cudbagr' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/8cudbagr</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'SELU()'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5753, Validation loss 0.6362\n",
      "Epoch 200, Training loss 0.5456, Validation loss 0.5706\n",
      "Epoch 300, Training loss 0.5042, Validation loss 0.6075\n",
      "Epoch 400, Training loss 0.4810, Validation loss 0.6511\n",
      "Epoch 500, Training loss 0.4514, Validation loss 0.5459\n",
      "Epoch 600, Training loss 0.4526, Validation loss 0.4673\n",
      "Epoch 700, Training loss 0.4395, Validation loss 0.4703\n",
      "Epoch 800, Training loss 0.4325, Validation loss 0.4599\n",
      "Epoch 900, Training loss 0.4237, Validation loss 0.5540\n",
      "Epoch 1000, Training loss 0.4348, Validation loss 0.4952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>Training Loss</td><td>██▇▇▇▆▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>▇▆▆▆█▆▆▅▄▄▃▄▅▄▃▄▃▂▃▂▂▂▁▂▁▃▂▇▁▂▂▁▁▂▁▁▅▃▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.43484</td></tr><tr><td>Validation Loss</td><td>0.49523</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_29_15</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/8cudbagr' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/8cudbagr</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232915-8cudbagr\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_232956-l5h8xtgy</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/l5h8xtgy' target=\"_blank\">2024_10_22_23_29_56</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/l5h8xtgy' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/l5h8xtgy</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'Softplus(beta=1.0, threshold=20.0)'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5801, Validation loss 0.6005\n",
      "Epoch 200, Training loss 0.5357, Validation loss 0.5657\n",
      "Epoch 300, Training loss 0.5015, Validation loss 0.5448\n",
      "Epoch 400, Training loss 0.4862, Validation loss 0.4649\n",
      "Epoch 500, Training loss 0.4721, Validation loss 0.4905\n",
      "Epoch 600, Training loss 0.4491, Validation loss 0.4573\n",
      "Epoch 700, Training loss 0.4422, Validation loss 0.4308\n",
      "Epoch 800, Training loss 0.4589, Validation loss 0.4911\n",
      "Epoch 900, Training loss 0.4426, Validation loss 0.4659\n",
      "Epoch 1000, Training loss 0.4274, Validation loss 0.4885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>Training Loss</td><td>███▇▇▇▇▇▆▆▆▆▆▆▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁</td></tr><tr><td>Validation Loss</td><td>▅▆▆▅▇▅▅▅▄▅▄▄▄▅▅▃██▂▃▂▂▃▁▁▄▂▆▂▂▁▅▁▂▁▁▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.42743</td></tr><tr><td>Validation Loss</td><td>0.48853</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_29_56</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/l5h8xtgy' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/l5h8xtgy</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_232956-l5h8xtgy\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241022_233040-2yn0r3ni</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL/runs/2yn0r3ni' target=\"_blank\">2024_10_22_23_30_40</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/2yn0r3ni' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/2yn0r3ni</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'SiLU()'}\n",
      "Device: cpu\n",
      "Epoch 100, Training loss 0.5984, Validation loss 0.5659\n",
      "Epoch 200, Training loss 0.5756, Validation loss 0.5402\n",
      "Epoch 300, Training loss 0.5440, Validation loss 0.5144\n",
      "Epoch 400, Training loss 0.5177, Validation loss 0.4767\n",
      "Epoch 500, Training loss 0.4911, Validation loss 0.4307\n",
      "Epoch 600, Training loss 0.4834, Validation loss 0.4354\n",
      "Epoch 700, Training loss 0.4774, Validation loss 0.4178\n",
      "Epoch 800, Training loss 0.4616, Validation loss 0.4551\n",
      "Epoch 900, Training loss 0.4544, Validation loss 0.4084\n",
      "Epoch 1000, Training loss 0.4514, Validation loss 0.4340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a1b92b79f77440992defa67a2905f89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>Training Loss</td><td>██▇▇▇▇▆▆▅▅▅▅▄▄▅▄▄▃▄▃▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▇▆█▆▇▇▆▇▅█▅▄▅▄▂▂▃▁▂▃▂▁▂▄▁▂▅▄▁▁▂▁▁▂▁▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training Loss</td><td>0.45142</td></tr><tr><td>Validation Loss</td><td>0.43403</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_22_23_30_40</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL/runs/2yn0r3ni' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL/runs/2yn0r3ni</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_233040-2yn0r3ni\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![wandb_activation_function_result](HW2_Titanic_Activation_Function.png)\n",
    "\n",
    "[그림 1] 여러 activation_function별 결과\n",
    "\n",
    "\n",
    "\n",
    "![wandb_config](HW2_Titanic_config.png)\n",
    "\n",
    "그림 2] 2024_10_22_23_30_40의 config\n",
    "\n",
    "\n",
    "epochs 1000을 기준으로 activation_function을 SiLU로 사용했을 때 validation_loss가 가장 작았다.\n",
    "어느 시점에 epochs을 멈추는 것이 좋은지 알기 위해서 early stop을 이용해 overfitting이 발생하기 전의 모델을 사용해 test data를 확인하기로 했다."
   ],
   "id": "62a3e09965c7de5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:45:50.892552Z",
     "start_time": "2024-10-25T07:45:50.886579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping: #모델을 저장하고, early stop 여부를 정하는 클래스\n",
    "    def __init__(\n",
    "            self, patience = 10, delta =0.0001, project_name = None, checkpoint_file_path = None, run_time_str = None\n",
    "    ): #early_stop_patience, 얼마나 차이나는 것을 count 할것인가., 모델 저장 명, 파일을 저장할 경로, 모델 저장 명\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.delta = delta\n",
    "        self.val_loss_min = None\n",
    "        self.file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "        )\n",
    "        self.latest_file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "        )\n",
    "    \n",
    "    def check_and_save(self, new_validation_loss, model): #val_loss를 확인하고, 작아졌는지 확인하고 count 여부를 정하며, 만약 counter가 patience보다 커지면 이를 알린다.\n",
    "        early_stop = False\n",
    "        message = None\n",
    "        \n",
    "        if self.val_loss_min is None:\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            message =f\"Early stopping is stated!\"\n",
    "        elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "            message =f'V_loss decreased ({self.val_loss_min:6.3f} --> {new_validation_loss:6.3f}). Saving model..'\n",
    "            self.save_checkpoint(new_validation_loss, model)\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "            if self.counter >= self.patience:\n",
    "                early_stop = True\n",
    "                message += \"*** TRAIN EARLY STOPPED! ***\"\n",
    "            \n",
    "        return message, early_stop\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model): #모델을 저장하는 함수\n",
    "        torch.save(model.state_dict(), self.file_path)\n",
    "        torch.save(model.state_dict(), self.latest_file_path)\n",
    "        self.val_loss_min = val_loss\n",
    "        "
   ],
   "id": "d1c1622e1069f1c8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T10:33:41.168298Z",
     "start_time": "2024-10-25T10:33:41.152622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassificationTrainer: # 모델의 학습을 담당하는 클래스\n",
    "    def __init__(\n",
    "            self, train_data_loader, validation_data_loader, test_data_loader, run_time_str, wandb, device, checkpoint_file_path\n",
    "    ):\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.validation_data_loader = validation_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.run_time_str = run_time_str\n",
    "        self.wandb = wandb\n",
    "        self.device = device\n",
    "        self.checkpoint_file_path = checkpoint_file_path\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model, self.optimizer = get_model_and_optimizer()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        \n",
    "    def do_train(self): # training 과정\n",
    "        self.model.train()\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        num_corrects_train = 0\n",
    "        num_trained_samples = 0\n",
    "        num_trains = 0\n",
    "        \n",
    "        for train_batch in self.train_data_loader:\n",
    "            input_train = train_batch[\"input\"]\n",
    "            target_train = train_batch[\"target\"].view(-1)\n",
    "            input_train = input_train.to(device=self.device)\n",
    "            target_train = target_train.to(device = self.device)\n",
    "            \n",
    "            output_train = self.model(input_train)\n",
    "            loss = self.loss_fn(output_train, target_train)\n",
    "            loss_train += loss.item()\n",
    "        \n",
    "            predicted_train = torch.argmax(output_train, dim =1 )\n",
    "            \n",
    "            num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "            num_trained_samples += len(input_train)\n",
    "            num_trains += 1\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        train_loss = loss_train / num_trains\n",
    "        train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "        return train_loss, train_accuracy\n",
    "    \n",
    "    def do_validation(self): # validation 과정\n",
    "        self.model.eval()\n",
    "        \n",
    "        loss_validation = 0.0\n",
    "        num_corrects_validation =0\n",
    "        num_validated_samples = 0\n",
    "        num_validations =0 \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for validation_batch in self.validation_data_loader:\n",
    "                input_validation = validation_batch[\"input\"]\n",
    "                target_validation = validation_batch[\"target\"].view(-1)\n",
    "                input_validation = input_validation.to(device = self.device)\n",
    "                target_validation = target_validation.to(device = self.device)\n",
    "                \n",
    "                output_validation = self.model(input_validation)\n",
    "                loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "                \n",
    "                predicted_validation = torch.argmax(output_validation, dim = 1)\n",
    "                num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "                num_validated_samples += len(input_validation)\n",
    "                num_validations += 1\n",
    "                \n",
    "            validation_loss = loss_validation / num_validations\n",
    "            validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "            \n",
    "            return validation_loss, validation_accuracy\n",
    "        \n",
    "    def train_loop(self): # 위에서 만든 do_train과 do_validation을 이용해 학습\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience = self.wandb.config.early_stop_patience, project_name = self.wandb.config.project_name, checkpoint_file_path = self.checkpoint_file_path, run_time_str = self.run_time_str\n",
    "        )\n",
    "        n_epochs = self.wandb.config.epochs\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss, train_accuracy = self.do_train()\n",
    "            \n",
    "            if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "                validation_loss, validation_accuracy = self.do_validation()\n",
    "                \n",
    "                message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "                \n",
    "                print(\n",
    "                    f\"[Epoch {epoch: >3}]\"\n",
    "                    f\"T_loss: {train_loss:6.3f},\"\n",
    "                    f\"T_accuracy: {train_accuracy:6.3f} | \"\n",
    "                    f\"V_loss: {validation_loss:6.3f}, \"\n",
    "                    f\"V_accuracy: {validation_accuracy:6.3f} | \"\n",
    "                    f\"{message} | \"\n",
    "                )\n",
    "                \n",
    "                self.wandb.log({\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Training loss\": train_loss,\n",
    "                    \"Training accuracy (%)\": train_accuracy,\n",
    "                    \"Validation loss\": validation_loss,\n",
    "                    \"Validation accuracy (%)\": validation_accuracy,\n",
    "                })\n",
    "                \n",
    "                if early_stop:\n",
    "                    break\n",
    "                    \n",
    "    def do_test(self): # 학습 후 만들어진 모델을 이용해서 test_data를 예측한 결과를 csv파일로 내보내는 함수\n",
    "        self.model.eval()\n",
    "        \n",
    "        predictions = []\n",
    "        indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_batch = next(iter(self.test_data_loader))\n",
    "            output_test = self.model(test_batch[\"input\"])\n",
    "            predicted_test = torch.argmax(output_test, dim= 1)\n",
    "            for idx, prediction in enumerate(predicted_test, start= 892):\n",
    "                predictions.append(prediction.item())\n",
    "                indices.append(idx)\n",
    "        results_df = pd.DataFrame({\n",
    "            'PassengerId': indices,\n",
    "            'Survived': predictions\n",
    "        })\n",
    "        \n",
    "        results_df.to_csv('submission.csv', index=False)\n",
    "        "
   ],
   "id": "74bd07273927004d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T10:33:41.298483Z",
     "start_time": "2024-10-25T10:33:41.291232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(epochs=1000, batch_size=512, learning_rate=1e-3, early_stop_patience = 10, validation_intervals = 10):\n",
    "    current_time_str = datetime.now().astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    CHECKPOINT_FILE_PATH = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "    config = {\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_hidden_unit_list': [30,30],\n",
    "        'Activation_Function': nn.SiLU(),\n",
    "        'early_stop_patience': early_stop_patience,\n",
    "        'validation_intervals': validation_intervals,\n",
    "        'project_name': \"Titanic\"\n",
    "    }\n",
    "    wandb.init( #wandb설정 값들\n",
    "        mode = \"online\" if wandb else \"disabled\",\n",
    "        project = \"link_DL_titanic\",\n",
    "        notes = \"HW2_Titanic\",\n",
    "        tags = [\"my_model\", \"Titanic\", \"SiLU\"],\n",
    "        name = current_time_str,\n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    \n",
    "    #데이터, 모델, 최적화 방법 로드\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "    \n",
    "    linear_model, optimizer = get_model_and_optimizer()\n",
    "    linear_model.to(device)\n",
    "    \n",
    "    train_data_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    validation_data_loader = DataLoader(dataset = validation_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_data_loader = DataLoader(dataset = test_dataset, batch_size = len(test_dataset))\n",
    "    \n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        train_data_loader=train_data_loader, \n",
    "        validation_data_loader=validation_data_loader, \n",
    "        test_data_loader = test_data_loader,\n",
    "        run_time_str=current_time_str, \n",
    "        wandb=wandb, device=device, \n",
    "        checkpoint_file_path=CHECKPOINT_FILE_PATH\n",
    "    )\n",
    "    classification_trainer.train_loop()\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    classification_trainer.do_test()\n",
    "    \n",
    "    return linear_model"
   ],
   "id": "42bd5a8213f2992b",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:46:05.541397Z",
     "start_time": "2024-10-25T07:45:52.285056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 1000\n",
    "    batch_size = 16\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 10\n",
    "    validation_intervals =10\n",
    "    \n",
    "    model = main(epochs, batch_size, learning_rate, early_stop_patience, validation_intervals)"
   ],
   "id": "dea186842481665a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:u39xztrb) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47e61dda93f0486cacc2680e867da135"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Training accuracy (%)</td><td>▁</td></tr><tr><td>Training loss</td><td>▁</td></tr><tr><td>Validation accuracy (%)</td><td>▁</td></tr><tr><td>Validation loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Training accuracy (%)</td><td>64.65638</td></tr><tr><td>Training loss</td><td>0.67124</td></tr><tr><td>Validation accuracy (%)</td><td>64.04494</td></tr><tr><td>Validation loss</td><td>0.63988</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_25_16_45_16</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/u39xztrb' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/u39xztrb</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_164516-u39xztrb\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:u39xztrb). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241025_164552-jt48aqy0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/jt48aqy0' target=\"_blank\">2024_10_25_16_45_52</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/jt48aqy0' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/jt48aqy0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'SiLU()', 'early_stop_patience': 10, 'validation_intervals': 10, 'project_name': 'Titanic'}\n",
      "Device: cpu\n",
      "[Epoch   1]T_loss:  0.990,T_accuracy: 47.546 | V_loss:  0.647, V_accuracy: 69.101 | Early stopping is stated! | \n",
      "[Epoch  10]T_loss:  0.600,T_accuracy: 68.864 | V_loss:  0.592, V_accuracy: 69.663 | V_loss decreased ( 0.647 -->  0.592). Saving model.. | \n",
      "[Epoch  20]T_loss:  0.600,T_accuracy: 69.425 | V_loss:  0.613, V_accuracy: 67.978 | Early stopping counter: 1 out of 10 | \n",
      "[Epoch  30]T_loss:  0.592,T_accuracy: 68.163 | V_loss:  0.611, V_accuracy: 67.416 | Early stopping counter: 2 out of 10 | \n",
      "[Epoch  40]T_loss:  0.596,T_accuracy: 69.846 | V_loss:  0.642, V_accuracy: 67.416 | Early stopping counter: 3 out of 10 | \n",
      "[Epoch  50]T_loss:  0.590,T_accuracy: 68.864 | V_loss:  0.604, V_accuracy: 65.730 | Early stopping counter: 4 out of 10 | \n",
      "[Epoch  60]T_loss:  0.589,T_accuracy: 70.407 | V_loss:  0.610, V_accuracy: 67.978 | Early stopping counter: 5 out of 10 | \n",
      "[Epoch  70]T_loss:  0.585,T_accuracy: 71.108 | V_loss:  0.604, V_accuracy: 69.101 | Early stopping counter: 6 out of 10 | \n",
      "[Epoch  80]T_loss:  0.584,T_accuracy: 70.126 | V_loss:  0.580, V_accuracy: 67.978 | V_loss decreased ( 0.592 -->  0.580). Saving model.. | \n",
      "[Epoch  90]T_loss:  0.583,T_accuracy: 71.108 | V_loss:  0.589, V_accuracy: 68.539 | Early stopping counter: 1 out of 10 | \n",
      "[Epoch 100]T_loss:  0.577,T_accuracy: 71.529 | V_loss:  0.609, V_accuracy: 68.539 | Early stopping counter: 2 out of 10 | \n",
      "[Epoch 110]T_loss:  0.577,T_accuracy: 71.248 | V_loss:  0.633, V_accuracy: 69.663 | Early stopping counter: 3 out of 10 | \n",
      "[Epoch 120]T_loss:  0.575,T_accuracy: 71.388 | V_loss:  0.589, V_accuracy: 70.225 | Early stopping counter: 4 out of 10 | \n",
      "[Epoch 130]T_loss:  0.569,T_accuracy: 72.090 | V_loss:  0.587, V_accuracy: 70.225 | Early stopping counter: 5 out of 10 | \n",
      "[Epoch 140]T_loss:  0.567,T_accuracy: 71.809 | V_loss:  0.593, V_accuracy: 69.663 | Early stopping counter: 6 out of 10 | \n",
      "[Epoch 150]T_loss:  0.565,T_accuracy: 72.230 | V_loss:  0.598, V_accuracy: 70.225 | Early stopping counter: 7 out of 10 | \n",
      "[Epoch 160]T_loss:  0.565,T_accuracy: 73.072 | V_loss:  0.621, V_accuracy: 70.787 | Early stopping counter: 8 out of 10 | \n",
      "[Epoch 170]T_loss:  0.561,T_accuracy: 72.651 | V_loss:  0.604, V_accuracy: 71.348 | Early stopping counter: 9 out of 10 | \n",
      "[Epoch 180]T_loss:  0.556,T_accuracy: 72.651 | V_loss:  0.605, V_accuracy: 70.225 | Early stopping counter: 10 out of 10*** TRAIN EARLY STOPPED! *** | \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>Training accuracy (%)</td><td>▁▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>Training loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy (%)</td><td>▅▆▄▃▃▁▄▅▄▅▅▆▇▇▆▇▇█▇</td></tr><tr><td>Validation loss</td><td>█▂▄▄▇▄▄▄▁▂▄▇▂▂▂▃▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>180</td></tr><tr><td>Training accuracy (%)</td><td>72.65077</td></tr><tr><td>Training loss</td><td>0.5557</td></tr><tr><td>Validation accuracy (%)</td><td>70.22472</td></tr><tr><td>Validation loss</td><td>0.60512</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_25_16_45_52</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/jt48aqy0' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/jt48aqy0</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_164552-jt48aqy0\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Early_stop1](HW2_Titanic_Early_stop1.png)\n",
    "\n",
    "[그림 3] early_stop적용 결과\n",
    " \n",
    "early_stop_patience = 10\n",
    "validation_intervals = 10\n",
    "인 경우로 18epoch만에 학습이 종료되었고, 8epoch의 경우가 최적의 epoch으로 나왔지만, 이전의 validation_loss 와 비교했을 때 더 큰값을 가지고 있었다.\n",
    "\n",
    "early_stop_patience를 100으로 늘리고, epochs을 10000으로 변경해서 테스트 진행"
   ],
   "id": "d895c18b69f14d87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T09:57:25.534610Z",
     "start_time": "2024-10-25T09:55:49.491084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 10000\n",
    "    batch_size = 16\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 100\n",
    "    validation_intervals =10\n",
    "    \n",
    "    model = main(epochs, batch_size, learning_rate, early_stop_patience, validation_intervals)"
   ],
   "id": "4edde2817184c53c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\PycharmProjects\\link_dl\\git\\link_dl\\_03_your_code\\wandb\\run-20241025_185549-r4nebs7x</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/r4nebs7x' target=\"_blank\">2024_10_25_18_55_49</a></strong> to <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/r4nebs7x' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/r4nebs7x</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 10000, 'batch_size': 16, 'learning_rate': 0.001, 'n_hidden_unit_list': [30, 30], 'Activation_Function': 'SiLU()', 'early_stop_patience': 100, 'validation_intervals': 10, 'project_name': 'Titanic'}\n",
      "Device: cpu\n",
      "[Epoch   1]T_loss:  0.695,T_accuracy: 58.205 | V_loss:  0.709, V_accuracy: 69.663 | Early stopping is stated! | \n",
      "[Epoch  10]T_loss:  0.613,T_accuracy: 68.583 | V_loss:  0.587, V_accuracy: 69.663 | V_loss decreased ( 0.709 -->  0.587). Saving model.. | \n",
      "[Epoch  20]T_loss:  0.613,T_accuracy: 69.004 | V_loss:  0.601, V_accuracy: 66.854 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch  30]T_loss:  0.605,T_accuracy: 69.986 | V_loss:  0.614, V_accuracy: 70.225 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch  40]T_loss:  0.602,T_accuracy: 70.266 | V_loss:  0.628, V_accuracy: 70.225 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch  50]T_loss:  0.599,T_accuracy: 69.986 | V_loss:  0.564, V_accuracy: 70.225 | V_loss decreased ( 0.587 -->  0.564). Saving model.. | \n",
      "[Epoch  60]T_loss:  0.599,T_accuracy: 70.407 | V_loss:  0.607, V_accuracy: 70.225 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch  70]T_loss:  0.593,T_accuracy: 70.407 | V_loss:  0.592, V_accuracy: 69.663 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch  80]T_loss:  0.594,T_accuracy: 69.285 | V_loss:  0.628, V_accuracy: 65.730 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch  90]T_loss:  0.594,T_accuracy: 70.827 | V_loss:  0.597, V_accuracy: 71.910 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 100]T_loss:  0.594,T_accuracy: 70.687 | V_loss:  0.608, V_accuracy: 67.978 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 110]T_loss:  0.588,T_accuracy: 70.407 | V_loss:  0.570, V_accuracy: 69.663 | Early stopping counter: 6 out of 100 | \n",
      "[Epoch 120]T_loss:  0.586,T_accuracy: 71.108 | V_loss:  0.600, V_accuracy: 70.225 | Early stopping counter: 7 out of 100 | \n",
      "[Epoch 130]T_loss:  0.587,T_accuracy: 70.407 | V_loss:  0.659, V_accuracy: 69.663 | Early stopping counter: 8 out of 100 | \n",
      "[Epoch 140]T_loss:  0.582,T_accuracy: 71.108 | V_loss:  0.592, V_accuracy: 71.910 | Early stopping counter: 9 out of 100 | \n",
      "[Epoch 150]T_loss:  0.585,T_accuracy: 70.547 | V_loss:  0.573, V_accuracy: 70.225 | Early stopping counter: 10 out of 100 | \n",
      "[Epoch 160]T_loss:  0.583,T_accuracy: 69.285 | V_loss:  0.599, V_accuracy: 71.910 | Early stopping counter: 11 out of 100 | \n",
      "[Epoch 170]T_loss:  0.573,T_accuracy: 70.827 | V_loss:  0.580, V_accuracy: 71.910 | Early stopping counter: 12 out of 100 | \n",
      "[Epoch 180]T_loss:  0.572,T_accuracy: 69.285 | V_loss:  0.567, V_accuracy: 69.663 | Early stopping counter: 13 out of 100 | \n",
      "[Epoch 190]T_loss:  0.566,T_accuracy: 71.529 | V_loss:  0.572, V_accuracy: 70.787 | Early stopping counter: 14 out of 100 | \n",
      "[Epoch 200]T_loss:  0.568,T_accuracy: 72.370 | V_loss:  0.558, V_accuracy: 70.225 | V_loss decreased ( 0.564 -->  0.558). Saving model.. | \n",
      "[Epoch 210]T_loss:  0.562,T_accuracy: 71.248 | V_loss:  0.595, V_accuracy: 70.787 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 220]T_loss:  0.558,T_accuracy: 71.108 | V_loss:  0.594, V_accuracy: 71.910 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 230]T_loss:  0.549,T_accuracy: 70.827 | V_loss:  0.587, V_accuracy: 73.034 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 240]T_loss:  0.557,T_accuracy: 70.827 | V_loss:  0.586, V_accuracy: 72.472 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 250]T_loss:  0.548,T_accuracy: 71.248 | V_loss:  0.548, V_accuracy: 73.596 | V_loss decreased ( 0.558 -->  0.548). Saving model.. | \n",
      "[Epoch 260]T_loss:  0.552,T_accuracy: 71.108 | V_loss:  0.572, V_accuracy: 70.787 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 270]T_loss:  0.546,T_accuracy: 71.950 | V_loss:  0.574, V_accuracy: 71.348 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 280]T_loss:  0.538,T_accuracy: 72.090 | V_loss:  0.523, V_accuracy: 70.225 | V_loss decreased ( 0.548 -->  0.523). Saving model.. | \n",
      "[Epoch 290]T_loss:  0.537,T_accuracy: 72.651 | V_loss:  0.549, V_accuracy: 74.719 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 300]T_loss:  0.528,T_accuracy: 72.370 | V_loss:  0.556, V_accuracy: 73.034 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 310]T_loss:  0.528,T_accuracy: 71.809 | V_loss:  0.584, V_accuracy: 75.281 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 320]T_loss:  0.529,T_accuracy: 73.212 | V_loss:  0.579, V_accuracy: 72.472 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 330]T_loss:  0.518,T_accuracy: 73.492 | V_loss:  0.504, V_accuracy: 73.596 | V_loss decreased ( 0.523 -->  0.504). Saving model.. | \n",
      "[Epoch 340]T_loss:  0.513,T_accuracy: 73.773 | V_loss:  0.588, V_accuracy: 75.843 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 350]T_loss:  0.509,T_accuracy: 76.157 | V_loss:  0.583, V_accuracy: 66.854 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 360]T_loss:  0.505,T_accuracy: 75.736 | V_loss:  0.522, V_accuracy: 75.281 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 370]T_loss:  0.501,T_accuracy: 76.297 | V_loss:  0.520, V_accuracy: 73.034 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 380]T_loss:  0.494,T_accuracy: 79.523 | V_loss:  0.495, V_accuracy: 75.843 | V_loss decreased ( 0.504 -->  0.495). Saving model.. | \n",
      "[Epoch 390]T_loss:  0.499,T_accuracy: 77.700 | V_loss:  0.530, V_accuracy: 76.966 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 400]T_loss:  0.477,T_accuracy: 77.279 | V_loss:  0.634, V_accuracy: 67.978 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 410]T_loss:  0.480,T_accuracy: 76.999 | V_loss:  0.567, V_accuracy: 74.719 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 420]T_loss:  0.492,T_accuracy: 76.718 | V_loss:  0.512, V_accuracy: 70.225 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 430]T_loss:  0.486,T_accuracy: 77.139 | V_loss:  0.547, V_accuracy: 79.213 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 440]T_loss:  0.482,T_accuracy: 81.487 | V_loss:  0.482, V_accuracy: 73.596 | V_loss decreased ( 0.495 -->  0.482). Saving model.. | \n",
      "[Epoch 450]T_loss:  0.501,T_accuracy: 75.877 | V_loss:  0.487, V_accuracy: 78.652 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 460]T_loss:  0.489,T_accuracy: 75.877 | V_loss:  0.480, V_accuracy: 73.596 | V_loss decreased ( 0.482 -->  0.480). Saving model.. | \n",
      "[Epoch 470]T_loss:  0.492,T_accuracy: 77.980 | V_loss:  0.498, V_accuracy: 77.528 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 480]T_loss:  0.475,T_accuracy: 78.261 | V_loss:  0.474, V_accuracy: 76.404 | V_loss decreased ( 0.480 -->  0.474). Saving model.. | \n",
      "[Epoch 490]T_loss:  0.460,T_accuracy: 80.365 | V_loss:  0.550, V_accuracy: 76.966 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 500]T_loss:  0.485,T_accuracy: 77.980 | V_loss:  0.477, V_accuracy: 76.404 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 510]T_loss:  0.479,T_accuracy: 78.261 | V_loss:  0.554, V_accuracy: 74.157 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 520]T_loss:  0.477,T_accuracy: 78.401 | V_loss:  0.477, V_accuracy: 78.652 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 530]T_loss:  0.463,T_accuracy: 82.188 | V_loss:  0.503, V_accuracy: 77.528 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 540]T_loss:  0.463,T_accuracy: 80.926 | V_loss:  0.460, V_accuracy: 77.528 | V_loss decreased ( 0.474 -->  0.460). Saving model.. | \n",
      "[Epoch 550]T_loss:  0.467,T_accuracy: 78.962 | V_loss:  0.449, V_accuracy: 78.652 | V_loss decreased ( 0.460 -->  0.449). Saving model.. | \n",
      "[Epoch 560]T_loss:  0.467,T_accuracy: 80.224 | V_loss:  0.473, V_accuracy: 76.966 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 570]T_loss:  0.463,T_accuracy: 80.224 | V_loss:  0.568, V_accuracy: 72.472 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 580]T_loss:  0.458,T_accuracy: 80.645 | V_loss:  0.464, V_accuracy: 76.404 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 590]T_loss:  0.449,T_accuracy: 80.084 | V_loss:  0.453, V_accuracy: 78.090 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 600]T_loss:  0.461,T_accuracy: 79.804 | V_loss:  0.608, V_accuracy: 73.034 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 610]T_loss:  0.459,T_accuracy: 79.944 | V_loss:  0.505, V_accuracy: 73.596 | Early stopping counter: 6 out of 100 | \n",
      "[Epoch 620]T_loss:  0.467,T_accuracy: 80.365 | V_loss:  0.473, V_accuracy: 78.090 | Early stopping counter: 7 out of 100 | \n",
      "[Epoch 630]T_loss:  0.481,T_accuracy: 78.401 | V_loss:  0.609, V_accuracy: 75.843 | Early stopping counter: 8 out of 100 | \n",
      "[Epoch 640]T_loss:  0.471,T_accuracy: 79.102 | V_loss:  0.455, V_accuracy: 78.652 | Early stopping counter: 9 out of 100 | \n",
      "[Epoch 650]T_loss:  0.459,T_accuracy: 80.785 | V_loss:  0.496, V_accuracy: 76.404 | Early stopping counter: 10 out of 100 | \n",
      "[Epoch 660]T_loss:  0.457,T_accuracy: 80.785 | V_loss:  0.478, V_accuracy: 75.843 | Early stopping counter: 11 out of 100 | \n",
      "[Epoch 670]T_loss:  0.458,T_accuracy: 79.944 | V_loss:  0.458, V_accuracy: 77.528 | Early stopping counter: 12 out of 100 | \n",
      "[Epoch 680]T_loss:  0.439,T_accuracy: 80.926 | V_loss:  0.532, V_accuracy: 75.843 | Early stopping counter: 13 out of 100 | \n",
      "[Epoch 690]T_loss:  0.458,T_accuracy: 81.206 | V_loss:  0.477, V_accuracy: 75.281 | Early stopping counter: 14 out of 100 | \n",
      "[Epoch 700]T_loss:  0.450,T_accuracy: 78.822 | V_loss:  0.470, V_accuracy: 76.966 | Early stopping counter: 15 out of 100 | \n",
      "[Epoch 710]T_loss:  0.436,T_accuracy: 81.206 | V_loss:  0.486, V_accuracy: 74.157 | Early stopping counter: 16 out of 100 | \n",
      "[Epoch 720]T_loss:  0.450,T_accuracy: 79.804 | V_loss:  0.445, V_accuracy: 76.404 | V_loss decreased ( 0.449 -->  0.445). Saving model.. | \n",
      "[Epoch 730]T_loss:  0.450,T_accuracy: 78.822 | V_loss:  0.623, V_accuracy: 72.472 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 740]T_loss:  0.454,T_accuracy: 80.645 | V_loss:  0.448, V_accuracy: 75.843 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 750]T_loss:  0.447,T_accuracy: 81.066 | V_loss:  0.525, V_accuracy: 73.034 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 760]T_loss:  0.455,T_accuracy: 79.243 | V_loss:  0.486, V_accuracy: 78.652 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 770]T_loss:  0.456,T_accuracy: 80.224 | V_loss:  0.524, V_accuracy: 77.528 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 780]T_loss:  0.440,T_accuracy: 81.487 | V_loss:  0.512, V_accuracy: 75.843 | Early stopping counter: 6 out of 100 | \n",
      "[Epoch 790]T_loss:  0.459,T_accuracy: 81.066 | V_loss:  0.498, V_accuracy: 75.843 | Early stopping counter: 7 out of 100 | \n",
      "[Epoch 800]T_loss:  0.463,T_accuracy: 79.243 | V_loss:  0.635, V_accuracy: 78.652 | Early stopping counter: 8 out of 100 | \n",
      "[Epoch 810]T_loss:  0.457,T_accuracy: 79.383 | V_loss:  0.501, V_accuracy: 76.966 | Early stopping counter: 9 out of 100 | \n",
      "[Epoch 820]T_loss:  0.452,T_accuracy: 80.365 | V_loss:  0.529, V_accuracy: 79.213 | Early stopping counter: 10 out of 100 | \n",
      "[Epoch 830]T_loss:  0.459,T_accuracy: 80.365 | V_loss:  0.517, V_accuracy: 74.719 | Early stopping counter: 11 out of 100 | \n",
      "[Epoch 840]T_loss:  0.455,T_accuracy: 80.365 | V_loss:  0.470, V_accuracy: 75.843 | Early stopping counter: 12 out of 100 | \n",
      "[Epoch 850]T_loss:  0.442,T_accuracy: 80.505 | V_loss:  0.482, V_accuracy: 78.652 | Early stopping counter: 13 out of 100 | \n",
      "[Epoch 860]T_loss:  0.432,T_accuracy: 81.627 | V_loss:  0.588, V_accuracy: 75.281 | Early stopping counter: 14 out of 100 | \n",
      "[Epoch 870]T_loss:  0.452,T_accuracy: 80.084 | V_loss:  0.451, V_accuracy: 75.281 | Early stopping counter: 15 out of 100 | \n",
      "[Epoch 880]T_loss:  0.445,T_accuracy: 80.365 | V_loss:  0.460, V_accuracy: 79.213 | Early stopping counter: 16 out of 100 | \n",
      "[Epoch 890]T_loss:  0.440,T_accuracy: 79.944 | V_loss:  0.537, V_accuracy: 75.281 | Early stopping counter: 17 out of 100 | \n",
      "[Epoch 900]T_loss:  0.439,T_accuracy: 81.627 | V_loss:  0.540, V_accuracy: 75.843 | Early stopping counter: 18 out of 100 | \n",
      "[Epoch 910]T_loss:  0.436,T_accuracy: 81.206 | V_loss:  0.475, V_accuracy: 79.213 | Early stopping counter: 19 out of 100 | \n",
      "[Epoch 920]T_loss:  0.429,T_accuracy: 81.487 | V_loss:  0.572, V_accuracy: 74.719 | Early stopping counter: 20 out of 100 | \n",
      "[Epoch 930]T_loss:  0.448,T_accuracy: 81.627 | V_loss:  0.461, V_accuracy: 77.528 | Early stopping counter: 21 out of 100 | \n",
      "[Epoch 940]T_loss:  0.450,T_accuracy: 81.206 | V_loss:  0.588, V_accuracy: 73.034 | Early stopping counter: 22 out of 100 | \n",
      "[Epoch 950]T_loss:  0.440,T_accuracy: 81.066 | V_loss:  0.522, V_accuracy: 76.966 | Early stopping counter: 23 out of 100 | \n",
      "[Epoch 960]T_loss:  0.459,T_accuracy: 80.505 | V_loss:  0.512, V_accuracy: 75.281 | Early stopping counter: 24 out of 100 | \n",
      "[Epoch 970]T_loss:  0.431,T_accuracy: 81.907 | V_loss:  0.479, V_accuracy: 75.281 | Early stopping counter: 25 out of 100 | \n",
      "[Epoch 980]T_loss:  0.446,T_accuracy: 80.926 | V_loss:  0.482, V_accuracy: 77.528 | Early stopping counter: 26 out of 100 | \n",
      "[Epoch 990]T_loss:  0.434,T_accuracy: 82.048 | V_loss:  0.537, V_accuracy: 74.719 | Early stopping counter: 27 out of 100 | \n",
      "[Epoch 1000]T_loss:  0.432,T_accuracy: 82.048 | V_loss:  0.604, V_accuracy: 75.281 | Early stopping counter: 28 out of 100 | \n",
      "[Epoch 1010]T_loss:  0.446,T_accuracy: 80.224 | V_loss:  0.487, V_accuracy: 78.652 | Early stopping counter: 29 out of 100 | \n",
      "[Epoch 1020]T_loss:  0.441,T_accuracy: 81.066 | V_loss:  0.446, V_accuracy: 78.090 | Early stopping counter: 30 out of 100 | \n",
      "[Epoch 1030]T_loss:  0.422,T_accuracy: 81.907 | V_loss:  0.537, V_accuracy: 75.281 | Early stopping counter: 31 out of 100 | \n",
      "[Epoch 1040]T_loss:  0.441,T_accuracy: 80.785 | V_loss:  0.493, V_accuracy: 80.337 | Early stopping counter: 32 out of 100 | \n",
      "[Epoch 1050]T_loss:  0.439,T_accuracy: 81.066 | V_loss:  0.547, V_accuracy: 74.719 | Early stopping counter: 33 out of 100 | \n",
      "[Epoch 1060]T_loss:  0.434,T_accuracy: 80.084 | V_loss:  0.470, V_accuracy: 75.281 | Early stopping counter: 34 out of 100 | \n",
      "[Epoch 1070]T_loss:  0.434,T_accuracy: 80.505 | V_loss:  0.522, V_accuracy: 75.843 | Early stopping counter: 35 out of 100 | \n",
      "[Epoch 1080]T_loss:  0.445,T_accuracy: 80.084 | V_loss:  0.480, V_accuracy: 75.281 | Early stopping counter: 36 out of 100 | \n",
      "[Epoch 1090]T_loss:  0.432,T_accuracy: 81.066 | V_loss:  0.529, V_accuracy: 78.090 | Early stopping counter: 37 out of 100 | \n",
      "[Epoch 1100]T_loss:  0.437,T_accuracy: 81.487 | V_loss:  0.464, V_accuracy: 76.966 | Early stopping counter: 38 out of 100 | \n",
      "[Epoch 1110]T_loss:  0.427,T_accuracy: 81.206 | V_loss:  0.587, V_accuracy: 75.281 | Early stopping counter: 39 out of 100 | \n",
      "[Epoch 1120]T_loss:  0.434,T_accuracy: 81.767 | V_loss:  0.498, V_accuracy: 75.281 | Early stopping counter: 40 out of 100 | \n",
      "[Epoch 1130]T_loss:  0.433,T_accuracy: 80.785 | V_loss:  0.534, V_accuracy: 74.719 | Early stopping counter: 41 out of 100 | \n",
      "[Epoch 1140]T_loss:  0.432,T_accuracy: 80.365 | V_loss:  0.536, V_accuracy: 79.213 | Early stopping counter: 42 out of 100 | \n",
      "[Epoch 1150]T_loss:  0.433,T_accuracy: 80.505 | V_loss:  0.626, V_accuracy: 75.281 | Early stopping counter: 43 out of 100 | \n",
      "[Epoch 1160]T_loss:  0.435,T_accuracy: 80.926 | V_loss:  0.474, V_accuracy: 75.281 | Early stopping counter: 44 out of 100 | \n",
      "[Epoch 1170]T_loss:  0.428,T_accuracy: 80.645 | V_loss:  0.516, V_accuracy: 75.281 | Early stopping counter: 45 out of 100 | \n",
      "[Epoch 1180]T_loss:  0.437,T_accuracy: 80.785 | V_loss:  0.498, V_accuracy: 79.213 | Early stopping counter: 46 out of 100 | \n",
      "[Epoch 1190]T_loss:  0.438,T_accuracy: 80.645 | V_loss:  0.503, V_accuracy: 79.213 | Early stopping counter: 47 out of 100 | \n",
      "[Epoch 1200]T_loss:  0.435,T_accuracy: 80.785 | V_loss:  0.452, V_accuracy: 79.775 | Early stopping counter: 48 out of 100 | \n",
      "[Epoch 1210]T_loss:  0.441,T_accuracy: 80.785 | V_loss:  0.519, V_accuracy: 75.843 | Early stopping counter: 49 out of 100 | \n",
      "[Epoch 1220]T_loss:  0.443,T_accuracy: 80.785 | V_loss:  0.567, V_accuracy: 79.775 | Early stopping counter: 50 out of 100 | \n",
      "[Epoch 1230]T_loss:  0.427,T_accuracy: 81.767 | V_loss:  0.692, V_accuracy: 71.910 | Early stopping counter: 51 out of 100 | \n",
      "[Epoch 1240]T_loss:  0.425,T_accuracy: 80.785 | V_loss:  0.506, V_accuracy: 75.281 | Early stopping counter: 52 out of 100 | \n",
      "[Epoch 1250]T_loss:  0.430,T_accuracy: 82.048 | V_loss:  0.593, V_accuracy: 79.213 | Early stopping counter: 53 out of 100 | \n",
      "[Epoch 1260]T_loss:  0.438,T_accuracy: 81.346 | V_loss:  0.488, V_accuracy: 75.281 | Early stopping counter: 54 out of 100 | \n",
      "[Epoch 1270]T_loss:  0.426,T_accuracy: 81.206 | V_loss:  0.457, V_accuracy: 77.528 | Early stopping counter: 55 out of 100 | \n",
      "[Epoch 1280]T_loss:  0.431,T_accuracy: 80.365 | V_loss:  0.499, V_accuracy: 76.966 | Early stopping counter: 56 out of 100 | \n",
      "[Epoch 1290]T_loss:  0.445,T_accuracy: 81.767 | V_loss:  0.523, V_accuracy: 74.719 | Early stopping counter: 57 out of 100 | \n",
      "[Epoch 1300]T_loss:  0.421,T_accuracy: 81.346 | V_loss:  0.548, V_accuracy: 78.652 | Early stopping counter: 58 out of 100 | \n",
      "[Epoch 1310]T_loss:  0.436,T_accuracy: 80.505 | V_loss:  0.491, V_accuracy: 78.090 | Early stopping counter: 59 out of 100 | \n",
      "[Epoch 1320]T_loss:  0.438,T_accuracy: 80.224 | V_loss:  0.493, V_accuracy: 79.213 | Early stopping counter: 60 out of 100 | \n",
      "[Epoch 1330]T_loss:  0.430,T_accuracy: 80.785 | V_loss:  0.520, V_accuracy: 76.404 | Early stopping counter: 61 out of 100 | \n",
      "[Epoch 1340]T_loss:  0.433,T_accuracy: 81.346 | V_loss:  0.483, V_accuracy: 80.337 | Early stopping counter: 62 out of 100 | \n",
      "[Epoch 1350]T_loss:  0.444,T_accuracy: 79.383 | V_loss:  0.556, V_accuracy: 80.337 | Early stopping counter: 63 out of 100 | \n",
      "[Epoch 1360]T_loss:  0.421,T_accuracy: 81.346 | V_loss:  0.484, V_accuracy: 77.528 | Early stopping counter: 64 out of 100 | \n",
      "[Epoch 1370]T_loss:  0.431,T_accuracy: 80.926 | V_loss:  0.482, V_accuracy: 75.281 | Early stopping counter: 65 out of 100 | \n",
      "[Epoch 1380]T_loss:  0.422,T_accuracy: 81.627 | V_loss:  0.478, V_accuracy: 76.404 | Early stopping counter: 66 out of 100 | \n",
      "[Epoch 1390]T_loss:  0.432,T_accuracy: 82.328 | V_loss:  0.542, V_accuracy: 75.281 | Early stopping counter: 67 out of 100 | \n",
      "[Epoch 1400]T_loss:  0.437,T_accuracy: 81.206 | V_loss:  0.632, V_accuracy: 69.663 | Early stopping counter: 68 out of 100 | \n",
      "[Epoch 1410]T_loss:  0.433,T_accuracy: 80.785 | V_loss:  0.470, V_accuracy: 77.528 | Early stopping counter: 69 out of 100 | \n",
      "[Epoch 1420]T_loss:  0.421,T_accuracy: 80.084 | V_loss:  0.450, V_accuracy: 79.213 | Early stopping counter: 70 out of 100 | \n",
      "[Epoch 1430]T_loss:  0.423,T_accuracy: 81.487 | V_loss:  0.503, V_accuracy: 78.652 | Early stopping counter: 71 out of 100 | \n",
      "[Epoch 1440]T_loss:  0.427,T_accuracy: 81.206 | V_loss:  0.519, V_accuracy: 75.281 | Early stopping counter: 72 out of 100 | \n",
      "[Epoch 1450]T_loss:  0.439,T_accuracy: 80.926 | V_loss:  0.512, V_accuracy: 75.281 | Early stopping counter: 73 out of 100 | \n",
      "[Epoch 1460]T_loss:  0.429,T_accuracy: 81.627 | V_loss:  0.488, V_accuracy: 79.775 | Early stopping counter: 74 out of 100 | \n",
      "[Epoch 1470]T_loss:  0.435,T_accuracy: 80.645 | V_loss:  0.575, V_accuracy: 75.843 | Early stopping counter: 75 out of 100 | \n",
      "[Epoch 1480]T_loss:  0.425,T_accuracy: 80.505 | V_loss:  0.489, V_accuracy: 74.719 | Early stopping counter: 76 out of 100 | \n",
      "[Epoch 1490]T_loss:  0.416,T_accuracy: 82.188 | V_loss:  0.462, V_accuracy: 74.719 | Early stopping counter: 77 out of 100 | \n",
      "[Epoch 1500]T_loss:  0.423,T_accuracy: 82.328 | V_loss:  0.551, V_accuracy: 75.281 | Early stopping counter: 78 out of 100 | \n",
      "[Epoch 1510]T_loss:  0.420,T_accuracy: 82.328 | V_loss:  0.533, V_accuracy: 76.404 | Early stopping counter: 79 out of 100 | \n",
      "[Epoch 1520]T_loss:  0.415,T_accuracy: 82.749 | V_loss:  0.538, V_accuracy: 74.157 | Early stopping counter: 80 out of 100 | \n",
      "[Epoch 1530]T_loss:  0.430,T_accuracy: 80.084 | V_loss:  0.500, V_accuracy: 78.652 | Early stopping counter: 81 out of 100 | \n",
      "[Epoch 1540]T_loss:  0.427,T_accuracy: 80.224 | V_loss:  0.454, V_accuracy: 78.652 | Early stopping counter: 82 out of 100 | \n",
      "[Epoch 1550]T_loss:  0.427,T_accuracy: 81.206 | V_loss:  0.467, V_accuracy: 76.966 | Early stopping counter: 83 out of 100 | \n",
      "[Epoch 1560]T_loss:  0.423,T_accuracy: 80.505 | V_loss:  0.481, V_accuracy: 78.652 | Early stopping counter: 84 out of 100 | \n",
      "[Epoch 1570]T_loss:  0.417,T_accuracy: 82.188 | V_loss:  0.570, V_accuracy: 77.528 | Early stopping counter: 85 out of 100 | \n",
      "[Epoch 1580]T_loss:  0.426,T_accuracy: 82.048 | V_loss:  0.522, V_accuracy: 78.090 | Early stopping counter: 86 out of 100 | \n",
      "[Epoch 1590]T_loss:  0.423,T_accuracy: 81.487 | V_loss:  0.623, V_accuracy: 74.157 | Early stopping counter: 87 out of 100 | \n",
      "[Epoch 1600]T_loss:  0.430,T_accuracy: 81.206 | V_loss:  0.569, V_accuracy: 76.966 | Early stopping counter: 88 out of 100 | \n",
      "[Epoch 1610]T_loss:  0.424,T_accuracy: 82.048 | V_loss:  0.518, V_accuracy: 74.157 | Early stopping counter: 89 out of 100 | \n",
      "[Epoch 1620]T_loss:  0.422,T_accuracy: 81.346 | V_loss:  0.461, V_accuracy: 76.404 | Early stopping counter: 90 out of 100 | \n",
      "[Epoch 1630]T_loss:  0.426,T_accuracy: 81.346 | V_loss:  0.479, V_accuracy: 79.213 | Early stopping counter: 91 out of 100 | \n",
      "[Epoch 1640]T_loss:  0.425,T_accuracy: 81.346 | V_loss:  0.474, V_accuracy: 80.337 | Early stopping counter: 92 out of 100 | \n",
      "[Epoch 1650]T_loss:  0.423,T_accuracy: 81.627 | V_loss:  0.507, V_accuracy: 79.213 | Early stopping counter: 93 out of 100 | \n",
      "[Epoch 1660]T_loss:  0.416,T_accuracy: 81.346 | V_loss:  0.487, V_accuracy: 82.022 | Early stopping counter: 94 out of 100 | \n",
      "[Epoch 1670]T_loss:  0.421,T_accuracy: 81.346 | V_loss:  0.442, V_accuracy: 76.404 | V_loss decreased ( 0.445 -->  0.442). Saving model.. | \n",
      "[Epoch 1680]T_loss:  0.439,T_accuracy: 81.627 | V_loss:  0.472, V_accuracy: 76.404 | Early stopping counter: 1 out of 100 | \n",
      "[Epoch 1690]T_loss:  0.422,T_accuracy: 81.206 | V_loss:  0.529, V_accuracy: 75.281 | Early stopping counter: 2 out of 100 | \n",
      "[Epoch 1700]T_loss:  0.422,T_accuracy: 82.328 | V_loss:  0.505, V_accuracy: 76.966 | Early stopping counter: 3 out of 100 | \n",
      "[Epoch 1710]T_loss:  0.421,T_accuracy: 81.346 | V_loss:  0.477, V_accuracy: 77.528 | Early stopping counter: 4 out of 100 | \n",
      "[Epoch 1720]T_loss:  0.437,T_accuracy: 81.066 | V_loss:  0.521, V_accuracy: 75.843 | Early stopping counter: 5 out of 100 | \n",
      "[Epoch 1730]T_loss:  0.421,T_accuracy: 81.767 | V_loss:  0.479, V_accuracy: 76.404 | Early stopping counter: 6 out of 100 | \n",
      "[Epoch 1740]T_loss:  0.428,T_accuracy: 81.206 | V_loss:  0.465, V_accuracy: 76.404 | Early stopping counter: 7 out of 100 | \n",
      "[Epoch 1750]T_loss:  0.422,T_accuracy: 81.346 | V_loss:  0.546, V_accuracy: 79.213 | Early stopping counter: 8 out of 100 | \n",
      "[Epoch 1760]T_loss:  0.434,T_accuracy: 80.645 | V_loss:  0.574, V_accuracy: 76.404 | Early stopping counter: 9 out of 100 | \n",
      "[Epoch 1770]T_loss:  0.429,T_accuracy: 81.767 | V_loss:  0.665, V_accuracy: 75.843 | Early stopping counter: 10 out of 100 | \n",
      "[Epoch 1780]T_loss:  0.433,T_accuracy: 81.206 | V_loss:  0.466, V_accuracy: 76.966 | Early stopping counter: 11 out of 100 | \n",
      "[Epoch 1790]T_loss:  0.418,T_accuracy: 81.627 | V_loss:  0.461, V_accuracy: 79.775 | Early stopping counter: 12 out of 100 | \n",
      "[Epoch 1800]T_loss:  0.419,T_accuracy: 82.188 | V_loss:  0.478, V_accuracy: 74.719 | Early stopping counter: 13 out of 100 | \n",
      "[Epoch 1810]T_loss:  0.426,T_accuracy: 81.066 | V_loss:  0.452, V_accuracy: 75.281 | Early stopping counter: 14 out of 100 | \n",
      "[Epoch 1820]T_loss:  0.412,T_accuracy: 82.749 | V_loss:  0.473, V_accuracy: 78.090 | Early stopping counter: 15 out of 100 | \n",
      "[Epoch 1830]T_loss:  0.415,T_accuracy: 81.487 | V_loss:  0.544, V_accuracy: 78.090 | Early stopping counter: 16 out of 100 | \n",
      "[Epoch 1840]T_loss:  0.422,T_accuracy: 80.785 | V_loss:  0.568, V_accuracy: 78.090 | Early stopping counter: 17 out of 100 | \n",
      "[Epoch 1850]T_loss:  0.426,T_accuracy: 80.926 | V_loss:  0.579, V_accuracy: 78.090 | Early stopping counter: 18 out of 100 | \n",
      "[Epoch 1860]T_loss:  0.418,T_accuracy: 82.468 | V_loss:  0.462, V_accuracy: 79.775 | Early stopping counter: 19 out of 100 | \n",
      "[Epoch 1870]T_loss:  0.429,T_accuracy: 80.926 | V_loss:  0.491, V_accuracy: 75.843 | Early stopping counter: 20 out of 100 | \n",
      "[Epoch 1880]T_loss:  0.416,T_accuracy: 82.188 | V_loss:  0.606, V_accuracy: 76.404 | Early stopping counter: 21 out of 100 | \n",
      "[Epoch 1890]T_loss:  0.415,T_accuracy: 82.609 | V_loss:  0.564, V_accuracy: 78.090 | Early stopping counter: 22 out of 100 | \n",
      "[Epoch 1900]T_loss:  0.418,T_accuracy: 82.048 | V_loss:  0.443, V_accuracy: 75.843 | Early stopping counter: 23 out of 100 | \n",
      "[Epoch 1910]T_loss:  0.422,T_accuracy: 82.328 | V_loss:  0.504, V_accuracy: 74.719 | Early stopping counter: 24 out of 100 | \n",
      "[Epoch 1920]T_loss:  0.419,T_accuracy: 81.346 | V_loss:  0.508, V_accuracy: 74.719 | Early stopping counter: 25 out of 100 | \n",
      "[Epoch 1930]T_loss:  0.425,T_accuracy: 81.206 | V_loss:  0.515, V_accuracy: 80.337 | Early stopping counter: 26 out of 100 | \n",
      "[Epoch 1940]T_loss:  0.422,T_accuracy: 81.346 | V_loss:  0.452, V_accuracy: 76.966 | Early stopping counter: 27 out of 100 | \n",
      "[Epoch 1950]T_loss:  0.429,T_accuracy: 80.926 | V_loss:  0.494, V_accuracy: 78.090 | Early stopping counter: 28 out of 100 | \n",
      "[Epoch 1960]T_loss:  0.420,T_accuracy: 81.487 | V_loss:  0.477, V_accuracy: 76.404 | Early stopping counter: 29 out of 100 | \n",
      "[Epoch 1970]T_loss:  0.415,T_accuracy: 82.048 | V_loss:  0.510, V_accuracy: 80.337 | Early stopping counter: 30 out of 100 | \n",
      "[Epoch 1980]T_loss:  0.412,T_accuracy: 82.188 | V_loss:  0.468, V_accuracy: 75.281 | Early stopping counter: 31 out of 100 | \n",
      "[Epoch 1990]T_loss:  0.408,T_accuracy: 81.346 | V_loss:  0.465, V_accuracy: 78.652 | Early stopping counter: 32 out of 100 | \n",
      "[Epoch 2000]T_loss:  0.409,T_accuracy: 81.907 | V_loss:  0.538, V_accuracy: 76.966 | Early stopping counter: 33 out of 100 | \n",
      "[Epoch 2010]T_loss:  0.423,T_accuracy: 82.188 | V_loss:  0.470, V_accuracy: 74.719 | Early stopping counter: 34 out of 100 | \n",
      "[Epoch 2020]T_loss:  0.419,T_accuracy: 80.645 | V_loss:  0.508, V_accuracy: 78.652 | Early stopping counter: 35 out of 100 | \n",
      "[Epoch 2030]T_loss:  0.409,T_accuracy: 82.889 | V_loss:  0.458, V_accuracy: 75.843 | Early stopping counter: 36 out of 100 | \n",
      "[Epoch 2040]T_loss:  0.418,T_accuracy: 81.907 | V_loss:  0.445, V_accuracy: 78.090 | Early stopping counter: 37 out of 100 | \n",
      "[Epoch 2050]T_loss:  0.420,T_accuracy: 81.907 | V_loss:  0.581, V_accuracy: 75.843 | Early stopping counter: 38 out of 100 | \n",
      "[Epoch 2060]T_loss:  0.417,T_accuracy: 81.627 | V_loss:  0.598, V_accuracy: 76.404 | Early stopping counter: 39 out of 100 | \n",
      "[Epoch 2070]T_loss:  0.409,T_accuracy: 81.066 | V_loss:  0.450, V_accuracy: 79.775 | Early stopping counter: 40 out of 100 | \n",
      "[Epoch 2080]T_loss:  0.414,T_accuracy: 81.066 | V_loss:  0.473, V_accuracy: 77.528 | Early stopping counter: 41 out of 100 | \n",
      "[Epoch 2090]T_loss:  0.414,T_accuracy: 82.048 | V_loss:  0.579, V_accuracy: 79.213 | Early stopping counter: 42 out of 100 | \n",
      "[Epoch 2100]T_loss:  0.421,T_accuracy: 81.767 | V_loss:  0.449, V_accuracy: 79.213 | Early stopping counter: 43 out of 100 | \n",
      "[Epoch 2110]T_loss:  0.433,T_accuracy: 80.785 | V_loss:  0.563, V_accuracy: 76.404 | Early stopping counter: 44 out of 100 | \n",
      "[Epoch 2120]T_loss:  0.411,T_accuracy: 81.487 | V_loss:  0.518, V_accuracy: 79.213 | Early stopping counter: 45 out of 100 | \n",
      "[Epoch 2130]T_loss:  0.410,T_accuracy: 82.328 | V_loss:  0.494, V_accuracy: 79.775 | Early stopping counter: 46 out of 100 | \n",
      "[Epoch 2140]T_loss:  0.409,T_accuracy: 82.188 | V_loss:  0.559, V_accuracy: 79.775 | Early stopping counter: 47 out of 100 | \n",
      "[Epoch 2150]T_loss:  0.412,T_accuracy: 81.487 | V_loss:  0.451, V_accuracy: 75.843 | Early stopping counter: 48 out of 100 | \n",
      "[Epoch 2160]T_loss:  0.411,T_accuracy: 82.749 | V_loss:  0.479, V_accuracy: 76.966 | Early stopping counter: 49 out of 100 | \n",
      "[Epoch 2170]T_loss:  0.416,T_accuracy: 81.907 | V_loss:  0.529, V_accuracy: 78.652 | Early stopping counter: 50 out of 100 | \n",
      "[Epoch 2180]T_loss:  0.412,T_accuracy: 82.749 | V_loss:  0.599, V_accuracy: 75.843 | Early stopping counter: 51 out of 100 | \n",
      "[Epoch 2190]T_loss:  0.411,T_accuracy: 82.048 | V_loss:  0.467, V_accuracy: 76.966 | Early stopping counter: 52 out of 100 | \n",
      "[Epoch 2200]T_loss:  0.417,T_accuracy: 82.468 | V_loss:  0.510, V_accuracy: 76.404 | Early stopping counter: 53 out of 100 | \n",
      "[Epoch 2210]T_loss:  0.414,T_accuracy: 82.188 | V_loss:  0.472, V_accuracy: 76.404 | Early stopping counter: 54 out of 100 | \n",
      "[Epoch 2220]T_loss:  0.417,T_accuracy: 82.749 | V_loss:  0.470, V_accuracy: 76.966 | Early stopping counter: 55 out of 100 | \n",
      "[Epoch 2230]T_loss:  0.422,T_accuracy: 80.785 | V_loss:  0.506, V_accuracy: 78.652 | Early stopping counter: 56 out of 100 | \n",
      "[Epoch 2240]T_loss:  0.407,T_accuracy: 82.188 | V_loss:  0.462, V_accuracy: 79.213 | Early stopping counter: 57 out of 100 | \n",
      "[Epoch 2250]T_loss:  0.416,T_accuracy: 80.926 | V_loss:  0.506, V_accuracy: 78.090 | Early stopping counter: 58 out of 100 | \n",
      "[Epoch 2260]T_loss:  0.409,T_accuracy: 82.609 | V_loss:  0.536, V_accuracy: 79.213 | Early stopping counter: 59 out of 100 | \n",
      "[Epoch 2270]T_loss:  0.416,T_accuracy: 82.188 | V_loss:  0.456, V_accuracy: 80.899 | Early stopping counter: 60 out of 100 | \n",
      "[Epoch 2280]T_loss:  0.411,T_accuracy: 83.170 | V_loss:  0.487, V_accuracy: 80.337 | Early stopping counter: 61 out of 100 | \n",
      "[Epoch 2290]T_loss:  0.406,T_accuracy: 81.346 | V_loss:  0.496, V_accuracy: 79.213 | Early stopping counter: 62 out of 100 | \n",
      "[Epoch 2300]T_loss:  0.415,T_accuracy: 81.487 | V_loss:  0.511, V_accuracy: 78.652 | Early stopping counter: 63 out of 100 | \n",
      "[Epoch 2310]T_loss:  0.415,T_accuracy: 82.609 | V_loss:  0.593, V_accuracy: 76.404 | Early stopping counter: 64 out of 100 | \n",
      "[Epoch 2320]T_loss:  0.409,T_accuracy: 81.627 | V_loss:  0.500, V_accuracy: 76.404 | Early stopping counter: 65 out of 100 | \n",
      "[Epoch 2330]T_loss:  0.428,T_accuracy: 82.328 | V_loss:  0.504, V_accuracy: 76.404 | Early stopping counter: 66 out of 100 | \n",
      "[Epoch 2340]T_loss:  0.417,T_accuracy: 81.627 | V_loss:  0.543, V_accuracy: 76.404 | Early stopping counter: 67 out of 100 | \n",
      "[Epoch 2350]T_loss:  0.423,T_accuracy: 82.328 | V_loss:  0.489, V_accuracy: 79.213 | Early stopping counter: 68 out of 100 | \n",
      "[Epoch 2360]T_loss:  0.412,T_accuracy: 82.048 | V_loss:  0.446, V_accuracy: 76.404 | Early stopping counter: 69 out of 100 | \n",
      "[Epoch 2370]T_loss:  0.422,T_accuracy: 82.188 | V_loss:  0.477, V_accuracy: 80.899 | Early stopping counter: 70 out of 100 | \n",
      "[Epoch 2380]T_loss:  0.404,T_accuracy: 82.609 | V_loss:  0.536, V_accuracy: 79.213 | Early stopping counter: 71 out of 100 | \n",
      "[Epoch 2390]T_loss:  0.413,T_accuracy: 81.907 | V_loss:  0.536, V_accuracy: 76.404 | Early stopping counter: 72 out of 100 | \n",
      "[Epoch 2400]T_loss:  0.406,T_accuracy: 82.468 | V_loss:  0.460, V_accuracy: 79.775 | Early stopping counter: 73 out of 100 | \n",
      "[Epoch 2410]T_loss:  0.414,T_accuracy: 82.048 | V_loss:  0.611, V_accuracy: 73.034 | Early stopping counter: 74 out of 100 | \n",
      "[Epoch 2420]T_loss:  0.412,T_accuracy: 80.926 | V_loss:  0.485, V_accuracy: 80.899 | Early stopping counter: 75 out of 100 | \n",
      "[Epoch 2430]T_loss:  0.416,T_accuracy: 82.468 | V_loss:  0.567, V_accuracy: 78.090 | Early stopping counter: 76 out of 100 | \n",
      "[Epoch 2440]T_loss:  0.409,T_accuracy: 82.468 | V_loss:  0.498, V_accuracy: 75.843 | Early stopping counter: 77 out of 100 | \n",
      "[Epoch 2450]T_loss:  0.409,T_accuracy: 82.749 | V_loss:  0.446, V_accuracy: 77.528 | Early stopping counter: 78 out of 100 | \n",
      "[Epoch 2460]T_loss:  0.414,T_accuracy: 82.188 | V_loss:  0.482, V_accuracy: 77.528 | Early stopping counter: 79 out of 100 | \n",
      "[Epoch 2470]T_loss:  0.412,T_accuracy: 82.749 | V_loss:  0.506, V_accuracy: 78.652 | Early stopping counter: 80 out of 100 | \n",
      "[Epoch 2480]T_loss:  0.404,T_accuracy: 81.907 | V_loss:  0.503, V_accuracy: 75.843 | Early stopping counter: 81 out of 100 | \n",
      "[Epoch 2490]T_loss:  0.407,T_accuracy: 82.609 | V_loss:  0.493, V_accuracy: 75.843 | Early stopping counter: 82 out of 100 | \n",
      "[Epoch 2500]T_loss:  0.408,T_accuracy: 80.926 | V_loss:  0.466, V_accuracy: 81.461 | Early stopping counter: 83 out of 100 | \n",
      "[Epoch 2510]T_loss:  0.413,T_accuracy: 81.487 | V_loss:  0.541, V_accuracy: 76.404 | Early stopping counter: 84 out of 100 | \n",
      "[Epoch 2520]T_loss:  0.409,T_accuracy: 82.048 | V_loss:  0.595, V_accuracy: 76.404 | Early stopping counter: 85 out of 100 | \n",
      "[Epoch 2530]T_loss:  0.404,T_accuracy: 83.170 | V_loss:  0.496, V_accuracy: 78.090 | Early stopping counter: 86 out of 100 | \n",
      "[Epoch 2540]T_loss:  0.407,T_accuracy: 82.468 | V_loss:  0.576, V_accuracy: 75.281 | Early stopping counter: 87 out of 100 | \n",
      "[Epoch 2550]T_loss:  0.406,T_accuracy: 82.188 | V_loss:  0.458, V_accuracy: 75.281 | Early stopping counter: 88 out of 100 | \n",
      "[Epoch 2560]T_loss:  0.402,T_accuracy: 83.310 | V_loss:  0.443, V_accuracy: 79.213 | Early stopping counter: 89 out of 100 | \n",
      "[Epoch 2570]T_loss:  0.413,T_accuracy: 81.627 | V_loss:  0.545, V_accuracy: 78.090 | Early stopping counter: 90 out of 100 | \n",
      "[Epoch 2580]T_loss:  0.404,T_accuracy: 82.328 | V_loss:  0.451, V_accuracy: 81.461 | Early stopping counter: 91 out of 100 | \n",
      "[Epoch 2590]T_loss:  0.412,T_accuracy: 81.066 | V_loss:  0.502, V_accuracy: 75.843 | Early stopping counter: 92 out of 100 | \n",
      "[Epoch 2600]T_loss:  0.416,T_accuracy: 83.029 | V_loss:  0.566, V_accuracy: 74.719 | Early stopping counter: 93 out of 100 | \n",
      "[Epoch 2610]T_loss:  0.405,T_accuracy: 81.627 | V_loss:  0.498, V_accuracy: 78.090 | Early stopping counter: 94 out of 100 | \n",
      "[Epoch 2620]T_loss:  0.412,T_accuracy: 82.328 | V_loss:  0.532, V_accuracy: 79.775 | Early stopping counter: 95 out of 100 | \n",
      "[Epoch 2630]T_loss:  0.421,T_accuracy: 81.346 | V_loss:  0.467, V_accuracy: 78.652 | Early stopping counter: 96 out of 100 | \n",
      "[Epoch 2640]T_loss:  0.413,T_accuracy: 82.609 | V_loss:  0.448, V_accuracy: 78.090 | Early stopping counter: 97 out of 100 | \n",
      "[Epoch 2650]T_loss:  0.415,T_accuracy: 80.926 | V_loss:  0.489, V_accuracy: 79.775 | Early stopping counter: 98 out of 100 | \n",
      "[Epoch 2660]T_loss:  0.405,T_accuracy: 81.346 | V_loss:  0.474, V_accuracy: 79.213 | Early stopping counter: 99 out of 100 | \n",
      "[Epoch 2670]T_loss:  0.405,T_accuracy: 82.328 | V_loss:  0.517, V_accuracy: 76.404 | Early stopping counter: 100 out of 100*** TRAIN EARLY STOPPED! *** | \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇█████</td></tr><tr><td>Training accuracy (%)</td><td>▁▄▄▅▄▅▅▆▇▆▇▇▇▇█▇▇▇██▇██▇███▇████████████</td></tr><tr><td>Training loss</td><td>██▇▇▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation accuracy (%)</td><td>▁▁▂▁▂▆▄▆▆▇▇▅▇▅▅▅▅█▅▇█▇▄▄▄▄▅▇▄█▅▅▆▅██▆▅▅▇</td></tr><tr><td>Validation loss</td><td>███▃▃▂▂▅▃▃▂▇▅▂▃▃▄▆▃▂▄▁▇▂▂▃▂▂▁▇▁▅▂▄▄▅▆▄█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2670</td></tr><tr><td>Training accuracy (%)</td><td>82.32819</td></tr><tr><td>Training loss</td><td>0.40477</td></tr><tr><td>Validation accuracy (%)</td><td>76.40449</td></tr><tr><td>Validation loss</td><td>0.51743</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_10_25_18_55_49</strong> at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/r4nebs7x' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic/runs/r4nebs7x</a><br/> View project at: <a href='https://wandb.ai/tmdrjs0040/link_DL_titanic' target=\"_blank\">https://wandb.ai/tmdrjs0040/link_DL_titanic</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_185549-r4nebs7x\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Titanic_early_stop2](HW2_Titanic_Early_stop2.png)\n",
    "\n",
    "[그림 4] 수정 후 early_stop\n"
   ],
   "id": "dbdffe3bbf8feaa5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kaggle 제출 결과\n",
    "![Kaggle_score1](Kaggle_score.png)"
   ],
   "id": "a431adbc8b76304e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기\n",
    "어째서인지 모르겠지만, 코드를 그대로 따라했을 때 train_loop에서 'input, target = train_batch'를 한 경우 input이 'str' type으로 나와서 오류가 발생하는 문제가 발생했다. 어때서 이렇게 되는 것인지 궁금합니다. "
   ],
   "id": "6aa84b8265b67bb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
