{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# a_tensor_initialization",
   "id": "6472d9fffc74dadf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:06:15.041062Z",
     "start_time": "2024-09-23T15:06:14.979311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu') \n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "#추가코드------------------------------\n",
    "a12 = torch.tensor([\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "])\n",
    "print(a12.shape, a12.ndim)\n",
    "#----------------------------------------\n",
    "\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "\n"
   ],
   "id": "a9ac2552eb825d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n",
      "torch.Size([4, 1, 2, 3]) 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 110\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28mprint\u001B[39m(a12\u001B[38;5;241m.\u001B[39mshape, a12\u001B[38;5;241m.\u001B[39mndim)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m#----------------------------------------\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m a11 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;00m\n\u001B[0;32m    111\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],\n\u001B[0;32m    112\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],\n\u001B[0;32m    113\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],\n\u001B[0;32m    114\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],\n\u001B[0;32m    115\u001B[0m ])\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "tensor의 사이즈와 깊이: tensor의 깊이(차원)은 축의 개수로 설명할 수 있으며, 사이즈의 경우 각 축의 원소 개수를 의미하고 텐서의 구조를 나타낸다.\n",
    "\n",
    "tensor의 내부의 리스트는 모두 동일한 크기를 가지고 있어야 한다. 하지만 a11은 첫번째 항목과 두번째 항목의 크기가 3과 2로 다르기 때문에 오류가 발생한다."
   ],
   "id": "43fc8f7ef256b0f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# b_tensor_initialization_copy",
   "id": "364c795c771e5236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:31:53.675478Z",
     "start_time": "2024-09-19T13:31:53.664528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "#코드 추가\n",
    "print(\"#\" * 100)\n",
    "print(l1, l2,l3,l4,l5,l6)\n",
    "l6[0] = 200\n",
    "l6[1] = 300\n",
    "l6[2] = 400\n",
    "\n",
    "print(l6)\n",
    "print(t6)"
   ],
   "id": "44f0348caa7a8610",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n",
      "####################################################################################################\n",
      "[100, 2, 3] [100, 2, 3] [100, 2, 3] [100   2   3] [100   2   3] [100   2   3]\n",
      "[200 300 400]\n",
      "tensor([200, 300, 400], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "list와 np.array의 텐서화\n",
    "\n",
    "list와 np.array를 텐서로 변경할 때 두 자료형과 텐설 변환시키는 함수별 차이점을 확인하기 위한 코드로 보인다.\n",
    "텐서를 만드는 함수로는 Tensor(), tensor(), as_tensor가 존재한다.\n",
    "\n",
    "- Tensor(): pytorch의 기본 텐서 클래스로 크기를 입력해 원하는 구조의 텐서를 만들수 도 있으나 이 경우 초기값으로 임의의 값이 설정된다. 위에처럼 이미 존재하는 데이터를 만들 수 있다.\n",
    "- tensor(): 주어진 데이터를 기반으로 텐서를 생성하는 함수이다. Tensor와 다르게 자동으로 데이터의 타입을 감지해 알맞은 데이터 타입의 텐서를 생성한다.\n",
    "- as_tensor(): 주어진 데이터를 기반으로 텐서를 생성하는 함수이다. 데이터의 복사를 최소화하기 위해 기존 데이터의 메모리를 공유한다. 때문에 원본 데이터를 변경하면 텐서의 값도 변경된다. 하지만 list의 경우에는 리스트를 변경하면 해당 리스트가 참조하는 주소가 변경되는 것이기 때문에 list 자료형을 as_tensor로 텐서를 만든 경우 list를 변경해도 텐서에 변화는 없다."
   ],
   "id": "1542ec8afe50a195"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# c_tensor_initialization_constant_values",
   "id": "f3e673cc2e87a48d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T14:24:52.554285Z",
     "start_time": "2024-09-19T14:24:52.533341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)\n"
   ],
   "id": "2b2a9b6d86686543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([-1.2177e+32,  1.1154e-42,  3.0000e+00,  0.0000e+00])\n",
      "tensor([1., 2., 3., 0.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ones, zeros, empty 등을 이용한 텐서의 생성과 like, eye를 이용한 텐서의 생성\n",
    "- ones: 주어진 크기(구조)의 텐서를 생성하며, 모든 요소를 1로 초기화한다.\n",
    "- zeros: 주어진 크기(구조)의 텐서를 생성하며, 모든 요소를 0으로 초기화한다.\n",
    "- empty: 주어진 크기(구조)의 텐서를 생성하며, 모든 요소를 임의의 값으로 초기화한다.\n",
    "\n",
    "- like: 주어진 텐서와 동일한 크기(구조)의 텐서를 생성한다. 앞쪽의 ones, zeros, empty중 무엇이 붙는지에 따라 원소의 초기화 값이 정해진다.\n",
    "\n",
    "- eye: 주어진 크기의 단위 행렬인 텐서를 생성한다. n x m 크기의 행렬로 생성할 수 있다. 이 경우 주대각선의 요소는 모두 1이고 나머지 요소는 0인 행렬을 생성한다. 항상 차원이 2인 텐서이다."
   ],
   "id": "3c5c5d3901cdeaf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# d_tensor_initialization_random_values",
   "id": "513e5c59231b77bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T13:05:42.177419Z",
     "start_time": "2024-09-23T13:05:37.805787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ],
   "id": "c09343bd691212b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 13]])\n",
      "tensor([[0.3695, 0.5365, 0.3038]])\n",
      "tensor([[ 1.3246, -0.1589,  1.8702]])\n",
      "tensor([[ 9.9485, 11.4228],\n",
      "        [ 8.7230,  8.4463],\n",
      "        [ 9.4099,  8.6061]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "torch.Size([3])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "랜덤값으로 초기화된 텐서의 생성\n",
    "- randint(low, high, size): 주어진 low, high 범위에서 임의의 정수를 생성한다. 생성하는 텐서의 구조는 size를 따른다\n",
    "- rand(size): 0과 1 사이의 값을 균등 분포로 선택해 임의의 실수를 생성한다. 생성하는 텐서의 구조는 size를 따른다.\n",
    "- randn(size): 평균이 0 표준편차가 1인 정규 본포에 따라 임의의 실수를 생성한다. 생성하는 텐서의 구조는 size를 따른다.\n",
    "- normal(mean, std, size): 입력받은 평균과 표준편차를 가지는 정규 분포에 따라 임의의 실수를 생성한다. 생성하는 텐서의 구조는 size를 따른다.\n",
    "- linspace(start, end, steps): 입력받은 start와 end 사이의 값을 steps개 만큼 균등하게 나누서 값을 생성한다. 1차원의 텐서를 가진다.\n",
    "- arange(a): 0부터 a까지의 정수를 생성한다. 1차언의 텐서를 가진다.\n",
    "- manual_seed(): 난수 생성의 시드를 생성해 난수 생성을 재현 가능하도록 한다.\n",
    "- rand(a,b): 0과 1사이의 값을 균등 분포로 선택해 임의의 실수로 생성한다. 생성하는 텐서의 구조는 a행 b열이다. \n",
    "- size = (a,b)\n"
   ],
   "id": "83eac0e113bb5784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# e_tensor_type_conversion.py",
   "id": "61629fc452a51797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:10:36.219938Z",
     "start_time": "2024-09-23T14:10:36.206066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "print(double_d.dtype) #추가 코드\n",
    "print(short_e.dtype)  # 추가 코드\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "print(double_d.dtype) # 추가 코드\n",
    "print(short_e.dtype)  # 추가 코드\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "print(double_d.dtype) # 추가 코드\n",
    "print(short_e.dtype)  # 추가 코드\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)\n"
   ],
   "id": "5cc5e56da284c8a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[11.6692, 18.3283,  0.2118],\n",
      "        [18.4972,  9.8370,  3.8937]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "텐서의 타입 변경\n",
    "- tensor.to(자료형): 해당 텐서를 입력한 자료형으로 변경\n",
    "- tensor.to(device): 텐서를 cpu에서 gpu로 또는 gpu에서 cpu로 이동\n",
    "- tensor.double(): 텐서의 자료형을 64비트 부동 소수점(torch.float64)으로 설정\n",
    "- tensor.short(): 텐서의 자료형을 16비트 정수(torch.int16)로 설정\n",
    "- tensor.type(자료형): 텐서를 입력받은 자료형으로 변경"
   ],
   "id": "4480faadd4ed29c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# f_tensor_operations.py",
   "id": "d94db33274fc2eff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:34:00.788784Z",
     "start_time": "2024-09-23T14:34:00.778406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n"
   ],
   "id": "f46d3a2acbbe721c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "텐서의 사칙연산 연산자와 함수\n",
    "- 연산자: 가독성이 좋다.\n",
    "- 함수: 추가 기능과 유연성 제공 (브로드캐스팅(두 텐서의 크기가 다른 경우 브로드캐스팅을 통해 크기를 맞춰준다.), in-place('add(t1,t2,out=t3)' 이렇게 기존 텐서에 저장 가능)"
   ],
   "id": "5b62648bfa7acf13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# g_tensor_operations_mm.py",
   "id": "b58409e6921b053a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T03:05:52.334779Z",
     "start_time": "2024-09-24T03:05:52.319510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t2) #추가 코드\n",
    "print(t3) #추가 코드\n",
    "print(t4) #추가 코드\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t5) #추가 코드\n",
    "print(t6) #추가 코드\n",
    "print(t7.size())\n",
    "\n",
    "\n",
    "#추가 코드 -------------------------------\n",
    "print(\"추가 코드\", end=\" \")\n",
    "print(\"#\" * 30)\n",
    "#matmul\n",
    "#tensor1: 1차원, tensor2: 1차원인 경우\n",
    "t8 = torch.randn(2)\n",
    "t9 = torch.randn(2)\n",
    "print(t8)\n",
    "print(t9)\n",
    "print(torch.matmul(t9,t8))\n",
    "\n",
    "#tensor1: 1차원, tensor2: 2차원인 경우\n",
    "t10 = torch.randn(2)\n",
    "t11 = torch.randn(2,1)\n",
    "print(t10)\n",
    "print(t11)\n",
    "print(torch.matmul(t10,t11))\n",
    "\n",
    "#tensor1: 2차원, tensor2: 1차원인 경우\n",
    "t10 = torch.randn(2,1)\n",
    "t11 = torch.randn(1)\n",
    "print(t10)\n",
    "print(t11)\n",
    "print(torch.matmul(t10,t11))\n",
    "\n",
    "t10 = torch.randn(2)\n",
    "t11 = torch.randn(2,1)\n",
    "print(t10)\n",
    "print(t11)\n",
    "print(torch.matmul(t10,t11))"
   ],
   "id": "6b3cfa210e578e8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[ 0.4742,  0.9797,  0.7206],\n",
      "        [ 0.3366,  0.6580, -0.5221]])\n",
      "tensor([[-0.9168, -1.7845],\n",
      "        [-0.6568,  1.1906],\n",
      "        [-2.3959, -0.7807]])\n",
      "tensor([[-2.8047, -0.2424],\n",
      "        [ 0.5101,  0.5902]])\n",
      "tensor([[-2.8047, -0.2424],\n",
      "        [ 0.5101,  0.5902]]) torch.Size([2, 2])\n",
      "tensor([[[ 0.4649,  1.0024, -0.4300,  0.0224],\n",
      "         [-0.4577, -0.4615,  0.5496, -0.0871],\n",
      "         [-0.1730,  1.6708, -0.0341,  1.0033]],\n",
      "\n",
      "        [[ 1.1079, -1.0454,  1.6364, -0.0242],\n",
      "         [-0.0278,  0.3844,  0.4644, -0.1355],\n",
      "         [ 0.2364,  0.7613, -1.6597, -1.3178]],\n",
      "\n",
      "        [[-0.7786, -0.2364, -0.8153,  0.3090],\n",
      "         [-0.8037,  2.5264, -0.2621,  0.7512],\n",
      "         [-0.0166,  1.6601,  1.7656, -0.9698]],\n",
      "\n",
      "        [[ 0.2893,  0.4487,  0.8493,  1.8208],\n",
      "         [-0.0544, -0.4368, -0.5627, -1.2856],\n",
      "         [ 0.2076,  0.2787, -0.8574,  0.7197]],\n",
      "\n",
      "        [[-0.1117,  0.9229,  0.2972,  2.2930],\n",
      "         [-1.3505, -1.5121,  1.0961, -0.4439],\n",
      "         [ 0.7262, -0.0774,  0.6937,  0.2995]],\n",
      "\n",
      "        [[-0.5228,  2.7110,  0.9476, -0.0330],\n",
      "         [ 0.2841, -0.1766, -0.5805,  0.5190],\n",
      "         [ 1.5591, -1.2488, -1.7554,  1.3689]],\n",
      "\n",
      "        [[-1.4087, -1.4708, -1.0873, -0.4146],\n",
      "         [-0.0929, -1.6667, -0.2274,  0.9758],\n",
      "         [ 0.0164, -0.7623,  0.9716, -0.4301]],\n",
      "\n",
      "        [[ 0.2974,  0.4025, -0.3150, -0.2723],\n",
      "         [-0.2385,  0.1096, -1.3879,  2.8023],\n",
      "         [-0.3004, -1.3686,  0.6119, -0.4265]],\n",
      "\n",
      "        [[-0.2245,  0.1304, -0.6022,  1.0086],\n",
      "         [-0.8943, -0.3407,  0.6979, -0.6453],\n",
      "         [ 0.1201,  0.9811,  1.6705,  0.4042]],\n",
      "\n",
      "        [[ 0.3806,  0.6696, -0.1552,  0.4832],\n",
      "         [ 0.6215,  0.5201,  0.2427,  0.4502],\n",
      "         [ 0.6029,  1.2334, -0.0323,  0.2274]]])\n",
      "tensor([[[-0.2860,  0.4167, -0.6343, -0.0190,  1.4494],\n",
      "         [ 0.5948,  0.0337,  0.6308,  0.5375,  0.1777],\n",
      "         [-1.5308,  0.9379,  0.1285, -1.7032, -0.9611],\n",
      "         [-0.1281,  2.0517, -0.1781, -0.0876,  1.9474]],\n",
      "\n",
      "        [[-1.3543,  0.1610,  0.6645,  1.4041, -0.3089],\n",
      "         [ 1.2550,  0.7058,  0.6213, -1.0275,  1.3536],\n",
      "         [ 0.8962,  0.0117, -1.6273, -0.8463, -0.2062],\n",
      "         [-0.2982, -1.5765,  1.8652,  0.3031,  0.1064]],\n",
      "\n",
      "        [[ 0.0138,  1.2044,  1.6347,  0.6887, -0.6301],\n",
      "         [-1.1178, -0.2876,  0.3329, -0.1921, -1.7639],\n",
      "         [-0.5534, -1.1757,  0.4032,  1.0892,  0.9365],\n",
      "         [-1.5310, -0.2926, -0.2111, -0.5056, -0.5551]],\n",
      "\n",
      "        [[ 1.1664,  0.4928, -1.3869, -1.8706, -0.3277],\n",
      "         [-0.5477,  0.3218, -1.1607,  1.6410, -1.4898],\n",
      "         [-0.5040, -1.5780, -1.5123, -0.1074,  0.0593],\n",
      "         [ 0.1172,  0.2582,  0.0356,  1.1503,  0.6971]],\n",
      "\n",
      "        [[-1.7785,  0.6386, -0.0499,  0.9114,  0.5910],\n",
      "         [ 0.1803,  0.7854, -0.4376, -0.6668, -2.2472],\n",
      "         [-0.6948,  2.0996,  1.0790,  0.7808,  0.6888],\n",
      "         [-0.6829,  1.2970, -1.2650,  0.2105,  0.3093]],\n",
      "\n",
      "        [[ 0.2960,  0.2131,  0.2846,  0.0071,  0.9954],\n",
      "         [-0.2972,  0.2640,  1.0424, -0.4686,  1.7139],\n",
      "         [-0.8728, -2.1096, -0.0925,  0.0104,  1.1585],\n",
      "         [ 0.2004, -0.2575,  0.6081,  0.6648, -0.0130]],\n",
      "\n",
      "        [[ 0.1613,  0.9159,  0.8573, -0.8192,  0.0619],\n",
      "         [ 1.5892,  0.0160,  1.1464,  1.3917, -0.3751],\n",
      "         [-0.1877,  0.3591,  0.3209,  0.8616, -0.6659],\n",
      "         [ 0.3542, -0.2475, -1.3644, -0.9279, -0.2983]],\n",
      "\n",
      "        [[ 0.7982,  1.3132,  1.2954,  0.9775, -1.3058],\n",
      "         [ 0.2675,  0.5924,  1.8755, -0.6649, -0.8058],\n",
      "         [ 1.4840,  1.3633,  0.3442,  0.7763, -1.0544],\n",
      "         [-0.5161,  0.1476,  1.2636, -0.0462,  0.0600]],\n",
      "\n",
      "        [[ 1.0778, -0.8283, -2.0197,  0.7205, -0.7958],\n",
      "         [ 0.7030, -0.6264,  1.7954,  0.2615,  0.9397],\n",
      "         [-0.1251,  0.2985,  1.3528,  1.0202,  1.2125],\n",
      "         [-1.2563, -1.0027, -1.0560, -0.7611, -0.1628]],\n",
      "\n",
      "        [[ 1.0585,  0.1744, -0.3567,  2.0558,  0.3057],\n",
      "         [-0.1519,  0.8511,  0.1285, -1.5452, -0.7290],\n",
      "         [ 0.7430,  0.0138, -0.2773, -1.6094,  1.8168],\n",
      "         [ 1.1939, -0.0377,  0.1144, -1.6207,  1.6884]]])\n",
      "torch.Size([10, 3, 5])\n",
      "추가 코드 ##############################\n",
      "tensor([-0.3778,  1.0394])\n",
      "tensor([-1.5753, -0.5641])\n",
      "tensor(0.0089)\n",
      "tensor([-0.8791, -0.6246])\n",
      "tensor([[0.7129],\n",
      "        [0.3365]])\n",
      "tensor([-0.8368])\n",
      "tensor([[-0.7961],\n",
      "        [ 1.4289]])\n",
      "tensor([0.7554])\n",
      "tensor([-0.6013,  1.0794])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "텐서의 행렬곱\n",
    "- dot(tensor1, tensor2): 두 1차원 텐서의 내적(같은 인덱스의 값들을 곱한 후 sum) 스칼라값을 반환한다.\n",
    "- mm(tensor1, tensor2): 두 2차원 텐서의 행렬 곱. nxm * mxp = nxp\n",
    "- bmm(tensor1, tensor2): 두 3차원 텐서의 행렬 곱. bxnxm * bxmxp = bxnxp\n",
    "- matmul(tensor1, tensor2): 입력받은 텐서의 형태에 따라 모드가 달라진다.\n",
    "- - 두 tensor 모두 1차원 -> 내적(스칼라)이 반환\n",
    "- - 두 tensor 모두 2차원 -> 행렬 곱이 반환. nxm * mxp = nxp\n",
    "- - tensor1이 1차원, tensor2가 2차원 -> tensor1에 차원을 추가해 동일한 차원으로 만들고 행렬 곱을 실행한 후, 추가된 차원을 삭제. n * nxm = m\n",
    "- - tensor1이 2차원, tensor2가 1차원 -> 행렬-벡터 곱. nxm * m = n"
   ],
   "id": "c93cc7106032110e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# h_tensor_operations_mmatmul.py",
   "id": "eb7a55a09be617be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T03:10:02.637285Z",
     "start_time": "2024-09-24T03:10:02.615631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "#추가코드----------------------------------\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(torch.matmul(t1, t2))\n",
    "print(\"#\" * 30)\n",
    "#-----------------------------------\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "#추가코드----------------------------------\n",
    "print(t3)\n",
    "print(t4)\n",
    "print(torch.matmul(t3, t4))\n",
    "print(\"#\" * 30)\n",
    "#-----------------------------------\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "#추가코드----------------------------------\n",
    "print(t5)\n",
    "print(t6)\n",
    "print(torch.matmul(t5, t6))\n",
    "print(\"#\" * 30)\n",
    "#-----------------------------------\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "#추가코드----------------------------------\n",
    "print(t7)\n",
    "print(t8)\n",
    "print(torch.matmul(t7, t8))\n",
    "print(\"#\" * 30)\n",
    "#-----------------------------------\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n",
    "#추가코드----------------------------------\n",
    "print(t9)\n",
    "print(t10)\n",
    "print(torch.matmul(t9, t10))\n",
    "print(\"#\" * 30)\n",
    "#-----------------------------------"
   ],
   "id": "1a1c33ca08b4b5bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor([-0.2807, -2.7677, -0.5109])\n",
      "tensor([ 0.6968,  0.0761, -0.3318])\n",
      "tensor(-0.2367)\n",
      "##############################\n",
      "torch.Size([3])\n",
      "tensor([[-0.5861, -1.5804,  1.3590,  0.2994],\n",
      "        [-0.5474,  1.4955,  1.0436,  0.1093],\n",
      "        [-0.8899,  0.9766,  0.4052,  0.4546]])\n",
      "tensor([-1.2611,  0.6301,  0.1542,  1.2123])\n",
      "tensor([0.3159, 1.9262, 2.3513])\n",
      "##############################\n",
      "torch.Size([10, 3])\n",
      "tensor([[[-0.8414,  1.3672,  0.7935,  0.5210],\n",
      "         [ 1.8761, -0.3060,  0.3790, -0.2551],\n",
      "         [-0.3607,  0.3743, -1.1030, -1.3587]],\n",
      "\n",
      "        [[-1.3723, -0.6601,  0.1113, -0.3373],\n",
      "         [ 1.2688,  1.2231,  0.0982,  0.3759],\n",
      "         [ 0.1432,  0.1852, -0.3809, -0.2826]],\n",
      "\n",
      "        [[-0.4288, -0.0769, -0.0326,  0.9893],\n",
      "         [ 0.3531,  0.9210, -0.8385,  0.2855],\n",
      "         [-0.2869, -0.5035,  1.7385,  1.1226]],\n",
      "\n",
      "        [[-0.9890, -0.1535, -0.6454,  0.1235],\n",
      "         [ 1.9671,  2.2096,  3.1521, -1.4426],\n",
      "         [ 0.4087,  0.4806, -0.8396, -1.7623]],\n",
      "\n",
      "        [[ 1.0112, -1.1958,  0.2709, -1.4754],\n",
      "         [-0.5433,  1.1918, -0.9607, -0.0348],\n",
      "         [ 1.8417,  0.7480,  0.2937,  0.1393]],\n",
      "\n",
      "        [[-1.0761,  0.7916,  1.1146,  0.8413],\n",
      "         [ 0.3204,  0.4797, -0.0113, -0.9274],\n",
      "         [-0.1174,  0.4966, -0.4565, -0.3368]],\n",
      "\n",
      "        [[ 0.3219, -0.3572,  1.3730, -0.1500],\n",
      "         [ 0.2107, -1.0375,  1.2644, -0.3755],\n",
      "         [ 1.2372, -1.2200, -0.3757,  1.3036]],\n",
      "\n",
      "        [[-1.8062,  0.0837, -0.3528,  0.2960],\n",
      "         [ 0.7496, -0.9265,  0.3590, -1.4751],\n",
      "         [-1.2496, -1.2390, -0.8207,  0.0710]],\n",
      "\n",
      "        [[ 0.2928,  1.3615,  1.3621, -1.6969],\n",
      "         [-1.1469,  1.7739,  1.1718,  1.1916],\n",
      "         [ 0.7774, -0.8235, -0.9463, -0.3454]],\n",
      "\n",
      "        [[-1.3445,  2.3131, -0.8541, -0.3011],\n",
      "         [-2.1110,  0.9082, -0.9283,  0.5268],\n",
      "         [ 0.1871, -0.5731,  1.2224, -0.6255]]])\n",
      "tensor([-0.4396, -1.5560, -1.2323, -0.4050])\n",
      "tensor([[-2.9464, -0.7123,  1.4858],\n",
      "        [ 1.6299, -2.7342,  0.2327],\n",
      "        [-0.0524, -0.6707, -1.6875],\n",
      "        [ 1.4190, -7.6031,  0.8211],\n",
      "        [ 1.6800, -0.4175, -2.3918],\n",
      "        [-2.4731, -0.4978, -0.0221],\n",
      "        [-1.2171,  0.1157,  1.2895],\n",
      "        [ 0.9786,  1.2671,  3.4599],\n",
      "        [-3.2384, -4.1826,  2.2457],\n",
      "        [-1.8336,  0.4454, -0.4436]])\n",
      "##############################\n",
      "torch.Size([10, 3, 5])\n",
      "tensor([[[ 0.8887, -0.2988, -0.3446, -0.4373],\n",
      "         [ 1.5365, -0.9295,  2.2922, -0.9531],\n",
      "         [-0.5272, -0.6913,  0.3841,  1.4231]],\n",
      "\n",
      "        [[-0.3641, -0.3610,  0.2965, -1.1500],\n",
      "         [-0.3995,  0.7779, -1.2330,  2.0065],\n",
      "         [-0.7529, -0.0899,  0.2056, -0.1294]],\n",
      "\n",
      "        [[ 0.8743,  1.6126, -2.0992, -1.3171],\n",
      "         [-0.5523, -1.1673,  0.3539,  1.1618],\n",
      "         [ 1.1284, -0.1100,  0.3527,  0.1435]],\n",
      "\n",
      "        [[ 0.4988,  0.5734, -0.6851, -0.0787],\n",
      "         [ 0.5951, -2.5614,  1.4062,  1.7404],\n",
      "         [ 1.2655,  0.6284, -0.3538, -0.5325]],\n",
      "\n",
      "        [[ 0.1277,  0.6736, -1.0380, -0.5133],\n",
      "         [-0.4997,  1.0914,  0.5172, -0.8820],\n",
      "         [ 0.0284,  0.6601,  0.6159, -0.5628]],\n",
      "\n",
      "        [[-0.7266, -0.5995,  0.7052, -0.1619],\n",
      "         [ 0.2680,  0.1679,  0.6752, -0.4916],\n",
      "         [-0.1201, -0.3482, -1.6788,  0.8300]],\n",
      "\n",
      "        [[-0.9141,  0.1429,  1.9690,  0.7456],\n",
      "         [-1.0671,  0.2229, -0.6459,  0.4333],\n",
      "         [-0.2699,  0.4932, -1.1191,  0.9602]],\n",
      "\n",
      "        [[ 0.2226, -0.1578, -0.0975, -0.1761],\n",
      "         [ 0.4149,  1.3989, -0.4247, -1.5307],\n",
      "         [ 1.7241, -0.3467,  0.6599, -2.1611]],\n",
      "\n",
      "        [[ 1.3124, -0.5257, -0.7018,  0.9348],\n",
      "         [ 0.9018, -1.3130, -0.0548, -1.3132],\n",
      "         [-0.4378,  0.1756,  0.0598, -0.5024]],\n",
      "\n",
      "        [[ 0.4405,  1.0446,  1.8438, -0.7663],\n",
      "         [-1.0000, -0.4752,  0.8432,  1.4144],\n",
      "         [-0.2200,  0.0755,  0.1436, -1.2288]]])\n",
      "tensor([[[ 1.4059e-01, -7.6400e-01,  1.7441e+00, -5.2150e-01,  2.1787e+00],\n",
      "         [ 2.1076e+00, -4.6252e-01, -1.1317e+00, -1.4236e+00, -8.8630e-01],\n",
      "         [ 3.8825e-01,  1.9976e-01, -1.0004e+00, -4.1790e-01,  2.8713e-01],\n",
      "         [ 1.0233e+00, -8.3739e-01, -9.3864e-01,  1.7364e+00,  5.9378e-01]],\n",
      "\n",
      "        [[-2.1718e-03, -7.3769e-01,  2.4602e-01,  8.0250e-01,  5.1254e-01],\n",
      "         [-6.4713e-01, -1.6403e+00, -9.3898e-01,  1.4760e-02, -2.7387e-01],\n",
      "         [-9.6987e-01, -5.0891e-01, -5.0671e-01, -1.0514e+00,  8.7659e-02],\n",
      "         [ 7.2534e-01, -1.1389e+00, -7.3447e-01, -1.6309e+00, -1.2840e+00]],\n",
      "\n",
      "        [[ 2.0785e-01, -5.4212e-01,  2.2896e+00, -1.0765e+00, -6.6989e-01],\n",
      "         [-4.1080e-01, -1.5222e+00, -5.0993e-01,  1.0248e+00,  5.5073e-01],\n",
      "         [-5.9220e-01, -1.2934e+00, -8.9107e-01,  2.3368e-01,  1.8737e+00],\n",
      "         [ 5.7695e-01, -8.8580e-02, -1.6281e+00,  1.5918e+00,  4.2008e-01]],\n",
      "\n",
      "        [[-1.6214e-01,  2.0596e-01,  7.8731e-01,  1.0029e+00,  6.7576e-01],\n",
      "         [ 2.8898e-01, -6.2130e-01,  1.4522e-01, -2.7723e-01,  4.0387e-01],\n",
      "         [ 2.7280e-01,  1.6186e+00, -5.8042e-01, -4.5003e-01,  1.3997e+00],\n",
      "         [ 1.3988e+00,  6.7973e-01, -3.2567e-01, -5.1782e-01,  1.0408e+00]],\n",
      "\n",
      "        [[ 1.4059e+00, -9.6320e-01,  1.0874e+00, -1.1477e+00, -2.2136e-01],\n",
      "         [-1.2693e-01,  4.2933e-01, -5.9644e-01,  8.8641e-01,  2.1088e-01],\n",
      "         [-1.1958e-01, -4.4525e-01, -2.5717e-01, -1.8178e+00, -6.8000e-01],\n",
      "         [ 6.7067e-01, -1.6612e+00,  4.5221e-01,  4.3203e-01,  2.9774e-01]],\n",
      "\n",
      "        [[ 6.8167e-01, -5.5802e-01, -3.5907e-01, -6.2789e-01,  2.7865e-01],\n",
      "         [ 6.3695e-04,  7.8274e-01,  1.0895e+00, -8.0874e-01,  1.8320e+00],\n",
      "         [ 1.4694e+00,  1.1001e+00, -4.9072e-01, -2.1706e-01,  7.8952e-01],\n",
      "         [-7.0874e-02, -1.8615e-01, -1.2084e+00, -8.6116e-01,  1.2276e+00]],\n",
      "\n",
      "        [[ 1.7729e+00, -2.0834e+00, -5.3319e-02, -1.9991e-01,  2.6716e+00],\n",
      "         [ 1.6286e+00, -5.7371e-01, -8.7369e-01, -2.0762e+00,  9.4715e-01],\n",
      "         [-9.2616e-01, -8.5023e-02,  4.9597e-01, -1.7542e+00,  2.4013e+00],\n",
      "         [ 1.4188e+00, -3.8659e-01,  1.6618e+00, -8.8927e-01,  2.5533e+00]],\n",
      "\n",
      "        [[-5.0959e-01, -4.7259e-01, -6.5725e-01,  1.8844e-01, -1.1097e+00],\n",
      "         [ 1.0774e+00,  1.0122e-01, -8.3711e-01, -1.2555e+00,  1.2032e-01],\n",
      "         [ 1.1223e-01, -1.1771e+00, -4.9458e-01, -6.3714e-02, -1.1918e+00],\n",
      "         [ 2.7938e-01, -1.2548e+00, -8.4670e-01, -1.8212e-01, -8.0142e-01]],\n",
      "\n",
      "        [[ 2.1118e-01, -5.2479e-02,  5.1283e-01,  2.4537e+00, -1.1071e+00],\n",
      "         [-7.2174e-01, -1.6615e+00, -9.6464e-01,  5.1823e-01, -5.3301e-01],\n",
      "         [ 1.0053e-01,  6.4946e-01, -1.6554e+00, -9.1920e-01, -1.1194e+00],\n",
      "         [-9.4973e-01,  3.8127e-01,  3.3434e-01,  4.4807e-03, -1.0172e+00]],\n",
      "\n",
      "        [[-1.5416e+00,  9.6006e-01,  1.4060e+00,  3.8619e-01,  9.1525e-01],\n",
      "         [ 3.3524e-01, -1.4006e+00,  2.2090e-01,  6.9397e-01, -3.5332e-01],\n",
      "         [-1.4672e+00,  6.3916e-02, -2.2735e+00, -1.3737e+00, -7.7849e-01],\n",
      "         [ 7.2960e-01, -5.2622e-01, -5.3600e-01, -4.5564e-01,  9.9366e-02]]])\n",
      "tensor([[[-1.0862e+00, -2.4334e-01,  2.6433e+00, -6.5338e-01,  1.8424e+00],\n",
      "         [-1.8284e+00,  5.1204e-01,  2.3335e+00, -2.0910e+00,  4.2637e+00],\n",
      "         [ 7.4286e-02, -3.9246e-01, -1.8572e+00,  3.5697e+00,  4.1937e-01]],\n",
      "\n",
      "        [[-8.8730e-01,  2.0197e+00,  9.4383e-01,  1.2663e+00,  1.4149e+00],\n",
      "         [ 2.1487e+00, -2.6391e+00, -1.6777e+00, -2.2852e+00, -3.1023e+00],\n",
      "         [-2.3348e-01,  7.4554e-01, -1.0999e-01, -6.1072e-01, -1.7711e-01]],\n",
      "\n",
      "        [[ 2.4775e-03, -9.6955e-02,  5.1944e+00, -1.8758e+00, -4.1841e+00],\n",
      "         [ 8.2547e-01,  1.5157e+00, -2.8760e+00,  1.3303e+00,  8.7819e-01],\n",
      "         [ 1.5364e-01, -9.1321e-01,  2.0917e+00, -1.0166e+00, -9.5300e-02]],\n",
      "\n",
      "        [[-2.1214e-01, -1.4159e+00,  8.9929e-01,  6.9038e-01, -4.7212e-01],\n",
      "         [ 1.9814e+00,  5.1730e+00, -1.2864e+00, -2.2712e-01,  3.1473e+00],\n",
      "         [-8.6503e-01, -1.0644e+00,  1.4664e+00,  1.5300e+00,  5.9576e-02]],\n",
      "\n",
      "        [[-1.2611e-01,  1.4811e+00, -2.2806e-01,  2.1156e+00,  6.6678e-01],\n",
      "         [-1.4945e+00,  2.1848e+00, -1.7262e+00,  2.1988e-01, -2.7348e-01],\n",
      "         [-4.9496e-01,  9.1678e-01, -7.7570e-01, -8.1017e-01, -4.5345e-01]],\n",
      "\n",
      "        [[ 5.5204e-01,  7.4209e-01, -5.4276e-01,  9.2739e-01, -9.4278e-01],\n",
      "         [ 1.2097e+00,  8.1617e-01,  3.4947e-01, -2.7290e-02,  3.1188e-01],\n",
      "         [-2.6079e+00, -2.2070e+00, -5.1540e-01,  6.6965e-03, -9.7801e-01]],\n",
      "\n",
      "        [[-2.1536e+00,  1.3669e+00,  2.1396e+00, -4.2310e+00,  4.3251e+00],\n",
      "         [-3.1586e-01,  1.9827e+00,  2.6183e-01,  4.9830e-01, -3.0844e+00],\n",
      "         [ 2.7236e+00,  3.2628e-03,  6.2410e-01,  1.3915e-01, -4.8947e-01]],\n",
      "\n",
      "        [[-3.4364e-01,  2.1461e-01,  1.8318e-01,  2.7841e-01, -8.6515e-03],\n",
      "         [ 8.2032e-01,  2.3662e+00,  6.2376e-02, -1.3722e+00,  1.4408e+00],\n",
      "         [-1.7819e+00,  1.0852e+00,  6.6056e-01,  1.1118e+00, -1.0094e+00]],\n",
      "\n",
      "        [[-3.0175e-01,  7.0518e-01,  2.6545e+00,  3.5971e+00, -1.3380e+00],\n",
      "         [ 2.3797e+00,  1.5980e+00,  1.3806e+00,  1.5768e+00,  1.0985e+00],\n",
      "         [ 2.6398e-01, -4.2149e-01, -6.6083e-01, -1.0405e+00,  8.3530e-01]],\n",
      "\n",
      "        [[-3.5933e+00, -5.1905e-01, -2.9311e+00, -1.2886e+00, -1.4774e+00],\n",
      "         [ 1.1772e+00, -9.8485e-01, -4.1861e+00, -2.5187e+00, -1.2632e+00],\n",
      "         [-7.4270e-01,  3.3881e-01,  3.9487e-02,  3.3004e-01, -4.6193e-01]]])\n",
      "##############################\n",
      "torch.Size([10, 3, 5])\n",
      "tensor([[[ 3.4916e-01,  8.7820e-01, -3.4442e+00, -9.4345e-01],\n",
      "         [ 1.8347e+00,  1.8226e+00, -1.1464e+00, -6.9051e-01],\n",
      "         [ 4.4127e-01, -6.3589e-01, -6.1495e-01,  5.5392e-02]],\n",
      "\n",
      "        [[-4.0845e-02,  9.4213e-01,  1.4997e-01,  7.4966e-01],\n",
      "         [ 7.6329e-02,  1.3955e+00,  6.8336e-01,  2.5313e-01],\n",
      "         [-1.3539e+00, -1.1655e+00, -2.0540e-01,  3.0239e-01]],\n",
      "\n",
      "        [[-1.1569e+00,  2.9268e-01, -6.6110e-02,  6.9960e-01],\n",
      "         [-1.5787e+00, -4.7441e-01,  1.6506e+00, -7.8173e-01],\n",
      "         [ 2.7612e-01,  5.2889e-01, -5.0695e-01, -7.2768e-01]],\n",
      "\n",
      "        [[ 3.2194e+00, -7.2480e-01,  1.4933e+00,  2.2458e+00],\n",
      "         [ 5.6238e-01, -5.5666e-01, -1.0459e+00, -6.2854e-01],\n",
      "         [-5.9401e-01, -7.1479e-01, -1.1503e+00,  3.0655e+00]],\n",
      "\n",
      "        [[ 1.4836e-01,  1.8263e+00,  2.0009e-01, -1.2038e+00],\n",
      "         [ 1.3908e+00,  6.4557e-01,  3.8128e-01, -4.0640e-01],\n",
      "         [-9.4671e-01,  8.9375e-01,  2.3972e-01,  1.3554e+00]],\n",
      "\n",
      "        [[-9.4156e-01,  4.8154e-01, -1.1418e-01, -9.7094e-01],\n",
      "         [-9.4517e-01, -1.1809e+00, -2.9073e-03, -2.7816e-01],\n",
      "         [ 1.9228e+00,  1.0736e-01,  3.5174e-01,  7.5133e-01]],\n",
      "\n",
      "        [[-6.5955e-01,  8.2490e-02,  1.6269e-01, -1.0319e+00],\n",
      "         [ 2.1304e-01, -4.9636e-01, -6.8125e-01,  1.3709e+00],\n",
      "         [-2.3483e-01, -7.7540e-01, -1.8339e+00, -1.3004e+00]],\n",
      "\n",
      "        [[-1.4271e-01, -3.3212e+00, -8.4229e-03,  3.1919e-01],\n",
      "         [-1.2174e+00,  6.7048e-01, -5.7598e-01,  1.4817e+00],\n",
      "         [ 1.0647e+00, -3.5207e-01, -2.3712e-01, -5.7172e-01]],\n",
      "\n",
      "        [[-9.4636e-01, -9.2240e-01, -1.1648e+00, -1.4031e+00],\n",
      "         [-6.2120e-01,  1.0812e+00,  3.7406e-01,  7.6926e-01],\n",
      "         [ 8.6909e-01, -2.6048e-01, -6.5512e-01,  1.0905e+00]],\n",
      "\n",
      "        [[-7.6306e-01, -3.9509e-01, -8.2337e-01,  3.3949e-01],\n",
      "         [ 2.2928e-02, -1.4418e+00,  5.5565e-02,  2.1560e+00],\n",
      "         [ 6.0297e-01,  1.4052e-01, -1.1818e+00,  1.3762e+00]]])\n",
      "tensor([[ 0.3657,  0.6444, -0.2294,  1.3527, -0.0482],\n",
      "        [-1.1918, -0.7509, -0.4208, -0.1105,  1.0150],\n",
      "        [ 1.5050, -1.7535, -0.1044, -0.5246, -1.7808],\n",
      "        [ 0.4212,  0.4413,  0.5606,  0.6295, -1.5027]])\n",
      "tensor([[[-6.5000,  5.1885, -0.6187,  1.5883,  8.4256],\n",
      "         [-3.5176,  1.5192, -1.4551,  2.4472,  4.8406],\n",
      "         [ 0.0171,  1.8646,  0.2616,  1.0246,  0.3452]],\n",
      "\n",
      "        [[-0.5963, -0.6660,  0.0175,  0.2339, -0.4354],\n",
      "         [-0.5003, -2.0853, -0.5342, -0.2500, -0.1845],\n",
      "         [ 0.7122,  0.4963,  0.9919, -1.4045, -1.2063]],\n",
      "\n",
      "        [[-0.5767, -0.5407,  0.5413, -1.1222, -0.5807],\n",
      "         [ 2.1430, -3.9005, -0.0488, -3.4412, -2.1701],\n",
      "         [-1.5989,  0.3486, -0.6408,  0.1230,  2.5198]],\n",
      "\n",
      "        [[ 5.2347,  0.9916,  0.6693,  5.0652, -6.9249],\n",
      "         [-0.9697,  2.3370, -0.1379,  0.9752,  2.2149],\n",
      "         [ 0.1947,  3.5239,  2.2755,  1.8086, -3.2549]],\n",
      "\n",
      "        [[-2.3284, -2.1579, -1.4982, -0.8638,  3.2992],\n",
      "         [ 0.1418, -0.4364, -0.8583,  1.3541,  0.5199],\n",
      "         [-0.4797, -1.1035,  0.5758, -0.6519, -1.5109]],\n",
      "\n",
      "        [[-1.4991, -1.1967, -0.5190, -1.8781,  2.1965],\n",
      "         [ 0.9403,  0.1600,  0.5581, -1.3216, -0.7299],\n",
      "         [ 1.4211,  0.8733, -0.1019,  2.8775, -1.7391]],\n",
      "\n",
      "        [[-0.5294, -1.2276, -0.4788, -1.6362,  1.3765],\n",
      "         [ 0.2217,  2.3096,  0.9996,  1.5634, -1.3611],\n",
      "         [-2.4695,  3.0727, -0.1573, -0.0885,  4.4442]],\n",
      "\n",
      "        [[ 4.0280,  2.5577,  1.6100,  0.3791, -3.8287],\n",
      "         [-1.4870,  0.3758,  0.8879, -0.4859, -0.4617],\n",
      "         [ 0.2113,  1.1140, -0.3918,  1.2436,  0.8727]],\n",
      "\n",
      "        [[-1.5908,  1.5060, -0.0596, -1.4504,  3.2921],\n",
      "         [-0.6288, -1.5287,  0.0797, -0.6717, -0.6947],\n",
      "         [ 0.1017,  2.3857,  0.5900,  2.2345, -0.7784]],\n",
      "\n",
      "        [[-0.9043,  1.3985,  0.6176, -0.3429,  0.5919],\n",
      "         [ 2.7187,  1.9515,  1.8042,  1.5183, -4.8033],\n",
      "         [-1.1459,  2.9626,  0.6974,  2.2864,  0.1500]]])\n",
      "##############################\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "matmul: 입력받은 텐서의 종류에 따라 모드가 달라진다.\n",
    "- 두 텐서 모두 1차원인 경우 -> 벡터의 내적을 통해 스칼라값 반환\n",
    "- tensor1: 2차원, tensor2: 1차원 -> 행렬과 벡터의 곱셈 수행. 1차원 배열을 반환. nxm * m = n\n",
    "- tensor1: 3차원, tensor2: 1차원 -> 3차원 텐서의 마지막 2개 차원과 1차원 벡터의 곱셈 수행. 2차원 배열을 반환. nxmxp * p = nxm\n",
    "- tensor1: 3차원, tensor2: 3차원 -> 두 3차원 텐서의 마지막 2개 차원을 행렬 곱셈을 수행. nxmxp * nxpxk = nxmxk\n",
    "- tensor1: 3차원, tensor2: 2차원 -> 3차원 텐서의 마지막 2개 차원과 2차원 행렬의 곱셈을 수행. nxmxp * pxk = nxmxk"
   ],
   "id": "a146ab24386d20e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# i_tensor_broadcasting.py",
   "id": "7b89b02fb9d8c359"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T03:26:11.051855Z",
     "start_time": "2024-09-24T03:26:11.022486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ],
   "id": "e0bbcd558a9acf86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "브로드 캐스팅\n",
    "- 스칼라와 벡터의 곱: 벡터의 모든 요소에 스칼라를 곱한다.\n",
    "- 행려과 벡터의 차: 행렬의 각 행에서 벡터를 뺀다.\n",
    "- 행렬에 스칼라값 사칙연산: 행렬의 모든 요소에 스칼라값을 연산자에 맞춰서 계산한다.\n",
    "- pow 함수를 이용한 거듭제곱: (tensor1, n): tensor1의 각 요소를 n 만큼 거듭제곱한다. (tensor1, tensor2): tensor1의 요소를 tensor2의 동일 인덱스의 값만큼 거듭 제곱한다."
   ],
   "id": "b624433b8f53b5bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# j_tensor_indexing_slicing.py",
   "id": "965b55a241a83c65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T03:33:11.398452Z",
     "start_time": "2024-09-24T03:33:11.380446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)\n"
   ],
   "id": "7149c242f7ad9664",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "index slicing\n",
    "- x[n]      : 텐서 x의 n번 행 전체\n",
    "- x[:, n]   : 텐서 x의 n번 열 전체\n",
    "- x[n, m]   : 텐서 x의 n행 m열의 값\n",
    "- x[:, -n]  : 텐서 x의 뒤에서 부터 n번째 열 전체(python은 음수 인덱스 사용 가능(-1부터 마지막 인덱스 -2는 마지막 인덱스의 이전 인덱스))\n",
    "- x[n:]     : 텐서 x의 n번 행 부터 마지막 행까지\n",
    "- x[n:, m:] : 텐서 x의 n번 행 부터 마지막 행까지 내용중 m번째 열부터 마지막 열까지 값\n",
    "- y[n:m, k] = 1: 텐서 y의 n번 행부터 m-1행의 k열의 값을 1로 변경\n",
    "- y[n:m, n:m]: 텐서 y의 n번 행부터 m-1행의 내용중 n번 열부터 m-1열의 값\n",
    "- z[:n]     : 텐서 z의 0번 행부터 n-1행의 값\n",
    "- z[n:, n:m]: 텐서 z의 n번 행부터 마지막 행 까지 값 중에 n번 열부터 m-1열의 값\n",
    "- z[:, 1:]  : 텐서 z의 1번 열부터 마지막 열까지 값"
   ],
   "id": "aab670f2cb9492de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# k_tensor_reshaping.py",
   "id": "ce1934b321d64440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T04:59:11.005345Z",
     "start_time": "2024-09-24T04:59:10.986956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)\n"
   ],
   "id": "5539a6d0c78da0e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "구조 변경\n",
    "- view(n,m): 텐서의 구조를 (n,m)으로 변경\n",
    "- reshape(n,m): 텐서의 구조를 (n,m)으로 변경\n",
    "- squeeze(): 크기가 1인 모든 차원을 제거한다.\n",
    "- squeeze(n): n번재 차원의 크기가 1이면 해당 차원을 제거한다.\n",
    "- unsqueeze(n): n번째 차원으로 크기가 1인 차원을 추가한다.\n",
    "- tensor.flatten(): 다차원 텐서를 1차원으로 변경한다.(평탄화)\n",
    "- torch.flatten(tensor): 텐서를 1차원으로 변경한다.\n",
    "- torch.flatten(tensor,start_dim=n): 텐서를 n번째 차원부터 평탄화를 진행한다.\n",
    "- permute(tensor, (n,m,k)): 텐서의 차원을 n,m,k로 재배열한다. 예를 들어 (2,3,5)형태의 텐서를 permute(tensor,(2,0,1)하면 (5,2,3)형태로 텐서의 구조가 변경된다.\n",
    "- transpose(tensor, n, m): n번 차원과 m번 차원을 교환\n",
    "- t(tensor): 2차원 텐서의 전치(행과 열을 교환)"
   ],
   "id": "c407a38eecb0d8ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# l_tensor_concat.py",
   "id": "7efd79b6d10f262b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T05:17:25.563855Z",
     "start_time": "2024-09-24T05:17:25.548454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ],
   "id": "bb5bd4557cb795e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "텐서의 병합\n",
    "- cat([tensor1,tensor2,...], dim=n): n번째 차원을 기준으로 tensor를 병합\n",
    "- cat((tensor1,tensor2,...), dim=n): n번째 차원을 기준으로 tensor를 병합"
   ],
   "id": "6dd4b8860c7882d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# m_tensor_stacking.py",
   "id": "ab680681bd6d2a80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T05:19:38.450452Z",
     "start_time": "2024-09-24T05:19:38.437450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ],
   "id": "595b464e609031d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "stack vs cat\n",
    "stack((tensor1, tensor2), dim=n) vs cat((tensor1, tensor2), dim=n)\n",
    "- 모두 동일한 결과가 나온다."
   ],
   "id": "5badb39cd669139b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# n_tensor_vstack_hstack.py",
   "id": "203204f27f3f414a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T05:23:22.876337Z",
     "start_time": "2024-09-24T05:23:22.859998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ],
   "id": "c31e8738982b78b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "vstack, hstack\n",
    "- vstack((tensor1, tensor2)): 두 개의 텐서를 첫 번째 차원에서 쌓는다.\n",
    "- hstack((tensor1, tensor2)): 두 개의 텐서를 두 번째 차원에서 쌓는다."
   ],
   "id": "f8e885c73c9153ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기\n",
    "텐서의 병합 파트가 생각한 것 보다 어렵다. 몇 번더 보고, 해볼 필요가 있다고 생각했다.\n"
   ],
   "id": "6322f80c53765441"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
