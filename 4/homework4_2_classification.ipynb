{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c695175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.6.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m203.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m260.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.19.2 setproctitle-1.3.4 wandb-0.19.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.692069Z",
     "start_time": "2024-12-18T04:23:46.210516Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd263028f01d48c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.698763Z",
     "start_time": "2024-12-18T04:23:53.694053Z"
    }
   },
   "outputs": [],
   "source": [
    "#체크포인트 경로 설정 및 폴도 만들기\n",
    "CHECKPOINT_FILE_PATH = os.path.join(os.getcwd(), 'checkpoints_classification2')\n",
    "\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "    os.makedirs(CHECKPOINT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134b8d3dc2b3a9ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.720035Z",
     "start_time": "2024-12-18T04:23:53.700756Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import strfdelta\n",
    "from utils.early_stopping import EarlyStopping\n",
    "\n",
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Will be explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=-1)\n",
    "\n",
    "      # >>> predicted_train: tensor([5, 8, 9, 0, 9, 8, 9, 8, ..., 0, 1, 3, 7, 1, 4, 3])\n",
    "      # >>> target_train:    tensor([5, 8, 9, 2, 9, 8, 7, 8, ..., 4, 1, 9, 6, 1, 4, 3])\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553568ab0a68c8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.734475Z",
     "start_time": "2024-12-18T04:23:53.721031Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, target_column='Close', y_normalizer=1.0e7, is_regression=True\n",
    "):\n",
    "  btc_krw_path = os.path.join(os.getcwd(), \"BTC_KRW.csv\")\n",
    "  df = pd.read_csv(btc_krw_path)\n",
    "  row_size = len(df)\n",
    "  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "  date_list = df['Date']\n",
    "\n",
    "  df = df.drop(columns=['Date'])\n",
    "\n",
    "  data_size = row_size - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  y_train_classification_list = []\n",
    "  y_train_date = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_train_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  y_validation_classification_list = []\n",
    "  y_validation_date = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_validation_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_validation = (X_validation - m) / s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  y_test_classification_list = []\n",
    "  y_test_date = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_test_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_test = (X_test - m) / s\n",
    "\n",
    "  if is_regression:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_regression, y_validation_regression, y_test_regression,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "  else:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_classification, y_validation_classification, y_test_classification,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6873009fbea0eaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.742426Z",
     "start_time": "2024-12-18T04:23:53.736468Z"
    }
   },
   "outputs": [],
   "source": [
    "class CryptoCurrencyDataset(Dataset):\n",
    "  def __init__(self, X, y, is_regression=True):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6342d922bc915e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.749603Z",
     "start_time": "2024-12-18T04:23:53.743419Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_btc_krw_data(sequence_size=21, validation_size=150, test_size=30, is_regression=True):\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date = get_cryptocurrency_data(\n",
    "    sequence_size= sequence_size, validation_size=validation_size, test_size=test_size, target_column='Close', y_normalizer=1.0e7, is_regression=is_regression\n",
    "  ) #데이터 분할\n",
    "  \n",
    "  #데이터를 커스텀 dataset에 저장\n",
    "  train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train)\n",
    "  validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation)\n",
    "  test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test)\n",
    "\n",
    "  #dataset을 dataloader에 저장해 배치 단위로 사용할 수 있게 함\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset = train_crypto_currency_dataset,batch_size=wandb.config.batch_size,shuffle=True\n",
    "  )\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset = validation_crypto_currency_dataset,batch_size=wandb.config.batch_size,shuffle=True\n",
    "  )\n",
    "  test_data_loader = DataLoader(\n",
    "    dataset=test_crypto_currency_dataset,batch_size=len(test_crypto_currency_dataset),shuffle=True\n",
    "  )\n",
    "  #데이터 로더 반환\n",
    "  return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27005c12f573739a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.756580Z",
     "start_time": "2024-12-18T04:23:53.750596Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, p):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.lstm=nn.LSTM(input_size=n_input, hidden_size=128, num_layers=9, batch_first = True)\n",
    "      self.fcn = nn.Linear(in_features=128, out_features = 64)\n",
    "      self.fcn2 = nn.Linear(in_features=64, out_features = 32)\n",
    "      self.fcn3 = nn.Linear(in_features=32, out_features = n_output)\n",
    "      self.dropout = nn.Dropout(p=p)\n",
    "    def forward(self, x):\n",
    "      x, hidden = self.lstm(x)\n",
    "      x = x[:,-1,:]\n",
    "      x = self.fcn(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fcn2(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fcn3(x)\n",
    "      return x\n",
    "  \n",
    "  my_model = MyModel(n_input= 5, n_output= 2, p=0.05)\n",
    "  \n",
    "  return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f442f9c7398854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.764362Z",
     "start_time": "2024-12-18T04:23:53.758088Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_train(epochs, batch_size, validation_intervals, learning_rate ,early_stop_patience, early_stop_delta, weight_decay):\n",
    "  \n",
    "  run_time_str = datetime.now().astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "  \n",
    "  config = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_intervals': validation_intervals,\n",
    "    'learning_rate': learning_rate,\n",
    "    'early_stop_patience': early_stop_patience,\n",
    "    'early_stop_delta': early_stop_delta,\n",
    "    'weight_decay': weight_decay\n",
    "  }\n",
    "  \n",
    "  project_name = 'lstm_classification_btc_krw'\n",
    "  wandb.init(\n",
    "    mode = 'online',\n",
    "    project  = project_name,\n",
    "    notes = 'btc_krw experiment with lstm',\n",
    "    tags = ['lstm', ' classification', 'btc_krw'],\n",
    "    name = run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  \n",
    "  train_data_loader, validation_data_loader, _ = get_btc_krw_data(is_regression = False)\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  \n",
    "  model = get_model()\n",
    "  model.to(device)\n",
    "  \n",
    "  optimizer = optim.Adam(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
    "  \n",
    "  classification_trainer = ClassificationTrainer(\n",
    "    project_name, model, optimizer, train_data_loader, validation_data_loader, None, run_time_str, wandb, device, CHECKPOINT_FILE_PATH\n",
    "  )\n",
    "  classification_trainer.train_loop()\n",
    "  \n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d2f1b334a3b8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.772033Z",
     "start_time": "2024-12-18T04:23:53.765359Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(test_model):\n",
    "  _, _, test_data_loader = get_btc_krw_data(is_regression=False)\n",
    "\n",
    "  test_model.eval()\n",
    "\n",
    "  num_corrects_test = 0\n",
    "  num_tested_samples = 0\n",
    "\n",
    "  print(\"[TEST DATA]\")\n",
    "  with torch.no_grad():\n",
    "    for test_batch in test_data_loader:\n",
    "      input_test, target_test = test_batch\n",
    "\n",
    "      output_test = test_model(input_test)\n",
    "\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "      num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "      num_tested_samples += len(input_test)\n",
    "\n",
    "    test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "    for idx, (output, target) in enumerate(zip(output_test, target_test)):\n",
    "      print(\"{0:2}: {1:6,.2f} <--> {2:6,.2f}\".format(\n",
    "        idx, torch.argmax(output).item(), target.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c54ab598acc9d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.779760Z",
     "start_time": "2024-12-18T04:23:53.774027Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_test(epochs, batch_size, validation_intervals, learning_rate ,early_stop_patience, early_stop_delta):\n",
    "  run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "  \n",
    "  config = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_intervals': validation_intervals,\n",
    "    'learning_rate': learning_rate,\n",
    "    'early_stop_patience': early_stop_patience,\n",
    "    'early_stop_delta': early_stop_delta,\n",
    "  }\n",
    "  \n",
    "  project_name = 'lstm_classification_btc_krw'\n",
    "  wandb.init(\n",
    "    mode=\"disabled\",\n",
    "    project=project_name,\n",
    "    notes=\"btc_krw experiment with lstm\",\n",
    "    tags=[\"lstm\", \"regression\", \"btc_krw\"],\n",
    "    name=run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  \n",
    "  test_model = get_model()\n",
    "  \n",
    "  latest_file_path = os.path.join(\n",
    "    CHECKPOINT_FILE_PATH, f\"{project_name}_checkpoint_latest.pt\"\n",
    "  )\n",
    "  \n",
    "  print(\"MODEL FILE: {0}\".format(latest_file_path))\n",
    "  test_model.load_state_dict(torch.load(latest_file_path, map_location=torch.device('cpu')))\n",
    "  \n",
    "  test(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de5a52dac21b5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:24:44.984399Z",
     "start_time": "2024-12-18T04:24:26.303698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/work/.netrc\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/4/wandb/run-20241218_111510-3jiynk95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/3jiynk95' target=\"_blank\">2024_12_18_11_15_04</a></strong> to <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/3jiynk95' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/3jiynk95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1] T_loss: 0.69198, T_accuracy: 52.9629 | V_loss: 0.68976, V_accuracy: 52.6667 | Early stopping is stated! | T_time: 00:00:01, T_speed: 1.000\n",
      "[Epoch  10] T_loss: 0.69165, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 1 out of 100 | T_time: 00:00:09, T_speed: 1.111\n",
      "[Epoch  20] T_loss: 0.69068, T_accuracy: 52.9629 | V_loss: 0.69362, V_accuracy: 52.6667 | Early stopping counter: 2 out of 100 | T_time: 00:00:18, T_speed: 1.111\n",
      "[Epoch  30] T_loss: 0.69183, T_accuracy: 52.9629 | V_loss: 0.69243, V_accuracy: 52.6667 | Early stopping counter: 3 out of 100 | T_time: 00:00:27, T_speed: 1.111\n",
      "[Epoch  40] T_loss: 0.69112, T_accuracy: 52.9629 | V_loss: 0.69243, V_accuracy: 52.6667 | Early stopping counter: 4 out of 100 | T_time: 00:00:37, T_speed: 1.081\n",
      "[Epoch  50] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.69702, V_accuracy: 52.6667 | Early stopping counter: 5 out of 100 | T_time: 00:00:46, T_speed: 1.087\n",
      "[Epoch  60] T_loss: 0.69139, T_accuracy: 52.9629 | V_loss: 0.69649, V_accuracy: 52.6667 | Early stopping counter: 6 out of 100 | T_time: 00:00:55, T_speed: 1.091\n",
      "[Epoch  70] T_loss: 0.69078, T_accuracy: 52.9629 | V_loss: 0.69516, V_accuracy: 52.6667 | Early stopping counter: 7 out of 100 | T_time: 00:01:04, T_speed: 1.094\n",
      "[Epoch  80] T_loss: 0.69169, T_accuracy: 52.9629 | V_loss: 0.69344, V_accuracy: 52.6667 | Early stopping counter: 8 out of 100 | T_time: 00:01:14, T_speed: 1.081\n",
      "[Epoch  90] T_loss: 0.69106, T_accuracy: 52.9629 | V_loss: 0.69497, V_accuracy: 52.6667 | Early stopping counter: 9 out of 100 | T_time: 00:01:23, T_speed: 1.084\n",
      "[Epoch 100] T_loss: 0.69162, T_accuracy: 52.9629 | V_loss: 0.69001, V_accuracy: 52.6667 | Early stopping counter: 10 out of 100 | T_time: 00:01:32, T_speed: 1.087\n",
      "[Epoch 110] T_loss: 0.69248, T_accuracy: 52.9629 | V_loss: 0.69242, V_accuracy: 52.6667 | Early stopping counter: 11 out of 100 | T_time: 00:01:39, T_speed: 1.111\n",
      "[Epoch 120] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.68766, V_accuracy: 52.6667 | V_loss decreased (0.68976 --> 0.68766). Saving model... | T_time: 00:01:44, T_speed: 1.154\n",
      "[Epoch 130] T_loss: 0.69142, T_accuracy: 52.9629 | V_loss: 0.69358, V_accuracy: 52.6667 | Early stopping counter: 1 out of 100 | T_time: 00:01:49, T_speed: 1.193\n",
      "[Epoch 140] T_loss: 0.69219, T_accuracy: 52.9629 | V_loss: 0.69245, V_accuracy: 52.6667 | Early stopping counter: 2 out of 100 | T_time: 00:01:54, T_speed: 1.228\n",
      "[Epoch 150] T_loss: 0.69160, T_accuracy: 52.9629 | V_loss: 0.69013, V_accuracy: 52.6667 | Early stopping counter: 3 out of 100 | T_time: 00:01:59, T_speed: 1.261\n",
      "[Epoch 160] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69005, V_accuracy: 52.6667 | Early stopping counter: 4 out of 100 | T_time: 00:02:04, T_speed: 1.290\n",
      "[Epoch 170] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.68875, V_accuracy: 52.6667 | Early stopping counter: 5 out of 100 | T_time: 00:02:08, T_speed: 1.328\n",
      "[Epoch 180] T_loss: 0.69162, T_accuracy: 52.9629 | V_loss: 0.69246, V_accuracy: 52.6667 | Early stopping counter: 6 out of 100 | T_time: 00:02:13, T_speed: 1.353\n",
      "[Epoch 190] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69358, V_accuracy: 52.6667 | Early stopping counter: 7 out of 100 | T_time: 00:02:18, T_speed: 1.377\n",
      "[Epoch 200] T_loss: 0.69161, T_accuracy: 52.9629 | V_loss: 0.69012, V_accuracy: 52.6667 | Early stopping counter: 8 out of 100 | T_time: 00:02:23, T_speed: 1.399\n",
      "[Epoch 210] T_loss: 0.69159, T_accuracy: 52.9629 | V_loss: 0.69245, V_accuracy: 52.6667 | Early stopping counter: 9 out of 100 | T_time: 00:02:28, T_speed: 1.419\n",
      "[Epoch 220] T_loss: 0.69187, T_accuracy: 52.9629 | V_loss: 0.69009, V_accuracy: 52.6667 | Early stopping counter: 10 out of 100 | T_time: 00:02:32, T_speed: 1.447\n",
      "[Epoch 230] T_loss: 0.69161, T_accuracy: 52.9629 | V_loss: 0.69614, V_accuracy: 52.6667 | Early stopping counter: 11 out of 100 | T_time: 00:02:37, T_speed: 1.465\n",
      "[Epoch 240] T_loss: 0.69190, T_accuracy: 52.9629 | V_loss: 0.69367, V_accuracy: 52.6667 | Early stopping counter: 12 out of 100 | T_time: 00:02:42, T_speed: 1.481\n",
      "[Epoch 250] T_loss: 0.69159, T_accuracy: 52.9629 | V_loss: 0.68999, V_accuracy: 52.6667 | Early stopping counter: 13 out of 100 | T_time: 00:02:47, T_speed: 1.497\n",
      "[Epoch 260] T_loss: 0.69134, T_accuracy: 52.9629 | V_loss: 0.69248, V_accuracy: 52.6667 | Early stopping counter: 14 out of 100 | T_time: 00:02:52, T_speed: 1.512\n",
      "[Epoch 270] T_loss: 0.69161, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 15 out of 100 | T_time: 00:02:56, T_speed: 1.534\n",
      "[Epoch 280] T_loss: 0.69219, T_accuracy: 52.9629 | V_loss: 0.69721, V_accuracy: 52.6667 | Early stopping counter: 16 out of 100 | T_time: 00:03:01, T_speed: 1.547\n",
      "[Epoch 290] T_loss: 0.69137, T_accuracy: 52.9629 | V_loss: 0.69367, V_accuracy: 52.6667 | Early stopping counter: 17 out of 100 | T_time: 00:03:06, T_speed: 1.559\n",
      "[Epoch 300] T_loss: 0.69211, T_accuracy: 52.9629 | V_loss: 0.69345, V_accuracy: 52.6667 | Early stopping counter: 18 out of 100 | T_time: 00:03:11, T_speed: 1.571\n",
      "[Epoch 310] T_loss: 0.69189, T_accuracy: 52.9629 | V_loss: 0.69353, V_accuracy: 52.6667 | Early stopping counter: 19 out of 100 | T_time: 00:03:16, T_speed: 1.582\n",
      "[Epoch 320] T_loss: 0.69213, T_accuracy: 52.9629 | V_loss: 0.69237, V_accuracy: 52.6667 | Early stopping counter: 20 out of 100 | T_time: 00:03:20, T_speed: 1.600\n",
      "[Epoch 330] T_loss: 0.69110, T_accuracy: 52.9629 | V_loss: 0.68896, V_accuracy: 52.6667 | Early stopping counter: 21 out of 100 | T_time: 00:03:25, T_speed: 1.610\n",
      "[Epoch 340] T_loss: 0.69106, T_accuracy: 52.9629 | V_loss: 0.69002, V_accuracy: 52.6667 | Early stopping counter: 22 out of 100 | T_time: 00:03:30, T_speed: 1.619\n",
      "[Epoch 350] T_loss: 0.69186, T_accuracy: 52.9629 | V_loss: 0.69360, V_accuracy: 52.6667 | Early stopping counter: 23 out of 100 | T_time: 00:03:35, T_speed: 1.628\n",
      "[Epoch 360] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.68766, V_accuracy: 52.6667 | Early stopping counter: 24 out of 100 | T_time: 00:03:40, T_speed: 1.636\n",
      "[Epoch 370] T_loss: 0.69161, T_accuracy: 52.9629 | V_loss: 0.69011, V_accuracy: 52.6667 | Early stopping counter: 25 out of 100 | T_time: 00:03:44, T_speed: 1.652\n",
      "[Epoch 380] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69032, V_accuracy: 52.6667 | Early stopping counter: 26 out of 100 | T_time: 00:03:49, T_speed: 1.659\n",
      "[Epoch 390] T_loss: 0.69106, T_accuracy: 52.9629 | V_loss: 0.69479, V_accuracy: 52.6667 | Early stopping counter: 27 out of 100 | T_time: 00:03:54, T_speed: 1.667\n",
      "[Epoch 400] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 28 out of 100 | T_time: 00:03:59, T_speed: 1.674\n",
      "[Epoch 410] T_loss: 0.69133, T_accuracy: 52.9629 | V_loss: 0.69606, V_accuracy: 52.6667 | Early stopping counter: 29 out of 100 | T_time: 00:04:04, T_speed: 1.680\n",
      "[Epoch 420] T_loss: 0.69191, T_accuracy: 52.9629 | V_loss: 0.69590, V_accuracy: 52.6667 | Early stopping counter: 30 out of 100 | T_time: 00:04:08, T_speed: 1.694\n",
      "[Epoch 430] T_loss: 0.69139, T_accuracy: 52.9629 | V_loss: 0.69239, V_accuracy: 52.6667 | Early stopping counter: 31 out of 100 | T_time: 00:04:13, T_speed: 1.700\n",
      "[Epoch 440] T_loss: 0.69108, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 32 out of 100 | T_time: 00:04:18, T_speed: 1.705\n",
      "[Epoch 450] T_loss: 0.69132, T_accuracy: 52.9629 | V_loss: 0.69000, V_accuracy: 52.6667 | Early stopping counter: 33 out of 100 | T_time: 00:04:23, T_speed: 1.711\n",
      "[Epoch 460] T_loss: 0.69107, T_accuracy: 52.9629 | V_loss: 0.69244, V_accuracy: 52.6667 | Early stopping counter: 34 out of 100 | T_time: 00:04:28, T_speed: 1.716\n",
      "[Epoch 470] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.69354, V_accuracy: 52.6667 | Early stopping counter: 35 out of 100 | T_time: 00:04:33, T_speed: 1.722\n",
      "[Epoch 480] T_loss: 0.69170, T_accuracy: 52.9629 | V_loss: 0.68539, V_accuracy: 52.6667 | V_loss decreased (0.68766 --> 0.68539). Saving model... | T_time: 00:04:37, T_speed: 1.733\n",
      "[Epoch 490] T_loss: 0.69132, T_accuracy: 52.9629 | V_loss: 0.69002, V_accuracy: 52.6667 | Early stopping counter: 1 out of 100 | T_time: 00:04:42, T_speed: 1.738\n",
      "[Epoch 500] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.69241, V_accuracy: 52.6667 | Early stopping counter: 2 out of 100 | T_time: 00:04:47, T_speed: 1.742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 510] T_loss: 0.69187, T_accuracy: 52.9629 | V_loss: 0.69236, V_accuracy: 52.6667 | Early stopping counter: 3 out of 100 | T_time: 00:04:52, T_speed: 1.747\n",
      "[Epoch 520] T_loss: 0.69108, T_accuracy: 52.9629 | V_loss: 0.69255, V_accuracy: 52.6667 | Early stopping counter: 4 out of 100 | T_time: 00:04:57, T_speed: 1.751\n",
      "[Epoch 530] T_loss: 0.69184, T_accuracy: 52.9629 | V_loss: 0.69562, V_accuracy: 52.6667 | Early stopping counter: 5 out of 100 | T_time: 00:05:01, T_speed: 1.761\n",
      "[Epoch 540] T_loss: 0.69133, T_accuracy: 52.9629 | V_loss: 0.68765, V_accuracy: 52.6667 | Early stopping counter: 6 out of 100 | T_time: 00:05:06, T_speed: 1.765\n",
      "[Epoch 550] T_loss: 0.69135, T_accuracy: 52.9629 | V_loss: 0.68887, V_accuracy: 52.6667 | Early stopping counter: 7 out of 100 | T_time: 00:05:11, T_speed: 1.768\n",
      "[Epoch 560] T_loss: 0.69187, T_accuracy: 52.9629 | V_loss: 0.69242, V_accuracy: 52.6667 | Early stopping counter: 8 out of 100 | T_time: 00:05:16, T_speed: 1.772\n",
      "[Epoch 570] T_loss: 0.69107, T_accuracy: 52.9629 | V_loss: 0.69244, V_accuracy: 52.6667 | Early stopping counter: 9 out of 100 | T_time: 00:05:21, T_speed: 1.776\n",
      "[Epoch 580] T_loss: 0.69105, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 10 out of 100 | T_time: 00:05:25, T_speed: 1.785\n",
      "[Epoch 590] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 11 out of 100 | T_time: 00:05:30, T_speed: 1.788\n",
      "[Epoch 600] T_loss: 0.69135, T_accuracy: 52.9629 | V_loss: 0.69001, V_accuracy: 52.6667 | Early stopping counter: 12 out of 100 | T_time: 00:05:35, T_speed: 1.791\n",
      "[Epoch 610] T_loss: 0.69134, T_accuracy: 52.9629 | V_loss: 0.69004, V_accuracy: 52.6667 | Early stopping counter: 13 out of 100 | T_time: 00:05:40, T_speed: 1.794\n",
      "[Epoch 620] T_loss: 0.69108, T_accuracy: 52.9629 | V_loss: 0.69365, V_accuracy: 52.6667 | Early stopping counter: 14 out of 100 | T_time: 00:05:44, T_speed: 1.802\n",
      "[Epoch 630] T_loss: 0.69111, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 15 out of 100 | T_time: 00:05:49, T_speed: 1.805\n",
      "[Epoch 640] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.68632, V_accuracy: 52.6667 | Early stopping counter: 16 out of 100 | T_time: 00:05:54, T_speed: 1.808\n",
      "[Epoch 650] T_loss: 0.69116, T_accuracy: 52.9629 | V_loss: 0.68802, V_accuracy: 52.6667 | Early stopping counter: 17 out of 100 | T_time: 00:05:59, T_speed: 1.811\n",
      "[Epoch 660] T_loss: 0.69172, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 18 out of 100 | T_time: 00:06:06, T_speed: 1.803\n",
      "[Epoch 670] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69467, V_accuracy: 52.6667 | Early stopping counter: 19 out of 100 | T_time: 00:06:15, T_speed: 1.787\n",
      "[Epoch 680] T_loss: 0.69106, T_accuracy: 52.9629 | V_loss: 0.69481, V_accuracy: 52.6667 | Early stopping counter: 20 out of 100 | T_time: 00:06:24, T_speed: 1.771\n",
      "[Epoch 690] T_loss: 0.69134, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 21 out of 100 | T_time: 00:06:33, T_speed: 1.756\n",
      "[Epoch 700] T_loss: 0.69137, T_accuracy: 52.9629 | V_loss: 0.68998, V_accuracy: 52.6667 | Early stopping counter: 22 out of 100 | T_time: 00:06:42, T_speed: 1.741\n",
      "[Epoch 710] T_loss: 0.69137, T_accuracy: 52.9629 | V_loss: 0.68635, V_accuracy: 52.6667 | Early stopping counter: 23 out of 100 | T_time: 00:06:52, T_speed: 1.723\n",
      "[Epoch 720] T_loss: 0.69076, T_accuracy: 52.9629 | V_loss: 0.68997, V_accuracy: 52.6667 | Early stopping counter: 24 out of 100 | T_time: 00:07:02, T_speed: 1.706\n",
      "[Epoch 730] T_loss: 0.69104, T_accuracy: 52.9629 | V_loss: 0.69253, V_accuracy: 52.6667 | Early stopping counter: 25 out of 100 | T_time: 00:07:13, T_speed: 1.686\n",
      "[Epoch 740] T_loss: 0.69137, T_accuracy: 52.9629 | V_loss: 0.69478, V_accuracy: 52.6667 | Early stopping counter: 26 out of 100 | T_time: 00:07:23, T_speed: 1.670\n",
      "[Epoch 750] T_loss: 0.69080, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 27 out of 100 | T_time: 00:07:32, T_speed: 1.659\n",
      "[Epoch 760] T_loss: 0.69162, T_accuracy: 52.9629 | V_loss: 0.69008, V_accuracy: 52.6667 | Early stopping counter: 28 out of 100 | T_time: 00:07:41, T_speed: 1.649\n",
      "[Epoch 770] T_loss: 0.69140, T_accuracy: 52.9629 | V_loss: 0.69691, V_accuracy: 52.6667 | Early stopping counter: 29 out of 100 | T_time: 00:07:51, T_speed: 1.635\n",
      "[Epoch 780] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.69370, V_accuracy: 52.6667 | Early stopping counter: 30 out of 100 | T_time: 00:08:00, T_speed: 1.625\n",
      "[Epoch 790] T_loss: 0.69065, T_accuracy: 52.9629 | V_loss: 0.69013, V_accuracy: 52.6667 | Early stopping counter: 31 out of 100 | T_time: 00:08:09, T_speed: 1.616\n",
      "[Epoch 800] T_loss: 0.69171, T_accuracy: 52.9629 | V_loss: 0.68993, V_accuracy: 52.6667 | Early stopping counter: 32 out of 100 | T_time: 00:08:18, T_speed: 1.606\n",
      "[Epoch 810] T_loss: 0.69162, T_accuracy: 52.9629 | V_loss: 0.69482, V_accuracy: 52.6667 | Early stopping counter: 33 out of 100 | T_time: 00:08:27, T_speed: 1.598\n",
      "[Epoch 820] T_loss: 0.69110, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 34 out of 100 | T_time: 00:08:37, T_speed: 1.586\n",
      "[Epoch 830] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.68769, V_accuracy: 52.6667 | Early stopping counter: 35 out of 100 | T_time: 00:08:46, T_speed: 1.578\n",
      "[Epoch 840] T_loss: 0.69135, T_accuracy: 52.9629 | V_loss: 0.68663, V_accuracy: 52.6667 | Early stopping counter: 36 out of 100 | T_time: 00:08:55, T_speed: 1.570\n",
      "[Epoch 850] T_loss: 0.69167, T_accuracy: 52.9629 | V_loss: 0.69009, V_accuracy: 52.6667 | Early stopping counter: 37 out of 100 | T_time: 00:09:03, T_speed: 1.565\n",
      "[Epoch 860] T_loss: 0.69105, T_accuracy: 52.9629 | V_loss: 0.69498, V_accuracy: 52.6667 | Early stopping counter: 38 out of 100 | T_time: 00:09:07, T_speed: 1.572\n",
      "[Epoch 870] T_loss: 0.69105, T_accuracy: 52.9629 | V_loss: 0.69249, V_accuracy: 52.6667 | Early stopping counter: 39 out of 100 | T_time: 00:09:12, T_speed: 1.576\n",
      "[Epoch 880] T_loss: 0.69138, T_accuracy: 52.9629 | V_loss: 0.69239, V_accuracy: 52.6667 | Early stopping counter: 40 out of 100 | T_time: 00:09:17, T_speed: 1.580\n",
      "[Epoch 890] T_loss: 0.69159, T_accuracy: 52.9629 | V_loss: 0.69013, V_accuracy: 52.6667 | Early stopping counter: 41 out of 100 | T_time: 00:09:22, T_speed: 1.584\n",
      "[Epoch 900] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.69247, V_accuracy: 52.6667 | Early stopping counter: 42 out of 100 | T_time: 00:09:32, T_speed: 1.573\n",
      "[Epoch 910] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.68880, V_accuracy: 52.6667 | Early stopping counter: 43 out of 100 | T_time: 00:09:43, T_speed: 1.561\n",
      "[Epoch 920] T_loss: 0.69138, T_accuracy: 52.9629 | V_loss: 0.69242, V_accuracy: 52.6667 | Early stopping counter: 44 out of 100 | T_time: 00:09:53, T_speed: 1.551\n",
      "[Epoch 930] T_loss: 0.69164, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 45 out of 100 | T_time: 00:10:04, T_speed: 1.540\n",
      "[Epoch 940] T_loss: 0.69161, T_accuracy: 52.9629 | V_loss: 0.69125, V_accuracy: 52.6667 | Early stopping counter: 46 out of 100 | T_time: 00:10:15, T_speed: 1.528\n",
      "[Epoch 950] T_loss: 0.69136, T_accuracy: 52.9629 | V_loss: 0.68898, V_accuracy: 52.6667 | Early stopping counter: 47 out of 100 | T_time: 00:10:25, T_speed: 1.520\n",
      "[Epoch 960] T_loss: 0.69187, T_accuracy: 52.9629 | V_loss: 0.69439, V_accuracy: 52.6667 | Early stopping counter: 48 out of 100 | T_time: 00:10:35, T_speed: 1.512\n",
      "[Epoch 970] T_loss: 0.69133, T_accuracy: 52.9629 | V_loss: 0.69126, V_accuracy: 52.6667 | Early stopping counter: 49 out of 100 | T_time: 00:10:44, T_speed: 1.506\n",
      "[Epoch 980] T_loss: 0.69163, T_accuracy: 52.9629 | V_loss: 0.69007, V_accuracy: 52.6667 | Early stopping counter: 50 out of 100 | T_time: 00:10:53, T_speed: 1.501\n",
      "[Epoch 990] T_loss: 0.69160, T_accuracy: 52.9629 | V_loss: 0.69128, V_accuracy: 52.6667 | Early stopping counter: 51 out of 100 | T_time: 00:11:03, T_speed: 1.493\n",
      "[Epoch 1000] T_loss: 0.69162, T_accuracy: 52.9629 | V_loss: 0.69242, V_accuracy: 52.6667 | Early stopping counter: 52 out of 100 | T_time: 00:11:12, T_speed: 1.488\n",
      "Final training time: 00:11:12\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>Training accuracy (%)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training loss</td><td>▅▆▅▃▃▄▃▆▄█▁▆▅▅▅▃▅▃▅▆▃▁▅▃▂▃▃▁▅▃▅▅▁▃▅▃▃▃▄▃</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁▁▁▁▁▂▃▃▃▅▅▅▅▆▆▆▆▇▇▇▇▇███████▇▆▆▆▆▆▅▅▅▅▅</td></tr><tr><td>Validation accuracy (%)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>▄▅▄▄█▆▄▂▄▃▅▅▅▄▃▄▇▄▃▅▅▄▃▃▄▁▄▃▃▅▃▄▃▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1000</td></tr><tr><td>Training accuracy (%)</td><td>52.96286</td></tr><tr><td>Training loss</td><td>0.69162</td></tr><tr><td>Training speed (epochs/sec.)</td><td>1.4881</td></tr><tr><td>Validation accuracy (%)</td><td>52.66667</td></tr><tr><td>Validation loss</td><td>0.69242</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_12_18_11_15_04</strong> at: <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/3jiynk95' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/3jiynk95</a><br> View project at: <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241218_111510-3jiynk95/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /home/work/DL/4/checkpoints_classification2/lstm_classification_btc_krw_checkpoint_latest.pt\n",
      "[TEST DATA]\n",
      "TEST RESULTS: 53.333%\n",
      " 0:   1.00 <-->   0.00\n",
      " 1:   1.00 <-->   1.00\n",
      " 2:   1.00 <-->   1.00\n",
      " 3:   1.00 <-->   0.00\n",
      " 4:   1.00 <-->   0.00\n",
      " 5:   1.00 <-->   0.00\n",
      " 6:   1.00 <-->   1.00\n",
      " 7:   1.00 <-->   1.00\n",
      " 8:   1.00 <-->   0.00\n",
      " 9:   1.00 <-->   0.00\n",
      "10:   1.00 <-->   1.00\n",
      "11:   1.00 <-->   0.00\n",
      "12:   1.00 <-->   0.00\n",
      "13:   1.00 <-->   0.00\n",
      "14:   1.00 <-->   1.00\n",
      "15:   1.00 <-->   0.00\n",
      "16:   1.00 <-->   1.00\n",
      "17:   1.00 <-->   1.00\n",
      "18:   1.00 <-->   1.00\n",
      "19:   1.00 <-->   0.00\n",
      "20:   1.00 <-->   1.00\n",
      "21:   1.00 <-->   0.00\n",
      "22:   1.00 <-->   1.00\n",
      "23:   1.00 <-->   0.00\n",
      "24:   1.00 <-->   1.00\n",
      "25:   1.00 <-->   1.00\n",
      "26:   1.00 <-->   1.00\n",
      "27:   1.00 <-->   0.00\n",
      "28:   1.00 <-->   1.00\n",
      "29:   1.00 <-->   1.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "    validation_intervals = 10\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 100\n",
    "    early_stop_delta = 0.00001\n",
    "    weight_decay  = 0.001\n",
    "  \n",
    "    main_train(epochs, batch_size, validation_intervals, learning_rate,early_stop_patience, early_stop_delta, weight_decay)\n",
    "    main_test(epochs, batch_size, validation_intervals, learning_rate, early_stop_patience, early_stop_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f6170",
   "metadata": {},
   "source": [
    "1. num_layers: 2 -> 4   : 53.333\n",
    "2. batch_size: 32 -> 64 : 53.333\n",
    "-------------------------------\n",
    "#### 2번에서 나온 결과 -> 전부 1로 예측\n",
    "-------------------------------\n",
    "3. num_layers: 4 -> 2    : 46.667\n",
    "-------------------------------\n",
    "#### 3번에서 나온 결과 -> 전부 0으로 예측\n",
    "-------------------------------\n",
    "4. num_layers: 2 -> 3    : 46.667\n",
    "-------------------------------\n",
    "#### 4번에서 나온 결과 -> 전부 0으로 예측\n",
    "-------------------------------\n",
    "5. num_layers: 3->2, fcn늘리기(128>64>32>n_output) :53.333\n",
    "-------------------------------\n",
    "#### 5번에서 나온 결과 -> 0과 1을 번갈아가면서 예측 / 잘못함 6번 이후로 알았는데 init에서 추가하고 forward에는 추가를 안함 -> 추가안한것\n",
    "-------------------------------\n",
    "6. weight_decay: 0.001 -> 0.0001 : 53.333\n",
    "-------------------------------\n",
    "#### 6번에서 나온 결과 -> 전부 1로 예측\n",
    "-------------------------------\n",
    "7. weight_decay: 0.0001 -> 0.001, fcn과 dropout(0.2)추가: 53.333\n",
    "-------------------------------\n",
    "#### 7번에서 나온 결과 -> 전부 1로 예측\n",
    "-------------------------------\n",
    "8. early_stop_patience: 10 -> 100 : 53.333\n",
    "-------------------------------\n",
    "#### 8번에서 나온 결과 -> 전부 1로 예측\n",
    "-------------------------------\n",
    "9. dropout 0.2 -> 0.05 : 53.333\n",
    "-------------------------------\n",
    "#### 9번에서 나온 결과 -> 전부 1로 예측\n",
    "-------------------------------\n",
    "10. num_layers 2->9 : 53.333\n",
    "-------------------------------\n",
    "#### 10번에서 나온 결과 -> 전부 1로 예측\n",
    "#### 뭔가 문제가 있는 것 같은데...\n",
    "    데이터에 문제가 있나?\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf77c3",
   "metadata": {},
   "source": [
    "## 숙제 후기\n",
    "\n",
    " 문제가 있는지 1번 과제때부터 계속해서 하나의 영역으로만 분류하는 문제가 있어 문제를 해결하지 못했다.\n",
    " 조금 일찍 시작해서 교수님이나 조교님에게 물어볼 시간을 확보했어야 했는데 너무 늦게 시작했다는 생각이 들었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3 (NGC 24.03/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
