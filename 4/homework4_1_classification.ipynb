{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c14430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.6.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m240.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.19.2 setproctitle-1.3.4 wandb-0.19.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.692069Z",
     "start_time": "2024-12-18T04:23:46.210516Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd263028f01d48c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.698763Z",
     "start_time": "2024-12-18T04:23:53.694053Z"
    }
   },
   "outputs": [],
   "source": [
    "#체크포인트 경로 설정 및 폴도 만들기\n",
    "CHECKPOINT_FILE_PATH = os.path.join(os.getcwd(), 'checkpoints_classification')\n",
    "\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "    os.makedirs(CHECKPOINT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134b8d3dc2b3a9ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.720035Z",
     "start_time": "2024-12-18T04:23:53.700756Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import strfdelta\n",
    "from utils.early_stopping import EarlyStopping\n",
    "\n",
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Will be explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=-1)\n",
    "\n",
    "      # >>> predicted_train: tensor([5, 8, 9, 0, 9, 8, 9, 8, ..., 0, 1, 3, 7, 1, 4, 3])\n",
    "      # >>> target_train:    tensor([5, 8, 9, 2, 9, 8, 7, 8, ..., 4, 1, 9, 6, 1, 4, 3])\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553568ab0a68c8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.734475Z",
     "start_time": "2024-12-18T04:23:53.721031Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, target_column='Close', y_normalizer=1.0e7, is_regression=True\n",
    "):\n",
    "  btc_krw_path = os.path.join(os.getcwd(), \"BTC_KRW.csv\")\n",
    "  df = pd.read_csv(btc_krw_path)\n",
    "  row_size = len(df)\n",
    "  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "  date_list = df['Date']\n",
    "\n",
    "  df = df.drop(columns=['Date'])\n",
    "\n",
    "  data_size = row_size - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  y_train_classification_list = []\n",
    "  y_train_date = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_train_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  y_validation_classification_list = []\n",
    "  y_validation_date = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_validation_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_validation = (X_validation - m) / s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  y_test_classification_list = []\n",
    "  y_test_date = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_test_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_test = (X_test - m) / s\n",
    "\n",
    "  if is_regression:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_regression, y_validation_regression, y_test_regression,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "  else:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_classification, y_validation_classification, y_test_classification,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6873009fbea0eaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.742426Z",
     "start_time": "2024-12-18T04:23:53.736468Z"
    }
   },
   "outputs": [],
   "source": [
    "class CryptoCurrencyDataset(Dataset):\n",
    "  def __init__(self, X, y, is_regression=True):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6342d922bc915e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.749603Z",
     "start_time": "2024-12-18T04:23:53.743419Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_btc_krw_data(sequence_size=21, validation_size=150, test_size=30, is_regression=True):\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date = get_cryptocurrency_data(\n",
    "    sequence_size= sequence_size, validation_size=validation_size, test_size=test_size, target_column='Close', y_normalizer=1.0e7, is_regression=is_regression\n",
    "  ) #데이터 분할\n",
    "  \n",
    "  #데이터를 커스텀 dataset에 저장\n",
    "  train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train)\n",
    "  validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation)\n",
    "  test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test)\n",
    "\n",
    "  #dataset을 dataloader에 저장해 배치 단위로 사용할 수 있게 함\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset = train_crypto_currency_dataset,batch_size=wandb.config.batch_size,shuffle=True\n",
    "  )\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset = validation_crypto_currency_dataset,batch_size=wandb.config.batch_size,shuffle=True\n",
    "  )\n",
    "  test_data_loader = DataLoader(\n",
    "    dataset=test_crypto_currency_dataset,batch_size=len(test_crypto_currency_dataset),shuffle=True\n",
    "  )\n",
    "  #데이터 로더 반환\n",
    "  return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27005c12f573739a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.756580Z",
     "start_time": "2024-12-18T04:23:53.750596Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.lstm=nn.LSTM(input_size=n_input, hidden_size=128, num_layers=2, batch_first = True)\n",
    "      self.fcn = nn.Linear(in_features=128, out_features = n_output)\n",
    "      \n",
    "    def forward(self, x):\n",
    "      x, hidden = self.lstm(x)\n",
    "      x = x[:,-1,:]\n",
    "      x = self.fcn(x)\n",
    "      return x\n",
    "  \n",
    "  my_model = MyModel(n_input= 5, n_output= 2)\n",
    "  \n",
    "  return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f442f9c7398854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.764362Z",
     "start_time": "2024-12-18T04:23:53.758088Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_train(epochs, batch_size, validation_intervals, learning_rate ,early_stop_patience, early_stop_delta, weight_decay):\n",
    "  \n",
    "  run_time_str = datetime.now().astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "  \n",
    "  config = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_intervals': validation_intervals,\n",
    "    'learning_rate': learning_rate,\n",
    "    'early_stop_patience': early_stop_patience,\n",
    "    'early_stop_delta': early_stop_delta,\n",
    "    'weight_decay': weight_decay\n",
    "  }\n",
    "  \n",
    "  project_name = 'lstm_classification_btc_krw'\n",
    "  wandb.init(\n",
    "    mode = 'online',\n",
    "    project  = project_name,\n",
    "    notes = 'btc_krw experiment with lstm',\n",
    "    tags = ['lstm', ' classification', 'btc_krw'],\n",
    "    name = run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  \n",
    "  train_data_loader, validation_data_loader, _ = get_btc_krw_data(is_regression = False)\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  \n",
    "  model = get_model()\n",
    "  model.to(device)\n",
    "  \n",
    "  optimizer = optim.Adam(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
    "  \n",
    "  classification_trainer = ClassificationTrainer(\n",
    "    project_name, model, optimizer, train_data_loader, validation_data_loader, None, run_time_str, wandb, device, CHECKPOINT_FILE_PATH\n",
    "  )\n",
    "  classification_trainer.train_loop()\n",
    "  \n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d2f1b334a3b8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.772033Z",
     "start_time": "2024-12-18T04:23:53.765359Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(test_model):\n",
    "  _, _, test_data_loader = get_btc_krw_data(is_regression=False)\n",
    "\n",
    "  test_model.eval()\n",
    "\n",
    "  num_corrects_test = 0\n",
    "  num_tested_samples = 0\n",
    "\n",
    "  print(\"[TEST DATA]\")\n",
    "  with torch.no_grad():\n",
    "    for test_batch in test_data_loader:\n",
    "      input_test, target_test = test_batch\n",
    "\n",
    "      output_test = test_model(input_test)\n",
    "\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "      num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "      num_tested_samples += len(input_test)\n",
    "\n",
    "    test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "    for idx, (output, target) in enumerate(zip(output_test, target_test)):\n",
    "      print(\"{0:2}: {1:6,.2f} <--> {2:6,.2f}\".format(\n",
    "        idx, torch.argmax(output).item(), target.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c54ab598acc9d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:23:53.779760Z",
     "start_time": "2024-12-18T04:23:53.774027Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_test(epochs, batch_size, validation_intervals, learning_rate ,early_stop_patience, early_stop_delta):\n",
    "  run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "  \n",
    "  config = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_intervals': validation_intervals,\n",
    "    'learning_rate': learning_rate,\n",
    "    'early_stop_patience': early_stop_patience,\n",
    "    'early_stop_delta': early_stop_delta,\n",
    "  }\n",
    "  \n",
    "  project_name = 'lstm_classification_btc_krw'\n",
    "  wandb.init(\n",
    "    mode=\"disabled\",\n",
    "    project=project_name,\n",
    "    notes=\"btc_krw experiment with lstm\",\n",
    "    tags=[\"lstm\", \"regression\", \"btc_krw\"],\n",
    "    name=run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  \n",
    "  test_model = get_model()\n",
    "  \n",
    "  latest_file_path = os.path.join(\n",
    "    CHECKPOINT_FILE_PATH, f\"{project_name}_checkpoint_latest.pt\"\n",
    "  )\n",
    "  \n",
    "  print(\"MODEL FILE: {0}\".format(latest_file_path))\n",
    "  test_model.load_state_dict(torch.load(latest_file_path, map_location=torch.device('cpu')))\n",
    "  \n",
    "  test(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de5a52dac21b5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T04:24:44.984399Z",
     "start_time": "2024-12-18T04:24:26.303698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/work/.netrc\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/4/wandb/run-20241218_042705-r6j82p0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/r6j82p0y' target=\"_blank\">2024_12_18_04_26_47</a></strong> to <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/r6j82p0y' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/r6j82p0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1] T_loss: 0.69176, T_accuracy: 52.4525 | V_loss: 0.69420, V_accuracy: 47.3333 | Early stopping is stated! | T_time: 00:00:00, T_speed: 0.000\n",
      "[Epoch  10] T_loss: 0.69159, T_accuracy: 52.3958 | V_loss: 0.69181, V_accuracy: 52.6667 | V_loss decreased (0.69420 --> 0.69181). Saving model... | T_time: 00:00:04, T_speed: 2.500\n",
      "[Epoch  20] T_loss: 0.69111, T_accuracy: 52.9062 | V_loss: 0.69345, V_accuracy: 47.3333 | Early stopping counter: 1 out of 10 | T_time: 00:00:08, T_speed: 2.500\n",
      "[Epoch  30] T_loss: 0.69095, T_accuracy: 52.9629 | V_loss: 0.69442, V_accuracy: 47.3333 | Early stopping counter: 2 out of 10 | T_time: 00:00:12, T_speed: 2.500\n",
      "[Epoch  40] T_loss: 0.69101, T_accuracy: 52.7360 | V_loss: 0.69485, V_accuracy: 47.3333 | Early stopping counter: 3 out of 10 | T_time: 00:00:16, T_speed: 2.500\n",
      "[Epoch  50] T_loss: 0.69084, T_accuracy: 52.6793 | V_loss: 0.69286, V_accuracy: 57.3333 | Early stopping counter: 4 out of 10 | T_time: 00:00:20, T_speed: 2.500\n",
      "[Epoch  60] T_loss: 0.69080, T_accuracy: 52.7644 | V_loss: 0.69245, V_accuracy: 52.6667 | Early stopping counter: 5 out of 10 | T_time: 00:00:24, T_speed: 2.500\n",
      "[Epoch  70] T_loss: 0.69105, T_accuracy: 52.7927 | V_loss: 0.69289, V_accuracy: 53.3333 | Early stopping counter: 6 out of 10 | T_time: 00:00:28, T_speed: 2.500\n",
      "[Epoch  80] T_loss: 0.69098, T_accuracy: 52.8778 | V_loss: 0.69275, V_accuracy: 53.3333 | Early stopping counter: 7 out of 10 | T_time: 00:00:32, T_speed: 2.500\n",
      "[Epoch  90] T_loss: 0.69091, T_accuracy: 53.1897 | V_loss: 0.69283, V_accuracy: 53.3333 | Early stopping counter: 8 out of 10 | T_time: 00:00:36, T_speed: 2.500\n",
      "[Epoch 100] T_loss: 0.69118, T_accuracy: 52.6226 | V_loss: 0.69355, V_accuracy: 47.3333 | Early stopping counter: 9 out of 10 | T_time: 00:00:40, T_speed: 2.500\n",
      "[Epoch 110] T_loss: 0.69057, T_accuracy: 52.8494 | V_loss: 0.69327, V_accuracy: 50.6667 | Early stopping counter: 10 out of 10 *** TRAIN EARLY STOPPED! *** | T_time: 00:00:44, T_speed: 2.500\n",
      "Final training time: 00:00:44\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Training accuracy (%)</td><td>▂▁▅▆▄▄▄▄▅█▃▅</td></tr><tr><td>Training loss</td><td>█▇▄▃▄▃▂▄▃▃▅▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁███████████</td></tr><tr><td>Validation accuracy (%)</td><td>▁▅▁▁▁█▅▅▅▅▁▃</td></tr><tr><td>Validation loss</td><td>▇▁▅▇█▃▂▃▃▃▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>110</td></tr><tr><td>Training accuracy (%)</td><td>52.84945</td></tr><tr><td>Training loss</td><td>0.69057</td></tr><tr><td>Training speed (epochs/sec.)</td><td>2.5</td></tr><tr><td>Validation accuracy (%)</td><td>50.66667</td></tr><tr><td>Validation loss</td><td>0.69327</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_12_18_04_26_47</strong> at: <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/r6j82p0y' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw/runs/r6j82p0y</a><br> View project at: <a href='https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw' target=\"_blank\">https://wandb.ai/tmdrjs0040/lstm_classification_btc_krw</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241218_042705-r6j82p0y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /home/work/DL/4/checkpoints_classification/lstm_classification_btc_krw_checkpoint_latest.pt\n",
      "[TEST DATA]\n",
      "TEST RESULTS: 53.333%\n",
      " 0:   1.00 <-->   1.00\n",
      " 1:   0.00 <-->   1.00\n",
      " 2:   0.00 <-->   0.00\n",
      " 3:   1.00 <-->   1.00\n",
      " 4:   1.00 <-->   1.00\n",
      " 5:   1.00 <-->   1.00\n",
      " 6:   1.00 <-->   1.00\n",
      " 7:   0.00 <-->   1.00\n",
      " 8:   1.00 <-->   1.00\n",
      " 9:   0.00 <-->   0.00\n",
      "10:   1.00 <-->   1.00\n",
      "11:   1.00 <-->   0.00\n",
      "12:   1.00 <-->   0.00\n",
      "13:   1.00 <-->   1.00\n",
      "14:   1.00 <-->   1.00\n",
      "15:   1.00 <-->   1.00\n",
      "16:   1.00 <-->   1.00\n",
      "17:   0.00 <-->   0.00\n",
      "18:   1.00 <-->   0.00\n",
      "19:   0.00 <-->   1.00\n",
      "20:   1.00 <-->   0.00\n",
      "21:   1.00 <-->   0.00\n",
      "22:   1.00 <-->   0.00\n",
      "23:   1.00 <-->   1.00\n",
      "24:   1.00 <-->   0.00\n",
      "25:   1.00 <-->   0.00\n",
      "26:   1.00 <-->   1.00\n",
      "27:   1.00 <-->   0.00\n",
      "28:   1.00 <-->   0.00\n",
      "29:   1.00 <-->   0.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    validation_intervals = 10\n",
    "    learning_rate = 1e-3\n",
    "    early_stop_patience = 10\n",
    "    early_stop_delta = 0.00001\n",
    "    weight_decay  = 0.001\n",
    "  \n",
    "    main_train(epochs, batch_size, validation_intervals, learning_rate,early_stop_patience, early_stop_delta, weight_decay)\n",
    "    main_test(epochs, batch_size, validation_intervals, learning_rate, early_stop_patience, early_stop_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea4f73",
   "metadata": {},
   "source": [
    "test결과 53%의 정확도를 보임 -> 그렇게 높지 않은 정도의 정확도를 보임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3 (NGC 24.03/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
